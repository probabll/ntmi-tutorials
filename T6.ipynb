{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open this notebook on Colab](https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T6.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "jzvWfS-ELNzE",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Guide\n",
    "\n",
    "* Before working on this tutorial, you should have worked through the [introduction to PyTorch](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n",
    "* Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
    "* Note that, as always, the notebook contains a condensed version of the theory We recommend you read the theory part before the LC session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "KqR7WUDXLeME",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ILOs\n",
    "\n",
    "After completing this lab you should be able to\n",
    "\n",
    "* develop neural sequence labellers in PyTorch\n",
    "* estimate parameters via MLE\n",
    "* predict tag sequences for novel data\n",
    "* evaluate tagging performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "YBR2bPwLL9gj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## General Notes\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3.\n",
    "* Use Torch. \n",
    "* This tutorial runs smoothly enough on CPU. \n",
    "\n",
    "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "CqDZh0QJJsOu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "toc"
    ]
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    "### Topics \n",
    "\n",
    "* [Data](#sec:Data)\n",
    "\t* [Vocabulary](#sec:Vocabulary)\n",
    "\t* [Corpus and Data Loader](#sec:Corpus_and_Data_Loader)\n",
    "* [Neural Tagger](#sec:Neural_Tagger)\n",
    "\t* [Formalisation](#sec:Formalisation)\n",
    "\t* [Encoder](#sec:Encoder)\n",
    "\t* [Parallel Tagging](#sec:Parallel_Tagging)\n",
    "\t* [Decoder](#sec:Decoder)\n",
    "\t* [Sequential Tagging](#sec:Sequential_Tagging)\n",
    "\t \t* [Markov Tagger](#sec:Markov_Tagger)\n",
    "\t \t* [Autoregressive Tagger](#sec:Autoregressive_Tagger)\n",
    "* [Training and Evaluation](#sec:Training_and_Evaluation)\n",
    "\t* [Perplexity](#sec:Perplexity)\n",
    "\t* [Decision rules and tagging accuracy](#sec:Decision_rules_and_tagging_accuracy)\n",
    "* [Experiment](#sec:Experiment)\n",
    "\n",
    "\n",
    "### Table of ungraded exercises\n",
    "\n",
    "1. [Tagger](#ungraded-1)\n",
    "1. [BasicEncoder](#ungraded-2)\n",
    "1. [ParallelTagger](#ungraded-3)\n",
    "1. [SequentialTagger](#ungraded-4)\n",
    "\n",
    "\n",
    "### Table of graded exercises\n",
    "\n",
    "*Important:* The grader may re-run your notebook to investigate its correctness, but you must upload your notebook with the cells already run and make sure that all your answers are visible without the need to re-run the notebook.\n",
    "\n",
    "\n",
    "Exercises have equal weights.\n",
    "\n",
    "\n",
    "1. [POS tagging data](#graded-1)\n",
    "1. [BidirectionalEncoder](#graded-2)\n",
    "1. [NGramDecoder](#graded-3)\n",
    "1. [AutoregressiveDecoder](#graded-4)\n",
    "1. [Comparison](#graded-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "fVnfg0kMLrsi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "LzjCfsJDNL1B",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "QV4oRuW-XYED",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "B8qybX6RNDKh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Data'></a>\n",
    "# Data\n",
    "\n",
    "In this tutorial we will develop models for sequence labelling. So our data for this tutorial will be collections of sentences, or *corpora*, annotated with token-level tags (e.g., part-of-speech tags or named-entity tags, etc). See [Section 8.2 and 8.3 of the textbook](https://web.stanford.edu/~jurafsky/slp3/8.pdf) to learn more about the data type. \n",
    "\n",
    "We will use corpora available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "KfbZPjfaZDdo",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "7ZuAZ5Il891R",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "# The treebank is small enough for experiments on CPU\n",
    "# even though it is small, it's an okay size for POS tagging\n",
    "\n",
    "corpus = treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "cjtCV9z3FuQO",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The method `tagged_sents` will give us a view of tokenized sentences with their token lag tag annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "HFQdJD079FY6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = corpus.tagged_sents(tagset='universal')[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "lhejnWoH_BlC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def tostring(seq_pair, vertical=True, headers=['Word', 'Tag']):\n",
    "    \"\"\"\n",
    "    :param seq_pair: a sequence of pairs, each pair is a token and a tag. \n",
    "    :param vertical: use True for vertical printing with tabulate.\n",
    "    :return: a string representing the sequence of pairs.\n",
    "    \"\"\"\n",
    "    if vertical:\n",
    "        return tabulate(list(seq_pair), headers=headers)\n",
    "    else:\n",
    "        return ' '.join(f'{w}/{t}' for w, t in seq_pair)\n",
    "\n",
    "def tostring2(tok_seq, tag_seq, vertical=True, headers=['Word', 'Tag']):\n",
    "    \"\"\"\n",
    "    :param tok_seq: a sequence of tokens\n",
    "    :param tag_seq: a corresponding sequence of tags\n",
    "        they must have the same length\n",
    "    :param vertical: use True for vertical printing with tabulate.\n",
    "    :return: a string representing the sequence of pairs.\n",
    "    \"\"\"\n",
    "    assert len(tok_seq) == len(tag_seq), \"I need the same number of elements in both sequences\"\n",
    "    return tostring(zip(tok_seq, tag_seq), vertical=vertical, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "N7NtsPiIF4Kr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tostring(example, vertical=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "WGl7q-_AG0WY",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tostring(example, vertical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "BNqS36sZNExH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_nltk_corpus(corpus, num_heldout, max_length=30, delete_traces=False, rng=np.random.RandomState(42)):\n",
    "    \"\"\"\n",
    "    Shuffle and split a corpus.\n",
    "    \n",
    "    :param corpus: an NLTK corpus of tagged sequences, each sequence is a pair, each pair is a token and a tag.    \n",
    "    :param num_heldout: number of sentences in the dev/test sets\n",
    "    :param max_length: discard sentences longer than this\n",
    "    :param delete_traces: the PennTreebank uses something called a \"trace\" to \n",
    "     annotate certain linguistic structures that we do not want to use in this class\n",
    "     use delete_traces=True to remove them from the data\n",
    "\n",
    "    :return: \n",
    "        (training word sequences, training tag sequences), \n",
    "        (dev word sequences, dev tag sequences), \n",
    "        (test word sequences, test tag sequences),         \n",
    "    \"\"\"\n",
    "    tagged_sentences = corpus.tagged_sents(tagset='universal')\n",
    "    # do not change the seed in here    \n",
    "    order = rng.permutation(np.arange(len(tagged_sentences)))    \n",
    "    word_sequences = [[w.lower() for w, t in tagged_sentences[i]] for i in order if len(tagged_sentences[i]) <= max_length]    \n",
    "    tag_sequences = [[t for w, t in tagged_sentences[i]] for i in order if len(tagged_sentences[i]) <= max_length]    \n",
    "\n",
    "    if delete_traces:\n",
    "        no_traces_word_sequences = []\n",
    "        no_traces_tag_sequences = []\n",
    "        for ws, ts in zip(word_sequences, tag_sequences):\n",
    "            words = []\n",
    "            tags = []\n",
    "            for w, t in zip(ws, ts):\n",
    "                if not w.startswith(\"*\"):\n",
    "                    words.append(w)\n",
    "                    tags.append(t)\n",
    "            no_traces_word_sequences.append(words)\n",
    "            no_traces_tag_sequences.append(tags)\n",
    "        word_sequences = no_traces_word_sequences\n",
    "        tag_sequences = no_traces_tag_sequences\n",
    "            \n",
    "    return (word_sequences[2*num_heldout:], tag_sequences[2*num_heldout:]), (word_sequences[num_heldout:2*num_heldout], tag_sequences[num_heldout:2*num_heldout]), (word_sequences[:num_heldout], tag_sequences[:num_heldout])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "IHYqnVljTaw5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "For `treebank` this will take about 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "7-pkD_jNOHBj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "(training_x, training_y), (dev_x, dev_y), (test_x, test_y) = split_nltk_corpus(\n",
    "    corpus, \n",
    "    num_heldout=100 if corpus is treebank else 1000,\n",
    "    delete_traces=corpus is treebank\n",
    ")\n",
    "print(f\"Number of sentences: training={len(training_x)} dev={len(dev_x)} test={len(test_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "Fk5nks5bKG0a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"# A few training sentences\\n\\n\")\n",
    "for n in range(3):    \n",
    "    print(tostring2(training_x[n], training_y[n]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-1'> **Graded Exercise 1 - POS tagging data** </a>\n",
    "\n",
    "* Plot the marginal frequency (number of times a tag occurs divided by total occurrences of tags) of the tags in the dataset\n",
    "* Plot the most frequent nouns, verbs, adjectives, and adverbs and their conditional frequency (given a tag, number of times a word occurs divided by number of occurrences of that tag). Use top-10. \n",
    "* What's the proportion of the vocabulary (of words) that exhibit ambiguity of POS. List some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "3qmFucRGr1Lh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Vocabulary'></a>\n",
    "## Vocabulary\n",
    "\n",
    "As always when dealing with NLP models, we need an object to maintain our vocabulary of known tokens. This time we will rely on word-tokenization rather than BPE tokenization (the reason behind this choice is that in POS tagging we **need** to maintain a one-to-one alignment between our word sequence and our tag sequence, BPE tokenisation could easily break that). Fortunately, NLK corpora are already tokenized.\n",
    "\n",
    "Our vocabulary class will maintain the set of known tokens, and a dictionary to convert tokens to codes and codes back to tokens. The class will also take care of some special symbols (e.g., BOS, EOS, UNK, PAD). \n",
    "\n",
    "Finally, if later on you test your model on sentences that are not word tokenized, you can use `nlt.tokenize.word_tokenize` or any other tokenizer you like (as long as the level of tokenization is similar to the one you used for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "EcuhA0GEr8W4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "gUUct5byIkKu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is how you can tokenize English sentences (but remember that we don't need to redo this for the training/dev/test data from NLKT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "kT2gOTZtr_9w",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_tokenize(\"This is a sentence, and this is another.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Kt5gPbIDIrx8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will adapt one of the classes we developed in previous tutorials, and this class will be used for maintaining both the vocabulary of known tokens and the set of known tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "yMrE8lvlr2aa",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, corpus: list, min_freq=1):        \n",
    "        \"\"\"\n",
    "        :param corpus: list of documents, each document is a list of tokens, each token is a string\n",
    "        :param min_freq: words that occur less than this value are discarded\n",
    "        \"\"\"\n",
    "        # Make the vocabulary of known words\n",
    "\n",
    "        # Count word occurrences\n",
    "        counter = Counter(chain(*corpus))\n",
    "        # sort them by frequency\n",
    "        sorted_by_freq_tuples = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = \"-PAD-\"        \n",
    "        self.bos_token = \"-BOS-\"\n",
    "        self.eos_token = \"-EOS-\"\n",
    "        self.unk_token = \"-UNK-\"\n",
    "        self.pad_id = 0\n",
    "        self.bos_id = 1\n",
    "        self.eos_id = 2\n",
    "        self.unk_id = 3\n",
    "\n",
    "        self.known_symbols = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n",
    "        self.counts = [0, 0]\n",
    "        \n",
    "        # Vocabulary\n",
    "        self.word2id = OrderedDict()                \n",
    "        self.word2id[self.pad_token] = self.pad_id        \n",
    "        self.word2id[self.bos_token] = self.bos_id\n",
    "        self.word2id[self.eos_token] = self.eos_id\n",
    "        self.word2id[self.unk_token] = self.unk_id\n",
    "        self.min_freq = min_freq\n",
    "        for w, n in sorted_by_freq_tuples: \n",
    "            if n >= min_freq: # discard infrequent words\n",
    "                self.word2id[w] = len(self.known_symbols)\n",
    "                self.known_symbols.append(w)\n",
    "                self.counts.append(n)\n",
    "        \n",
    "        # store the counts for later\n",
    "        self.counts = np.array(self.counts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.known_symbols)\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        \"\"\"\n",
    "        :param word: a string\n",
    "        :return: the id (int) of a word (str)\n",
    "            unk_id is returned for unknown words\n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def encode(self, doc: list, add_bos=False, add_eos=False, pad_right=0):\n",
    "        \"\"\"\n",
    "        Transform a document into a sequence of integer token identifiers.\n",
    "        :param doc: list of tokens, each token is a string\n",
    "        :param add_bos: whether to add the BOS token\n",
    "        :param add_eos: whether to add the EOS token\n",
    "        :param pad_right: number of suffix padding tokens \n",
    "        \n",
    "        :return: a list of codes (possibly with BOS and EOS added as well as padding)\n",
    "        \"\"\"\n",
    "        return [self.word2id.get(w, self.unk_id) for w in chain([self.bos_token] * int(add_bos), doc, [self.eos_token] * int(add_eos), [self.pad_token] * pad_right)]\n",
    "\n",
    "    def batch_encode(self, docs: list, add_bos=False, add_eos=False):\n",
    "        \"\"\"\n",
    "        Transform a batch of documents into a numpy array of integer token identifiers.\n",
    "        This will pad the shorter documents to the length of the longest document.\n",
    "        \n",
    "        :param docs: a list of documents\n",
    "        :param add_bos: whether to add the BOS token\n",
    "        :param add_eos: whether to add the EOS token\n",
    "        :param pad_right: number of suffix padding tokens\n",
    "\n",
    "        :return: numpy array with shape [len(docs), longest_doc + add_bos + add_eos]\n",
    "        \"\"\"\n",
    "        max_len = max(len(doc) for doc in docs)\n",
    "        return np.array([self.encode(doc, add_bos=add_bos, add_eos=add_eos, pad_right=max_len-len(doc)) for doc in docs])\n",
    "\n",
    "    def decode(self, ids, strip_pad=False):\n",
    "        \"\"\"\n",
    "        Transform a np.array document into a list of tokens.\n",
    "        :param ids: np.array with shape [num_tokens] \n",
    "        :param strip_pad: whether PAD tokens should be deleted from the output\n",
    "\n",
    "        :param return: list of strings with size [num_tokens - num_padding]\n",
    "        \"\"\"\n",
    "        if strip_pad:\n",
    "            return [self.known_symbols[id] for id in ids if id != self.pad_id]\n",
    "        else:\n",
    "            return [self.known_symbols[id] for id in ids]\n",
    "\n",
    "    def batch_decode(self, docs, strip_pad=False):\n",
    "        \"\"\"\n",
    "        Transform a np.array collection of documents into a collection of lists of tokens.\n",
    "        :param ids: np.array with shape [num_docs, max_length] \n",
    "        :param strip_pad: whether PAD tokens should be deleted from the output\n",
    "\n",
    "        :return: list of documents, each a list of tokens, each token a string\n",
    "        \"\"\"\n",
    "        return [self.decode(doc, strip_pad=strip_pad) for doc in docs]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "HxTwRreJI7DC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "ZVR6Nfh5skV-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we get a vocabulary for words\n",
    "word_vocab = Vocabulary(training_x, min_freq=2)\n",
    "# and a vocabulary for tags\n",
    "tag_vocab = Vocabulary(training_y, min_freq=1)\n",
    "# you can see their sizes V and C:\n",
    "len(word_vocab), len(tag_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "AM64EcW2JEoo",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The `encode` method turns a sequence of (str) symbols into a sequence of (int) codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "KY5XFhxZtTyI",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tostring2(word_vocab.encode(training_x[0]), tag_vocab.encode(training_y[0]), vertical=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "N4U7tdm-JwAN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can also have `encode` add some special symbols for us (but remember to be consistent, you should always have token sequences and tag sequences that match in length):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "tMkuE8O6JxWH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tostring2(word_vocab.encode(training_x[0], add_eos=True), tag_vocab.encode(training_y[0], add_eos=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "LnoD6FZQKIM8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here for example, we will add BOS, EOS, and we are going to encode and decode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "dL88zQGjs6zM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tostring2(word_vocab.decode(word_vocab.encode(training_x[0], add_bos=True, add_eos=True)), tag_vocab.decode(tag_vocab.encode(training_y[0], add_bos=True, add_eos=True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "dcGLLsUsKTrM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can also encode and decode entire batches of sequences. This will use pad symbols/codes to make the sequences in the same batch have the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "rOAVwaWX6kA9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_vocab.batch_encode(training_x[:2], add_bos=False, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "_ScoTY8-6u0X",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_vocab.batch_decode(word_vocab.batch_encode(training_x[:2], add_bos=False, add_eos=True), strip_pad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "06BMGjPT7BFp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Corpus_and_Data_Loader'></a>\n",
    "## Corpus and Data Loader\n",
    "\n",
    "We will be developing our models in torch, thus we need to wrap our corpus into a `Dataset` and a `DataLoader`.\n",
    "\n",
    "A common API for sequence labellers is to imagine a sequence pair (text, annotation) as **three sequences of equal length**:\n",
    "* `x` is a tokenized version of the text, where we have EOS at the end;\n",
    "* `y_in` is a tokenized version of the annotation, where we have BOS at the beginning;\n",
    "* `y_out` is a tokenized version of the annotation, where we have EOS at the end.\n",
    "\n",
    "Here's an example\n",
    "* text = \"a dog chases a cat\"\n",
    "* annotation = \"DET NOUN VERB DET NOUN\"\n",
    "becomes\n",
    "```\n",
    "x =     [id(a),    id(dog),  id(chases), id(a),    id(cat),  id(EOS)]\n",
    "y_in  = [id(BOS),  id(DET),  id(NOUN),   id(VERB), id(DET),  id(NOUN)]\n",
    "y_out = [id(DET),  id(NOUN), id(VERB),   id(DET),  id(NOUN), id(EOS)]\n",
    "```\n",
    "    \n",
    "This way you can see that for any position `i` in `x`, the corresponding position `i` in `y_out` is its POS tag, \n",
    " and it is safe to condition on elements in `y_in` up until that same position (including it) without risk of \n",
    " conditioning on present and future tags.\n",
    "\n",
    "Of course, there are other ways to code such a tagged dataset. We teach it this way because it's a rather common way to do it, and you will find public APIs that do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "3ptZKyBw7FN3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TaggedCorpus(Dataset):\n",
    "    \"\"\"\n",
    "    A torch dataset for sequence labelling\n",
    "\n",
    "    A common API for sequence labellers is to imagine a sequence pair (text, annotation) as three sequences of equal length:\n",
    "        x is a tokenized version of the text, where we have EOS at the end;\n",
    "        y_in is a tokenized version of the annotation, where we have BOS at the beginning\n",
    "        y_out is a tokenized version of the annotation, where we have EOS at the end.\n",
    "    Here's an example\n",
    "        text = \"a dog chases a cat\"\n",
    "        annotation = \"DET NOUN VERB DET NOUN\"\n",
    "    becomes\n",
    "        x =     [id(a),    id(dog),  id(chases), id(a),    id(cat),  id(EOS)]\n",
    "        y_in  = [id(BOS),  id(DET),  id(NOUN),   id(VERB), id(DET),  id(NOUN)]\n",
    "        y_out = [id(DET),  id(NOUN), id(VERB),   id(DET),  id(NOUN), id(EOS)]\n",
    "    \n",
    "    This way you can see that for any position i in x, the corresponding position i y_out is its POS tag\n",
    "     and it is safe to condition on elements in y_in up until that same position (including it) without risk of \n",
    "     conditioning on present and future tags.\n",
    "\n",
    "    (Of course, there are other ways to code a TaggedCorpus Dataset. \n",
    "     We teach it this way because it's a rather common way to do it, and you will find public APIs that do the same.)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_x, corpus_y, word_vocab: Vocabulary, tag_vocab: Vocabulary):\n",
    "        \"\"\"\n",
    "        In PyTorch we better always manipulate numerical codes, rather than text.\n",
    "        So, our Corpus object will contain a vocab that converts words to codes.\n",
    "\n",
    "        :param corpus_x: token sequences\n",
    "        :param corpus_y: tag sequences\n",
    "        :param word_vocab: vocabulary for token sequences\n",
    "        :param tag_vocab: vocabulary for tag sequences\n",
    "        \"\"\"\n",
    "        self.corpus_x = list(corpus_x)\n",
    "        self.corpus_y = list(corpus_y)\n",
    "        assert len(self.corpus_x) == len(self.corpus_y), \"I need sequence pairs\"\n",
    "        assert all(len(x) == len(y) for x, y in zip(corpus_x, corpus_y)), \"A sequence pair should match in number of elements\"\n",
    "        self.word_vocab = word_vocab\n",
    "        self.tag_vocab = tag_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Size of the corpus in number of sequence pairs\"\"\"\n",
    "        return len(self.corpus_x)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        :param idx: a number of 0 to len(self)-1\n",
    "        :return: \n",
    "            corpus_x[idx] (with add_eos=True) \n",
    "            corpus_y[idx] (with add_bos=True)\n",
    "            corpus_y[idx] (with add_eos=True)\n",
    "            all strings converted to codes using the correct vocabulary\n",
    "        \"\"\"\n",
    "        x = self.word_vocab.encode(self.corpus_x[idx], add_bos=False, add_eos=True)\n",
    "        y_in = self.tag_vocab.encode(self.corpus_y[idx], add_bos=True, add_eos=False)\n",
    "        y_out = self.tag_vocab.encode(self.corpus_y[idx], add_bos=False, add_eos=True)\n",
    "        return x, y_in, y_out\n",
    "\n",
    "\n",
    "def pad_to_longest(sequences, pad_id=0):\n",
    "    \"\"\"\n",
    "    Take a list of coded sequences and returns a torch tensor where\n",
    "    every sentence has the same length (by means of using PAD tokens)\n",
    "\n",
    "    :param sequences: these are tokenized sequences of variable length\n",
    "    :param pad_id: the id of the PAD symbol\n",
    "\n",
    "    :return: batch of padded sequences\n",
    "    \"\"\"\n",
    "    longest = max(len(x) for x in sequences)\n",
    "    x = torch.tensor([x + [pad_id] * (longest - len(x)) for x in sequences])\n",
    "\n",
    "    return x\n",
    "\n",
    "def pad_to_longest_aligned(aligned_sequences, pad_id=0):\n",
    "    \"\"\"\n",
    "    Take a list of coded sequence pairs and returns 3 torch tensors where\n",
    "        every sentence has the same length (by means of using PAD tokens)\n",
    "\n",
    "    :param aligned_sequences: these are triplets of tokenized sequences of variable length\n",
    "        one is x, the other is y_in, the last one is y_out\n",
    "    :param pad_id: the id of the PAD symbol\n",
    "\n",
    "    :return: batch of padded sequences x, batch of padded sequences y_in, batch of padded sequences y_out\n",
    "    \"\"\"\n",
    "    seqs_x = pad_to_longest([x for x, y_in, y_out in aligned_sequences])\n",
    "    seqs_in = pad_to_longest([y_in for x, y_in, y_out in aligned_sequences])\n",
    "    seqs_out = pad_to_longest([y_out for x, y_in, y_out in aligned_sequences])\n",
    "    return seqs_x, seqs_in, seqs_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "_jqOgaYwLPhE",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here we join the token sequences and tag sequences from NLTK into `Dataset` objects for training, development and testing. Note that they share the same vocabularies which were constructed using the training set alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "rVrZjx0R8gYU",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training = TaggedCorpus(training_x, training_y, word_vocab, tag_vocab)\n",
    "dev = TaggedCorpus(dev_x, dev_y, word_vocab, tag_vocab)\n",
    "test = TaggedCorpus(test_x, test_y, word_vocab, tag_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "I-ebPbVxLha8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here's an example of how we get a `DataLoader` for a corpus, we simply choose the `Dataset` object we want (training/dev/test), the batch size we want, whether we need shuffling (e.g., for training batches in SGD), and how we \"glue\" data points of different length together (i.e., a function such as `pad_to_longest_aligned`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "NhVrqdcI827_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batcher = DataLoader(training, batch_size=3, shuffle=True, collate_fn=pad_to_longest_aligned)\n",
    "for batch_x, batch_y_in, batch_y_out in batcher:\n",
    "    print(\"# This is how the sequence pairs in a batch come out of the data loader\\n\")\n",
    "\n",
    "    for x, y_in, y_out in zip(batch_x, batch_y_in, batch_y_out):\n",
    "        print(tostring2(x, y_out))\n",
    "        print()\n",
    "\n",
    "    print(\"# And we can always decode them for inspection\\n\")\n",
    "    # stripping padding makes it easier to read the examples\n",
    "    for x, y in zip(word_vocab.batch_decode(batch_x, strip_pad=True), tag_vocab.batch_decode(batch_y_out, strip_pad=True)):\n",
    "        print(tostring2(x, y))\n",
    "        print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "JlqKGKLn1FSH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Neural_Tagger'></a>\n",
    "# Neural Tagger\n",
    "\n",
    "In sequence labelling we have two sequences of equal length: a word sequence $x_{1:l}$ and a tag sequence $y_{1:l}$.\n",
    "\n",
    "The word-sequence $x_{1:l} = \\langle x_1, \\ldots, x_l \\rangle$, where $l$ is the sequence length, is such that token $x_i$ belongs to a vocabulary $\\mathcal W$ of $V$ known words. \n",
    "\n",
    "The tag-sequence $y_{1:l} = \\langle y_1, \\ldots, y_l \\rangle$, where $l$ is the same length as the document $x_{1:l}$, is such that each tag $y_i$ belongs to a vocabulary $\\mathcal T$ of $C$ known tags.\n",
    "\n",
    "The NLP task (sequence labelling or tagging) is the task of predicting a tag-sequence appropriate for a given word-sequence. The statistical task that makes the NLP task possible is that of learning a distribution over all possible ways to tag the input $x_{1:l}$. Then, for test-time predictions, we search through this distribution for a tag-sequence that's assigned high probability under the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Formalisation'></a>\n",
    "## Formalisation\n",
    "\n",
    "We formalise a sequence labeller or tagger as a conditional probabilistic model. We regard the word-sequence as a random sequence $X_{1:L}$, where each element $X_i$ is a random word from the vocabulary $\\mathcal W$, and the tag-sequence as a random sequence $Y_{1:L}$, where each element $Y_i$ is a random tag from the tagset $\\mathcal T$. \n",
    "\n",
    "A general tagger is a conditional distribution which assigns probability \n",
    "\\begin{align}\n",
    "    P(Y_{1:L}=y_{1:l}|X_{1:L}=x_{1:l}) &= \\prod_{i=1}^l P(Y_i=y_i|X_{1:L}=x_{1:l}, Y_{<i}=y_{<i})\n",
    "\\end{align}\n",
    "to a tag-sequence $y_{1:l}$ given a word-sequence $x_{1:l}$. \n",
    "This is a general factorisation that uses chain rule to re-express the left-hand side in terms of factors of the kind $P(Y_i=y_i|X_{1:L}=x_{1:l}, Y_{<i}=y_{<i})$, each such factor assigns probability to the next tag $y_i$ given the entire word-sequence $x_{1:l}$ and the history of tags $y_{<i}$ generated before $y_i$.\n",
    "Specific implementations of this factor may exploit certain conditional independence assumptions in order to limit access to the history $y_{<i}$.\n",
    "\n",
    "Note that we *condition* on the word-sequence, but do not generate it (or assign probability to it), instead we generate (or assign probability to) the tag-sequence.\n",
    "\n",
    "Here we specify a general template for the elementary factor of the model. As $Y_i$ is \n",
    "a random variable over the space $\\mathcal T = [C]$ of possible tags, we use a Categorical distribution over $C$ classes\n",
    "\\begin{align}\n",
    "Y_i | X_{1:L}=x_{1:l}, Y_{<i}=y_{<i} &\\sim \\mathrm{Categorical}(\\mathbf g(x_{1:l}, y_{<i}; \\theta))\n",
    "\\end{align}\n",
    "where $\\mathbf g$ is a neural network as follows:\n",
    "\\begin{align}\n",
    "\\mathbf u_{1:l} &= \\mathrm{encoder}_{H}(x_{1:l}; \\theta_{\\text{enc}})\\\\\n",
    "\\mathbf h_i &= \\mathrm{decoder}_K(y_{<i}; \\theta_{\\text{dec}})\\\\\n",
    "\\mathbf v_i &= \\mathrm{concat}(\\mathbf u_i, \\mathbf h_i)\\\\\n",
    "\\mathbf s_i &= \\mathrm{ffnn}_C(\\mathbf v_i; \\theta_{\\text{out}})\\\\\n",
    "\\mathbf g(x_{1:l}, y_{<i}; \\theta) &= \\mathrm{softmax}(\\mathbf s_i)\n",
    "\\end{align}\n",
    "\n",
    "The first step is to encode the elements of the word-sequence $x_{1:l}$ into $l$ vectors $\\mathbf u_{1:l}$ of fixed dimensionality. Next, relative to each position $i \\in [l]$, we encode the *history* $y_{<i}$ into a single vetor $\\mathbf h_i$ (this is what _decoder_ components typically do in NLP, they encode the \"past\", so we can predict the \"future\"), the concatenation of the representation $\\mathbf u_i$ of the $i$th word of the input with the representation $\\mathbf h_i$ of the $i$th history gives us the vector $\\mathbf v_i$, which we take as a representation of the state of the tagger for that step. Finally, we use an FFNN to predict $C$-dimensional $\\mathbf s_i$ (this is a score for each possible tag in the tagset) from the state $\\mathbf v_i$ of the tagger, which then softmax turns into probabilities for the Categorical cpd.\n",
    "\n",
    "Next, we develop each one of these blocks. For the decoder block we will have a few different options varying in how much of the history they actually use (from nothing  to everything). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "z_z8N_ZpTNMy",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_all(seed=42):    \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "pf3iDZ-nhL9E",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as td\n",
    "import torch.optim as opt\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "9GLMX3_Hvmel",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is a general class, which we will specialise into different taggers, it prescribes everything a tagger needs to support:\n",
    "* a `forward` method, which maps `x` (the word-sequence) and `y_in` (a delayed version of the tag-sequence interpreted as the history) to Categorical distributions over the tagset for each and every step of the input word-sequence;\n",
    "* a `log_prob` method, which uses the output of `forward` to assign probability to a tag-sequence `y_out` given `x` and `y_in`;\n",
    "* a `loss` method, which uses `log_prob` to compute the loss for a batch;\n",
    "* a `sample` method, which can be used to draw samples (i.e., tag-sequences) from the distribution;\n",
    "* a `greedy` method, which can be used to search for a tag-sequence to which the model assigns high probability (possibly the highest, depending on the concrete model, as we shall see).\n",
    "\n",
    "We also have a couple of support methods like `device` and `num_parameters`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-1'></a> **Ungraded Exercise 1 - Tagger**\n",
    "\n",
    "Study `log_prob` in the `Tagger` below, see how it matches the theory. Also, see how `log_prob` is used to define the loss. For now you cannot test this class, since the `forward` method is yet to be implemented (by specialised versions of this class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tagger(nn.Module):\n",
    "\n",
    "    def __init__(self, word_vocab: Vocabulary, tag_vocab: Vocabulary):\n",
    "        \"\"\"\n",
    "        :param word_vocab: the vocabulary of known words\n",
    "        :param tag_vocab: the vocabulary of known tags\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._word_vocab = word_vocab\n",
    "        self._tag_vocab = tag_vocab\n",
    "\n",
    "    @property\n",
    "    def word_vocab(self):\n",
    "        return self._word_vocab    \n",
    "\n",
    "    @property\n",
    "    def tag_vocab(self):\n",
    "        return self._tag_vocab    \n",
    "\n",
    "    def device(self):\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def num_parameters(self, trainable_only=True):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the model\n",
    "\n",
    "        :param trainable_only: change to False to count all parameters (even those in frozen layers)\n",
    "        \"\"\"\n",
    "        if trainable_only:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters() if theta.requires_grad)\n",
    "        else:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters())\n",
    "\n",
    "    def forward(self, x, y_in):\n",
    "        \"\"\"\n",
    "        In a concrete implementation, this will execute the steps in the Section Formalisation:\n",
    "            u[1]...u[l] = encoder(x[1]...x[l])\n",
    "            h[i] = decoder(y[<i])\n",
    "            v[i] = concat(u[i], h[i])\n",
    "            s[i] = ffnn(v[i])\n",
    "            g(x[1]...x[l], y[<i]) = softmax(s[i])\n",
    "        Then return a Categorical(g(x[1]...x[l], y[<i])) object. \n",
    "\n",
    "        Of course, it should do this for batches of inputs.\n",
    "\n",
    "        :param x: batched word_sequences (tokenised with add_eos=True)\n",
    "            [batch_size, max_length]\n",
    "        :param y_in: batched histories (tokenised with add_bos=True, so that it's delayed w.r.t. y_out)\n",
    "            [batch_size, max_length]\n",
    "        :return: a td.Categorical object whose logit has shape\n",
    "            [batch_size, max_length, tag_vocab_size]\n",
    "            since this must be a Categorical cpd for each variable Y_i, \n",
    "            and we have one such rv for each step of each and every batched word-sequence.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Concrete classes should implement me\")\n",
    "    \n",
    "    def log_prob(self, x, y_in, y_out):\n",
    "        \"\"\"\n",
    "        Computes the log conditional probability of each tag sequence in a batch\n",
    "         as shown in the section Formalisation:\n",
    "\n",
    "        \\log P(y[1]...y[l]|x[1]...x[l]) = \\log \\prod_i Cat(y[i]|g(x[1]...x[l], y[<i]))\n",
    "         = \\sum_i \\log Cat(y[i]|g(x[1]...x[l], y[<i]))\n",
    "\n",
    "        :param x: [batch_size, max_length]\n",
    "        :param y_in: [batch_size, max_length]\n",
    "        :param y_out: [batch_size, max_length]\n",
    "        :return: a batch of log probabilities, one per tag-sequence given its corresponding word-sequence\n",
    "            shape [batch_size]\n",
    "        \"\"\"\n",
    "        # one C-dimensional Categorical cpd for each token in the batch\n",
    "        cpds = self(x=x, y_in=y_in)\n",
    "        # [batch_size, max_length]        \n",
    "        logp = cpds.log_prob(y_out)\n",
    "        # [batch_size]\n",
    "        logp = torch.where(y_out != self.tag_vocab.pad_id, logp, torch.zeros_like(logp)).sum(-1)\n",
    "        return logp  \n",
    "\n",
    "    def loss(self, x, y_in, y_out):   \n",
    "        \"\"\"\n",
    "        Compute a scalar loss from a batch of sentences.\n",
    "        As we learn via MLE, the loss is the negative log likelihood of the model estimated on a single batch, \n",
    "         which is the average negative log prob for the batch.\n",
    "\n",
    "        :param x: word sequences [batch_size, max_length] \n",
    "        :param y: tag sequences [batch_size, max_length] \n",
    "        :return: average negative log prob with shape []\n",
    "        \"\"\"\n",
    "        # we reduce the batch dimension using the mean\n",
    "        return -self.log_prob(x=x, y_in=y_in, y_out=y_out).mean(0)\n",
    "\n",
    "    def sample(self, x, sample_size=None):\n",
    "        \"\"\"\n",
    "        Draws a number of samples from the model, each sample is a complete sequence.\n",
    "        We impose a maximum number of steps, to avoid infinite loops.\n",
    "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.    \n",
    "\n",
    "        The concrete implementation will depend on some choices we will make later.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def greedy(self, x):\n",
    "        \"\"\"\n",
    "        Greedly returns the argmax prediction from each Y[i]|x[1]...x[l],y[<i].\n",
    "        This is locally tagging each position i with the tag that's most probable in context.\n",
    "        \n",
    "        The concrete implementation will depend on some choices we will make later. \n",
    "        \n",
    "        :param x: [batch_size, max_length]\n",
    "        :return: a batch of tag-sequences [batch_size, max_length] \n",
    "        \"\"\"        \n",
    "        raise NotImplementedError(\"Implement me!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Encoder'></a>\n",
    "## Encoder\n",
    "\n",
    "In sequence labelling the block we call the **encoder** is a kind of text encoding function, however, instead of representing the input in a single vector (as we did in text classification), we are interested in having one vector representation for each and every position of the input text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "ex-header": "Encoder",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Study the Encoder abstract class. It offers only some basic functionality:\n",
    "* it knows a vocabulary over words\n",
    "* it knows the dimensionality of its output vectors\n",
    "* and it can count the number of trainable parameters\n",
    "Two methods will be implemented by concrete examples:\n",
    "* a method to return the device where the model component is stored\n",
    "* and the `forward` method, which will be responsible for mapping batches of word-sequences to a tensor containing the vector representation of each and every token in each and every word-sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an encoder for conditioning on the input sequence (text), \n",
    "     this encoder should return a representation for each token of an input sequence.\n",
    "\n",
    "    So, while in text classification, encoders map from x to a single vector, \n",
    "     in sequence labelling we map from x to a sequence of vectors, one vector per element of x.\n",
    "\n",
    "    The forward method is the one responsible for encoding the input x.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_vocab: Vocabulary, output_dim):\n",
    "        \"\"\"\n",
    "        :param word_vocab: Vocabulary of known words\n",
    "        :param output_dim: dimensionality of the representation of each token\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        \n",
    "        self._word_vocab = word_vocab    \n",
    "        self._output_dim = output_dim  \n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        \"\"\"Dimensionality of the output state for each position\"\"\"\n",
    "        return self._output_dim\n",
    "\n",
    "    @property\n",
    "    def word_vocab(self):\n",
    "        \"\"\"Vocabulary of known words\"\"\"\n",
    "        return self._word_vocab\n",
    "\n",
    "    def num_parameters(self, trainable_only=True):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the model\n",
    "\n",
    "        :param trainable_only: change to False to count all parameters (even those in frozen layers)\n",
    "        \"\"\"\n",
    "        if trainable_only:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters() if theta.requires_grad)\n",
    "        else:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters())    \n",
    "\n",
    "    def device(self):\n",
    "        \"\"\"Torch device where the parameters of the model are stored\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In a concrete implementation, this will ecode each token in a batch of sentences.\n",
    "        \n",
    "        :param x: batch of sentences with shape [batch_size, max_length]\n",
    "        :return: a tensor with shape [batch_size, max_length, output_dim]\n",
    "            each element represents the corresponding token in the batch x\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The most basic such function is an embedding layer:\n",
    "\\begin{align}\n",
    "\\mathbf e_i &= \\mathrm{embed}_D(x_i; \\theta_{\\text{in}})  & i \\in \\{1, \\ldots, l\\}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-2'></a> **Ungraded Exercise 2 - BasicEncoder**\n",
    "\n",
    "Complete the `BasicEncoder` below (constructor and forward). We have provided a few tests to help you debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class BasicEncoder(Encoder):\n",
    "    \"\"\"    \n",
    "    This is a basic encoder that does not contextualise the representations.\n",
    "\n",
    "    Here's an example (for a batch of two sentences)\n",
    "\n",
    "    intput (word-sequence): \n",
    "        [\n",
    "            [id(no),     id(so),   id(good), id(EOS)],\n",
    "            [id(pretty), id(good), id(EOS),  id(PAD)]\n",
    "        ]\n",
    "\n",
    "    output:\n",
    "        [\n",
    "            [embed(id(not)),    embed(id(so)),   embed(id(good)), embed(id(EOS))],\n",
    "            [embed(id(pretty)), embed(id(good)), embed(id(EOS)),  embed(id(PAD))],\n",
    "        ]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_vocab: Vocabulary, word_embed_dim: int):\n",
    "        \"\"\"\n",
    "        :param word_vocab: Vocabulary of known words\n",
    "        :param output_dim: dimensionality of the output token encoding     \n",
    "        \"\"\"        \n",
    "        super().__init__(word_vocab, word_embed_dim)\n",
    "        # **EXERCISE**\n",
    "        # 1. Construct an embedding matrix for the vocabulary of words\n",
    "        self._word_embed = None \n",
    "        \n",
    "    def device(self):\n",
    "        \"\"\"Torch device where the parameters of the model are stored\"\"\"\n",
    "        return self._word_embed.weight.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Encode each token in a batch of sentences.\n",
    "        \n",
    "        :param x: batch of sentences with shape [batch_size, max_length]\n",
    "        :return: a tensor with shape [batch_size, max_length, output_dim]\n",
    "        \"\"\"        \n",
    "        # **EXERCISE**\n",
    "        # 2. Embed the words and return their representation\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "class BasicEncoder(Encoder):\n",
    "    \"\"\"    \n",
    "    This is a basic encoder that does not contextualise the representations.\n",
    "\n",
    "    Here's an example (for a batch of two sentences)\n",
    "\n",
    "    intput (word-sequence): \n",
    "        [\n",
    "            [id(no),     id(so),   id(good), id(EOS)],\n",
    "            [id(pretty), id(good), id(EOS),  id(PAD)]\n",
    "        ]\n",
    "\n",
    "    output:\n",
    "        [\n",
    "            [embed(id(not)),    embed(id(so)),   embed(id(good)), embed(id(EOS))],\n",
    "            [embed(id(pretty)), embed(id(good)), embed(id(EOS)),  embed(id(PAD))],\n",
    "        ]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_vocab: Vocabulary, word_embed_dim: int):\n",
    "        \"\"\"\n",
    "        :param word_vocab: Vocabulary of known words\n",
    "        :param output_dim: dimensionality of the output token encoding     \n",
    "        \"\"\"        \n",
    "        super().__init__(word_vocab, word_embed_dim)\n",
    "        # Construct an embedding matrix [vocab_size, word_embed_dim]\n",
    "        self._word_embed = nn.Embedding(len(word_vocab), embedding_dim=word_embed_dim)\n",
    "        \n",
    "    def device(self):\n",
    "        \"\"\"Torch device where the parameters of the model are stored\"\"\"\n",
    "        return self._word_embed.weight.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Encode the each token in a batch of sentences.\n",
    "        \n",
    "        :param x: batch of sentences with shape [batch_size, max_length]\n",
    "        :return: a tensor with shape [batch_size, max_length, output_dim]\n",
    "        \"\"\"        \n",
    "        # [batch_size, max_length, word_embed_dim]\n",
    "        return self._word_embed(x)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We coded a few tests, so you can debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_basic_encoder(emb_dim=32):\n",
    "    seed_all()    \n",
    "    toy_basic_encoder = BasicEncoder(\n",
    "        word_vocab=word_vocab,\n",
    "        word_embed_dim=emb_dim,\n",
    "    )\n",
    "    print(toy_basic_encoder)\n",
    "    print(f\"Number of parameters: {toy_basic_encoder.num_parameters()}\\n\")\n",
    "\n",
    "    assert toy_basic_encoder.output_dim == emb_dim, \"Did you change the dimensionality?\"\n",
    "    assert toy_basic_encoder.word_vocab is word_vocab, \"Did you change the vocabulary?\"\n",
    "\n",
    "    # here we test your constructor\n",
    "    assert type(toy_basic_encoder._word_embed) is nn.Embedding, \"An embedding layer is an instance of nn.Embedding\"\n",
    "    \n",
    "    # here we test the shape of your embedding matrix\n",
    "    matrix_shape = toy_basic_encoder._word_embed.weight.shape\n",
    "    assert matrix_shape == (len(word_vocab), emb_dim), f\"An embedding layer should store one vector per word in the vocabulary, got {matrix_shape} instead\"\n",
    "\n",
    "    x = torch.from_numpy(word_vocab.batch_encode(training_x[:2], add_eos=True))\n",
    "    u = toy_basic_encoder(x)\n",
    "\n",
    "    assert u.shape == x.shape + (emb_dim,), \"The output of your tensor should be [batch_size, max_lenth, emb_dim]\"\n",
    "\n",
    "test_basic_encoder(12)\n",
    "test_basic_encoder(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The problem with the embedding layer is that the representation $\\mathbf e_i$ of the $i$th token is not at all informed by the context in which that token appears. That is, in a sentence `not so good` the embedding of the word `good` is the exact same as it is in a sentence `very very good` or in a sentence `that is really good`, etc. A good encoder for sequence labelling would return contextualised representations, whereby the vector that stands for `good` in the sentence `not so good after all` is informed by the complete context available in that sentence.\n",
    "\n",
    "Here's what the typical encoder for sequence labelling really looks like:\n",
    "\\begin{align}\n",
    "\\mathrm{encoder}_{H}(x_{1:l}; \\theta_{\\text{enc}}) &= \\mathbf u_{1:l} \\\\\n",
    "\\qquad \\text{where}&\\\\\n",
    "\\mathbf u_{1:l} &= \\mathrm{birnn}_{2K}(\\mathbf e_{1:l}; \\theta_{\\text{rnns}}) \\\\\n",
    "\\mathbf e_j &= \\mathrm{embed}_D(x_j; \\theta_{\\text{in}})  & j \\in \\{1, \\ldots, l\\}\n",
    "\\end{align}\n",
    "\n",
    "First, we embed each and every word in the word-sequence $x_{1:l}$, producing the sequence of $D$-dimensional vectors $\\mathbf e_{1:l}$. The embedding $\\mathbf e_i$ of the $i$th position is specific to the word $x_i$ and is not affected by other words in the sentence. \n",
    "To *contextualise* our representations of each token (that is, make them specific to the context in which these tokens occurred), we use a bidirectional RNN encoder (typically a BiLSTM). \n",
    "\n",
    "See the example in the class documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-2'> **Graded Exercise 2 - BidirectionalEncoder** </a>\n",
    "\n",
    "Complete the `BidirectionalEncoder` below (constructor and forward). We have provided a few tests to help you debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class BidirectionalEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    This is an encoder for conditioning on the input sequence (text), \n",
    "     this encoder should return a representation for each token of an input sequence.\n",
    "\n",
    "    So, while in text classification, encoders map from x to a single vector, \n",
    "     in sequence labelling we map from x to a sequence of vectors, one vector per element of x.\n",
    "\n",
    "\n",
    "    We illustrate the general idea with one sentence below (but the implementation\n",
    "     would have to handle batched inputs, of course).\n",
    "\n",
    "    intput (word-sequence): \n",
    "        w[1]=not\n",
    "        w[2]=so\n",
    "        w[3]=good\n",
    "        w[4]=EOS\n",
    "\n",
    "    embeddings:    \n",
    "        e[1]=emb(id(not))\n",
    "        e[2]=emb(id(so))\n",
    "        e[3]=emb(id(good))\n",
    "        e[4]=emb(id(EOS))\n",
    "    forward-rnn:\n",
    "        f[0]=zeroes\n",
    "        f[1]=rnnstep(f[0],e[1]) \n",
    "        f[2]=rnnstep(f[1],e[2]) \n",
    "        f[3]=rnnstep(f[2],e[3]) \n",
    "        f[4]=rnnstep(f[3],e[4]) \n",
    "    backward-rnn:\n",
    "        b[0]=zeroes\n",
    "        b[1]=rnnstep(b[0],e[4]) \n",
    "        b[2]=rnnstep(b[1],e[3]) \n",
    "        b[3]=rnnstep(b[2],e[2]) \n",
    "        b[4]=rnnstep(b[3],e[1]) \n",
    "    birnn:\n",
    "        u[0]=(zeroes, zeroes)\n",
    "        u[1]=(f[1], b[4])\n",
    "        u[2]=(f[2], b[3])\n",
    "        u[3]=(f[3], b[2])\n",
    "        u[4]=(f[4], b[1])\n",
    "    output:\n",
    "        u[1], ..., u[4]\n",
    "\n",
    "    You can see how any u[i] is a function of \n",
    "     every word before w[i], after w[i], and w[i] itself\n",
    "     and how the order matters (since the rnnstep is nonlinear).    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_vocab: Vocabulary, word_embed_dim: int, cell_size: int):\n",
    "        \"\"\"\n",
    "        :param word_vocab: Vocabulary of known words\n",
    "        :param word_embed_dim: dimensionality of the word embedding layer\n",
    "        :param cell_size: size of the (bidirectional) LSTM cell\n",
    "            use 0 to disable the BiLSTM (for example, for an ablation experiment)\n",
    "        \"\"\"        \n",
    "        # When we use the BiLSTM, the output of our encoding function\n",
    "        # is a sequence of vectors, one per token in the input, each vector\n",
    "        # will have 2*cell_size dimensions\n",
    "        super().__init__(word_vocab, 2*cell_size)\n",
    "                \n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        # **EXERCISE**\n",
    "        # 1. Construct an embedding layer for words in the word_vocab\n",
    "        self._word_embed = None\n",
    "        # 2. Construct a BiLSTM \n",
    "        self._birnn = None\n",
    "        \n",
    "    def device(self):\n",
    "        \"\"\"Torch device where the parameters of the model are stored\"\"\"\n",
    "        return self._word_embed.weight.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Encode each token in a batch of sentences.\n",
    "        \n",
    "        :param x: batch of sentences with shape [batch_size, max_length]\n",
    "        :return: a tensor with shape [batch_size, max_length, output_dim]\n",
    "        \"\"\"\n",
    "        # **EXERCISE**\n",
    "        # 3. Return contextualised representations from the BiLSTM        \n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We coded a few tests, so you can debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_bidirectional_encoder(emb_dim=32, cell_size=16):\n",
    "    seed_all()    \n",
    "    toy_bidir_encoder = BidirectionalEncoder(\n",
    "        word_vocab=word_vocab,\n",
    "        word_embed_dim=emb_dim,\n",
    "        cell_size=cell_size,\n",
    "    )\n",
    "    print(toy_bidir_encoder)\n",
    "    print(f\"Trainable parameters: {toy_bidir_encoder.num_parameters()}\\n\")\n",
    "\n",
    "    assert toy_bidir_encoder.output_dim == 2*cell_size, \"The output dimensionality is that of the BiRNN encoder's state\"\n",
    "    assert toy_bidir_encoder.word_vocab is word_vocab, \"Did you change the vocabulary?\"\n",
    "\n",
    "    # here we test your constructor\n",
    "    assert type(toy_bidir_encoder._word_embed) is nn.Embedding, \"An embedding layer is an instance of nn.Embedding\"\n",
    "    assert type(toy_bidir_encoder._birnn) is nn.LSTM, \"Are you sure you are using an nn.LSTM?\"\n",
    "    assert toy_bidir_encoder._birnn.bidirectional, \"Are you sure your encoder is bidirectional?\"\n",
    "    assert toy_bidir_encoder._birnn.batch_first, \"Are you sure your LSTM is set to batch-first?\"\n",
    "    \n",
    "    # here we test the shape of your embedding matrix\n",
    "    matrix_shape = toy_bidir_encoder._word_embed.weight.shape\n",
    "    assert matrix_shape == (len(word_vocab), emb_dim), f\"An embedding layer should store one vector per word in the vocabulary, got {matrix_shape} instead\"\n",
    "\n",
    "    x = torch.from_numpy(word_vocab.batch_encode(training_x[:2], add_eos=True))\n",
    "    u = toy_bidir_encoder(x)\n",
    "\n",
    "    assert u.shape == x.shape + (2*cell_size,), \"The output of your tensor should be [batch_size, max_lenth, 2*cell_size]\"\n",
    "\n",
    "test_bidirectional_encoder()\n",
    "test_bidirectional_encoder(20, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Parallel_Tagging'></a>\n",
    "## Parallel Tagging\n",
    "\n",
    "With the Encoder above we can already design a simple type of tagger, which uses information from $x_{1:l}$ but which _does not_ exploit information from the history $y_{<i}$. This type of tagger can be thought of as $l$ parallel applications of a neural $C$-way classifier: this probabilistic classifier maps a token position, represented by the decoder state $\\mathbf u_i$, to a distribution over the tagset using a neural network.\n",
    "\n",
    "Here is the model of the $i$th tag given $x_{1:l}$:\n",
    "\\begin{align}\n",
    "Y_i | X_{1:l}=x_{1:l} &\\sim \\mathrm{Categorical}(\\mathbf g(x_{1:l}, i; \\theta))\n",
    "\\end{align}\n",
    "where $\\mathbf g$ is a neural network that conditions on the entire word-sequence $x_{1:l}$ and does not use the history (it only knows what position $i$ we want to tag, but does not know the tag sequence before it). For example:\n",
    "\\begin{align}\n",
    "\\mathbf u_{1:l} &= \\mathrm{encoder}_H(x_{1:l};\\theta_{\\text{enc}})\\\\\n",
    "\\mathbf s_i &= \\mathrm{ffnn}_C(\\mathbf u_i; \\theta_{\\text{out}})\\\\\n",
    "\\mathbf g(x_{1:l}, i) &= \\mathrm{softmax}(\\mathbf s_i)\n",
    "\\end{align}\n",
    "The encoder represents the token-sequence and the ouput layer maps the $i$th state to the probabilities over tags. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-3'></a> **Ungraded Exercise 3 - ParallelTagger**\n",
    "\n",
    "Complete the `ParallelTagger` below (constructor and forward). We have provided a few tests to help you debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class ParallelTagger(Tagger):\n",
    "    \"\"\"\n",
    "    A parallel tagger is built upon a simple Categorical classifier, \n",
    "     an encoder processes the word-sequence w[1]...w[l]\n",
    "     producing contextualised representations u[1]...u[l]\n",
    "    Then, the simple Categorical classifier takes each u[i] and maps it to probabilities via \n",
    "     an FFNN: softmax(FFNN(u[i]))\n",
    "    Because it has no access to any information in the tag-sequence, we can tag all steps\n",
    "     independently of one another (i.e., in parallel).      \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder: Encoder, tag_vocab: Vocabulary, hidden_size: int, p_drop=0.):\n",
    "        \"\"\"\n",
    "        :param encoder: an encoder for tokens in word-sequences\n",
    "        :param tag_vocab: a vocabulary of known tags\n",
    "        :param hidden_size: the hidden size of the output FFNN layer\n",
    "        :param p_drop: dropout rate for regularisation before linear layers\n",
    "        \"\"\"\n",
    "        super().__init__(encoder.word_vocab, tag_vocab)\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # **EXERCISE**\n",
    "        # 1. Construct an FFNN to predicts logits over tagset\n",
    "        #   for each token position \n",
    "        self.logits_predictor = None\n",
    "\n",
    "    def device(self):\n",
    "        return encoder.device()\n",
    "\n",
    "    def forward(self, x, y_in=None):\n",
    "        \"\"\"\n",
    "        Execute the steps in the Section Parallel Tagging: for each i,\n",
    "            u[1]...u[l] = encoder(x[1]...x[l])            \n",
    "            s[i] = ffnn(u[i])\n",
    "            g(x[1]...x[l], i) = softmax(s[i])\n",
    "        Then return a Categorical(g(x[1]...x[l], i)) object. \n",
    "\n",
    "        Of course, it should do this for batches of inputs, and for all steps in each input.\n",
    "\n",
    "        :param x: batched word_sequences (tokenised with add_eos=True)\n",
    "            [batch_size, max_length]\n",
    "        :param y_in: this is ignored in parallel taggers\n",
    "        :return: a td.Categorical object whose logit has shape\n",
    "            [batch_size, max_length, tag_vocab_size]\n",
    "            since this must be a Categorical cpd for each variable Y_i, \n",
    "            and we have one such rv for each step of each and every batched word-sequence.\n",
    "        \"\"\"\n",
    "        # **EXERCISE**\n",
    "        # 2. Encode the tokens in x\n",
    "        # 3. Compute logits over the tagset for each position\n",
    "        # 4. Parameterise and return a batch of Categorical object using these logits\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def greedy(self, x):\n",
    "        \"\"\"\n",
    "        For each cpd Y[i]|x[1]...x[l], predicts the mode of the cpd.\n",
    "\n",
    "        Because this is a parallel tagger (i.e., we do not use information from y[<i]), \n",
    "         we can perform this in a very efficient way. See below.\n",
    "        \n",
    "        :param x: [batch_size, max_length]\n",
    "        :return: tag sequences [batch_size, max_length]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        max_length = x.shape[1]\n",
    "        with torch.no_grad(): # we do not need gradients for test-time predictions\n",
    "            cpds = self(x, y_in=None)  # parallel taggers do not have tag history\n",
    "            # [batch_size, max_length]\n",
    "            y_pred = torch.argmax(cpds.probs, -1)\n",
    "            # if a position in x is padded, it should be padded in y\n",
    "            y_pred = torch.where(x != self.word_vocab.pad_id, y_pred, torch.zeros_like(y_pred) + self.tag_vocab.pad_id)\n",
    "            return y_pred\n",
    "\n",
    "    def sample(self, x, sample_size=None):\n",
    "        \"\"\"\n",
    "        Per word sequence in the batch, draws a number of samples from the model, each sample is a complete tag sequence.\n",
    "\n",
    "        Because this is a parallel tagger (i.e., we do not use information from y[<i]), \n",
    "         we can perform this in a very efficient way, whereby we sample each tag independently. \n",
    "         See below.\n",
    "\n",
    "        :param x: [batch_size, max_len]\n",
    "\n",
    "        :return: tag sequences with shape [batch_size, max_len] if sample_size is None\n",
    "            else with shape [sample_size, batch_size, max_len]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        max_length = x.shape[1]\n",
    "        with torch.no_grad(): # we do not need gradients for test-time sampling\n",
    "            cpds = self(x, y_in=None)  # parallel taggers do not have tag history\n",
    "            if sample_size is None: \n",
    "                shape = (1,)  # this is needed for sampling a tensor of the correct shape\n",
    "            else:\n",
    "                shape = (sample_size,)\n",
    "            # [sample_size, batch_size, max_length]\n",
    "            y_pred = cpds.sample(shape)\n",
    "            # if a position in x is padding, it must be padded in y too\n",
    "            y_pred = torch.where(x.unsqueeze(0) != self.word_vocab.pad_id, y_pred, torch.zeros_like(y_pred) + self.tag_vocab.pad_id)\n",
    "            # takes care of output shape\n",
    "            if sample_size is None:\n",
    "                return y_pred.squeeze(0)\n",
    "            else:\n",
    "                return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "class ParallelTagger(Tagger):\n",
    "    \"\"\"\n",
    "    A parallel tagger is built upon a simple Categorical classifier, \n",
    "     an encoder processes the word-sequence w[1]...w[l]\n",
    "     producing contextualised representations u[1]...u[l]\n",
    "    Then, the simple Categorical classifier takes each u[i] and maps it to probabilities via \n",
    "     an FFNN: softmax(FFNN(u[i]))\n",
    "    Because it has no access to any information in the tag-sequence, we can tag all steps\n",
    "     independently of one another (i.e., in parallel).      \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder: Encoder, tag_vocab: Vocabulary, hidden_size: int, p_drop=0.):\n",
    "        \"\"\"\n",
    "        :param encoder: an encoder for tokens in word-sequences\n",
    "        :param tag_vocab: a vocabulary of known tags\n",
    "        :param hidden_size: the hidden size of the output FFNN layer\n",
    "        :param p_drop: dropout rate for regularisation before linear layers\n",
    "        \"\"\"\n",
    "        super().__init__(encoder.word_vocab, tag_vocab)\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # This is the FFNN output layer\n",
    "        self.logits_predictor = nn.Sequential(\n",
    "            nn.Dropout(p_drop), # regularisation before a linear layer\n",
    "            nn.Linear(encoder.output_dim, hidden_size),  # from encoder output dim to hidden\n",
    "            nn.ReLU(),  # nonlinearity\n",
    "            nn.Dropout(p_drop), # regularisation before a linear layer\n",
    "            nn.Linear(hidden_size, hidden_size),  # from hidden to hidden\n",
    "            nn.ReLU(), # nonlinearity\n",
    "            nn.Dropout(p_drop), # regularisation before a linear layer\n",
    "            nn.Linear(hidden_size, len(tag_vocab)),  # from hidden to tagset size\n",
    "        ) \n",
    "\n",
    "    def device(self):\n",
    "        return encoder.device()\n",
    "\n",
    "    def forward(self, x, y_in=None):\n",
    "        \"\"\"\n",
    "        Execute the steps in the Section Parallel Tagging: for each i,\n",
    "            u[1]...u[l] = encoder(x[1]...x[l])            \n",
    "            s[i] = ffnn(u[i])\n",
    "            g(x[1]...x[l], i) = softmax(s[i])\n",
    "        Then return a Categorical(g(x[1]...x[l], i)) object. \n",
    "\n",
    "        Of course, it should do this for batches of inputs, and for all steps in each input.\n",
    "\n",
    "        :param x: batched word_sequences (tokenised with add_eos=True)\n",
    "            [batch_size, max_length]\n",
    "        :param y_in: this is ignored in parallel taggers\n",
    "        :return: a td.Categorical object whose logit has shape\n",
    "            [batch_size, max_length, tag_vocab_size]\n",
    "            since this must be a Categorical cpd for each variable Y_i, \n",
    "            and we have one such rv for each step of each and every batched word-sequence.\n",
    "        \"\"\"\n",
    "        # [batch_size, max_length, H]\n",
    "        u = self.encoder(x)\n",
    "        # [batch_size, max_length, tagset_size]\n",
    "        s = self.logits_predictor(u)\n",
    "        return td.Categorical(logits=s)\n",
    "\n",
    "    def greedy(self, x):\n",
    "        \"\"\"\n",
    "        For each cpd Y[i]|x[1]...x[l], predicts the mode of the cpd.\n",
    "\n",
    "        Because this is a parallel tagger (i.e., we do not use information from y[<i]), \n",
    "         we can perform this in a very efficient way. See below.\n",
    "        \n",
    "        :param x: [batch_size, max_length]\n",
    "        :return: tag sequences [batch_size, max_length]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        max_length = x.shape[1]\n",
    "        with torch.no_grad(): # we do not need gradients for test-time predictions\n",
    "            cpds = self(x, y_in=None)  # parallel taggers do not have tag history\n",
    "            # [batch_size, max_length]\n",
    "            y_pred = torch.argmax(cpds.probs, -1)\n",
    "            # if a position in x is padded, it should be padded in y\n",
    "            y_pred = torch.where(x != self.word_vocab.pad_id, y_pred, torch.zeros_like(y_pred) + self.tag_vocab.pad_id)\n",
    "            return y_pred\n",
    "\n",
    "    def sample(self, x, sample_size=None):\n",
    "        \"\"\"\n",
    "        Per word sequence in the batch, draws a number of samples from the model, each sample is a complete tag sequence.\n",
    "\n",
    "        Because this is a parallel tagger (i.e., we do not use information from y[<i]), \n",
    "         we can perform this in a very efficient way, whereby we sample each tag independently. \n",
    "         See below.\n",
    "\n",
    "        :param x: [batch_size, max_len]\n",
    "\n",
    "        :return: tag sequences with shape [batch_size, max_len] if sample_size is None\n",
    "            else with shape [sample_size, batch_size, max_len]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        max_length = x.shape[1]\n",
    "        with torch.no_grad(): # we do not need gradients for test-time sampling\n",
    "            cpds = self(x, y_in=None)  # parallel taggers do not have tag history\n",
    "            if sample_size is None: \n",
    "                shape = (1,)  # this is needed for sampling a tensor of the correct shape\n",
    "            else:\n",
    "                shape = (sample_size,)\n",
    "            # [sample_size, batch_size, max_length]\n",
    "            y_pred = cpds.sample(shape)\n",
    "            # if a position in x is padding, it must be padded in y too\n",
    "            y_pred = torch.where(x.unsqueeze(0) != self.word_vocab.pad_id, y_pred, torch.zeros_like(y_pred) + self.tag_vocab.pad_id)\n",
    "            # takes care of output shape\n",
    "            if sample_size is None:\n",
    "                return y_pred.squeeze(0)\n",
    "            else:\n",
    "                return y_pred\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We coded a few tests, so you can debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_parallel_tagger(bidirectional=False):\n",
    "    seed_all()\n",
    "\n",
    "    # Construct the relevant model\n",
    "    if not bidirectional:\n",
    "        toy_uni_tagger = ParallelTagger(\n",
    "            encoder=BasicEncoder(\n",
    "                word_vocab=word_vocab,\n",
    "                word_embed_dim=32,\n",
    "            ),\n",
    "            tag_vocab=tag_vocab,\n",
    "            hidden_size=32\n",
    "        )\n",
    "    else:\n",
    "        toy_uni_tagger = ParallelTagger(\n",
    "            encoder=BidirectionalEncoder(\n",
    "                word_vocab=word_vocab,\n",
    "                word_embed_dim=32,\n",
    "                cell_size=32\n",
    "            ),\n",
    "            tag_vocab=tag_vocab,\n",
    "            hidden_size=32\n",
    "        )\n",
    "    print(toy_uni_tagger) \n",
    "    print(f\"Number of trainable parameters: {toy_uni_tagger.num_parameters()}\\n\")\n",
    "\n",
    "\n",
    "    # Here's a batch with 2 sentences\n",
    "    x = torch.from_numpy(word_vocab.batch_encode(training_x[:2], add_eos=True))\n",
    "    y_in = torch.from_numpy(tag_vocab.batch_encode(training_y[:2], add_bos=True))\n",
    "    y_out = torch.from_numpy(tag_vocab.batch_encode(training_y[:2], add_eos=True))\n",
    "    \n",
    "    # Is the forward returning a td.Categorical object?        \n",
    "    assert type(toy_uni_tagger(x)) is td.Categorical, \"Is the forward returning a td.Categorical object?\"\n",
    "\n",
    "    # Is log_prob returning the right shape?\n",
    "    assert toy_uni_tagger.log_prob(x, None, y_out).shape == (2,), \"Is log_prob returning the right shape?\"\n",
    "    assert torch.all(toy_uni_tagger.log_prob(x, y_in, y_out) == toy_uni_tagger.log_prob(x, None, y_out)) , \"A Parallel tagger should not let y_in affect results\"\n",
    "\n",
    "    assert toy_uni_tagger.loss(x, None, y_out).shape == tuple(), \"Is loss returning the right shape?\"\n",
    "\n",
    "    assert toy_uni_tagger.loss(x, None, y_out) == toy_uni_tagger.loss(x, y_in, y_out), \"A Parallel tagger should not let y_in affect results\"\n",
    "\n",
    "    assert toy_uni_tagger.sample(x).shape == word_vocab.batch_encode(x.tolist()).shape, \"Please don't change anything in sample\"\n",
    "\n",
    "    assert toy_uni_tagger.sample(x, 3).shape == (3,) + word_vocab.batch_encode(x.tolist()).shape, \"Please don't change anything in sample\"\n",
    "\n",
    "    assert toy_uni_tagger.greedy(x).shape == word_vocab.batch_encode(x.tolist()).shape, \"Please don't change anything in sample\"\n",
    "    \n",
    "\n",
    "test_parallel_tagger()\n",
    "test_parallel_tagger(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Decoder'></a>\n",
    "## Decoder\n",
    "\n",
    "The tool that encodes a _history_ is typically referred to as a **decoder**.\n",
    "We call it decoder in order to remind ourselves of the fact that for this component the notion of what is considered _past_ (or _already available_ for conditioning) relative to what's considered _future_ (or _not yet available_ for conditioning) is important. In other words, the _decoder_ can only represent information that would be available, in principle, at generation time. Don't be confused: during training we already have access to complete tag-sequences, but our models are designed as if we didn't, afterall, we want to use models precisely for predictive tasks.\n",
    "\n",
    "The design of the decoder depends a lot of the kinds of assumptions we make about how we condition on history. So let's start with a general template, that we will specialise later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The decoder is the tool that encoders the available history (so we can generate/decode the next step).\n",
    "    \n",
    "    That is, if we are given a delayed tag-sequence (that's relative to position i, it stores past), \n",
    "     and we encode it in a single vector. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tag_vocab, output_dim):\n",
    "        \"\"\"\n",
    "        :param output_dim: the dimensionality of the output state (which represents the history)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._tag_vocab = tag_vocab\n",
    "        self._output_dim = output_dim\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        \"\"\"Return dimensionality of the history representation\"\"\"\n",
    "        return self._output_dim    \n",
    "\n",
    "    @property\n",
    "    def tag_vocab(self):\n",
    "        \"\"\"Vocabulary of known tags\"\"\"\n",
    "        return self._tag_vocab    \n",
    "\n",
    "    def num_parameters(self, trainable_only=True):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the model\n",
    "\n",
    "        :param trainable_only: change to False to count all parameters (even those in frozen layers)\n",
    "        \"\"\"\n",
    "        if trainable_only:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters() if theta.requires_grad)\n",
    "        else:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters())\n",
    "\n",
    "    def device(self):\n",
    "        \"\"\"Torch device where decoder is stored\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "        \n",
    "    def forward(self, y_in):\n",
    "        \"\"\"\n",
    "        In a concrete implementation, this encodes batched histories.\n",
    "        That is, for each history y[<i] in a batch, we compute a representation h[i].\n",
    "        \n",
    "        :param y_in: a batch of histories (ie, delayed tag-sequences, those tokenised with add_bos=True)\n",
    "            [batch_size, max_length]\n",
    "        :return: a tensor representing all histories\n",
    "            [batch_size, max_length, output_dim]\n",
    "        \"\"\"\n",
    "        # [batch_size, max_length, output_dim]\n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before we implement a concrete instance of Decoder, let's talk about sequential tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Sequential_Tagging'></a>\n",
    "## Sequential Tagging\n",
    "\n",
    "With both an Encoder and a Decoder we can specify taggers that make no or fewer conditional independence assumptions. These taggers compute output probabilities for $Y_i$ given $x_{1:l}$ and information from the history $y_{<i}$ via:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf g(x_{1:l}, y_{<i}; \\theta) &= \\mathrm{softmax}(\\mathbf s_i)\\\\\n",
    "\\quad\\text{where}&\\\\\n",
    "\\mathbf s_i &= \\mathrm{ffnn}_C(\\mathbf v_i; \\theta_{\\text{out}})\\\\\n",
    "\\mathbf v_i &= \\mathrm{concat}(\\mathbf u_i, \\mathbf h_i)\\\\\n",
    "\\mathbf h_i &= \\mathrm{decoder}_K(y_{<i}; \\theta_{\\text{dec}})\\\\\n",
    "\\mathbf u_{1:l} &= \\mathrm{encoder}_H(x_{1:l}; \\theta_{\\text{enc}})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-4'></a> **Ungraded Exercise 4 - SequentialTagger**\n",
    "\n",
    "Complete the `SequentialTagger` below (constructor and forward). We have provided a few tests to help you debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class SequentialTagger(Tagger):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, hidden_size: int, p_drop=0.):\n",
    "        \"\"\"\n",
    "        :param encoder: encodes the tokens in the word-sequence\n",
    "        :param decoder: encoders the history\n",
    "        :param hidden_size: hidden size for the FFNN output layer\n",
    "        :param p_drop: dropout rate for regularisation before linear layers\n",
    "        \"\"\"        \n",
    "        super().__init__(encoder.word_vocab, decoder.tag_vocab)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        # **EXERCISE**\n",
    "        # 1. Construct the FFNN to predict logits\n",
    "        # from the concatenation of the encoder and decoder states        \n",
    "        self.logits_predictor = None\n",
    "\n",
    "    def device(self):\n",
    "        return self.encoder.device()\n",
    "\n",
    "    def forward(self, x, y_in):\n",
    "        \"\"\"\n",
    "        Executes the steps in the Section Sequential Tagging:\n",
    "            u[1]...u[l] = encoder(x[1]...x[l])\n",
    "            h[i] = decoder(y[<i])\n",
    "            v[i] = concat(u[i], h[i])\n",
    "            s[i] = ffnn(v[i])\n",
    "            g(x[1]...x[l], y[<i]) = softmax(s[i])\n",
    "        Then return a Categorical(g(x[1]...x[l], y[<i])) object. \n",
    "\n",
    "        Of course, it should do this for batches of inputs.\n",
    "\n",
    "        :param x: batched word_sequences (tokenised with add_eos=True)\n",
    "            [batch_size, max_length]\n",
    "        :param y_in: batched histories (tokenised with add_bos=True, so that it's delayed w.r.t. y_out)\n",
    "            [batch_size, max_length]\n",
    "        :return: a td.Categorical object whose logit has shape\n",
    "            [batch_size, max_length, tag_vocab_size]\n",
    "            since this must be a Categorical cpd for each variable Y_i, \n",
    "            and we have one such rv for each step of each and every batched word-sequence.\n",
    "        \"\"\"\n",
    "        # **EXERCISE**\n",
    "        # 2. Implement the mapping from (x, y_in) to logits\n",
    "        # 3. Return a batch of Categorical objects parameterised by those logits\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def _sample(self, x, greedy=False):\n",
    "        \"\"\"\n",
    "        Draws a single tag-sequence for each input word-sequence\n",
    "        from the cpds the model predict iteratively from left-to-right.\n",
    "        \n",
    "        In a sequential tagger, we cannot perform this in parallel, instead\n",
    "            we iterate with a for loop making predictions per step i of x.\n",
    "            Onl after we obtain the prediction for step i and make a decision (e.g., draw a sample or\n",
    "            use the mode/argmax), we can continue to the next step.\n",
    "\n",
    "        We impose a maximum number of steps, to avoid infinite loops.\n",
    "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
    "\n",
    "        :param x: batched word-sequences [batch_size, max_length]\n",
    "        :param greedy: use True to replace sampling from each cpd by argmax from each cpd.\n",
    "        :return: a single tag-sequence per word-sequence in x\n",
    "            either this is a sample from the cpds or the argmax from the cpds\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        max_length = x.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # add the beginning we do not know the tag sequence\n",
    "            # but NNs work with fixed dimensional tensors, \n",
    "            # so we allocate a tensor full of BOS codes\n",
    "            y_out = torch.full((batch_size, max_length), self.tag_vocab.pad_id, device=self.device())\n",
    "            bos = torch.full((batch_size, 1), self.tag_vocab.bos_id, device=self.device())\n",
    "            # Per step\n",
    "            for i in range(max_length):\n",
    "                # we parameterise a cpd for Y[i]|X=x\n",
    "                # note that the forward method takes care of not conditioning on y[i] itself\n",
    "                # and only using the ngram_size-1 previous tags\n",
    "                # at this point, the tag y[i] is a dummy code\n",
    "                # the forward method recomputes all cds in the batch, this will include the cpd for Y[i]\n",
    "                # (y_in is as long as x, which is a bit wasteful given that we haven't reached the end yet\n",
    "                #  but that's the simplest strategy, something more efficient\n",
    "                #  would require a lot more code)\n",
    "                y_in = torch.cat([bos, y_out[:,:-1]], 1)\n",
    "                # [batch_size, max_len, C] \n",
    "                cpds = self(x, y_in)\n",
    "                if greedy:\n",
    "                    # we get their modes via argmax\n",
    "                    # [batch_size, max_len]\n",
    "                    y_i = torch.argmax(cpds.probs, -1)[:, i]\n",
    "                else:\n",
    "                    # we draw samples\n",
    "                    # [batch_size, max_len]\n",
    "                    y_i = cpds.sample()[:, i]\n",
    "                \n",
    "                # Here we update the current token to the freshly obtained mode\n",
    "                #  and also replace the token by 0 (pad) in case the sentence is already complete\n",
    "                y_out[:, i] = y_i\n",
    "            # where we had a PAD token in x, we change the y token to PAD too\n",
    "            y_out = torch.where(x != self.word_vocab.pad_id, y_out, torch.zeros_like(y_out) + self.tag_vocab.pad_id)\n",
    "            \n",
    "            return y_out\n",
    "\n",
    "\n",
    "    def sample(self, x, sample_size=None):\n",
    "        \"\"\"\n",
    "        Draws a number of samples from the model, each sample is a complete tag-sequence for each word-sequence.\n",
    "        We impose a maximum number of steps, to avoid infinite loops.\n",
    "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
    "\n",
    "        :param x: batched word-sequences [batch_size, max_length]\n",
    "        :return: sampled tag sequences with shape [batch_size, max_length] if sample_size is None\n",
    "            or shape [sample_size, batch_size, max_length] otherwise\n",
    "        \"\"\"\n",
    "        if sample_size is None:\n",
    "            return self._sample(x)\n",
    "        else:\n",
    "            samples = [self._sample(x) for _ in range(sample_size)]\n",
    "            return torch.stack(samples)\n",
    "\n",
    "    def greedy(self, x):\n",
    "        \"\"\"\n",
    "        For each word-sequence in the batch, compute a tag-sequence made of greedily \n",
    "         choosing the mode/argmax of the cpd for each position, in a sequential manner.\n",
    "         We implement this as a simple modification to _sample, see below.\n",
    "         \n",
    "        :param x: batched word-sequences [batch_size, max_length]\n",
    "        :return: a single tag-sequence per word-sequence in x\n",
    "            [batch_size, max_length]\n",
    "        \"\"\"\n",
    "        return self._sample(x, greedy=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "class SequentialTagger(Tagger):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, hidden_size: int, p_drop=0.):\n",
    "        \"\"\"\n",
    "        :param encoder: encodes the tokens in the word-sequence\n",
    "        :param decoder: encoders the history\n",
    "        :param hidden_size: hidden size for the FFNN output layer\n",
    "        :param p_drop: dropout rate for regularisation before linear layers\n",
    "        \"\"\"        \n",
    "        super().__init__(encoder.word_vocab, decoder.tag_vocab)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.logits_predictor = nn.Sequential(\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(encoder.output_dim + decoder.output_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_size, len(decoder.tag_vocab)),\n",
    "        ) \n",
    "\n",
    "    def device(self):\n",
    "        return self.encoder.device()\n",
    "\n",
    "    def forward(self, x, y_in):\n",
    "        \"\"\"\n",
    "        Executes the steps in the Section Sequential Tagging:\n",
    "            u[1]...u[l] = encoder(x[1]...x[l])\n",
    "            h[i] = decoder(y[<i])\n",
    "            v[i] = concat(u[i], h[i])\n",
    "            s[i] = ffnn(v[i])\n",
    "            g(x[1]...x[l], y[<i]) = softmax(s[i])\n",
    "        Then return a Categorical(g(x[1]...x[l], y[<i])) object. \n",
    "\n",
    "        Of course, it should do this for batches of inputs.\n",
    "\n",
    "        :param x: batched word_sequences (tokenised with add_eos=True)\n",
    "            [batch_size, max_length]\n",
    "        :param y_in: batched histories (tokenised with add_bos=True, so that it's delayed w.r.t. y_out)\n",
    "            [batch_size, max_length]\n",
    "        :return: a td.Categorical object whose logit has shape\n",
    "            [batch_size, max_length, tag_vocab_size]\n",
    "            since this must be a Categorical cpd for each variable Y_i, \n",
    "            and we have one such rv for each step of each and every batched word-sequence.\n",
    "        \"\"\"\n",
    "        u = self.encoder(x)\n",
    "        h = self.decoder(y_in)\n",
    "        v = torch.cat([u, h], -1)\n",
    "        s = self.logits_predictor(v)\n",
    "        return td.Categorical(logits=s)\n",
    "\n",
    "    def _sample(self, x, greedy=False):\n",
    "        \"\"\"\n",
    "        Draws a single tag-sequence for each input word-sequence\n",
    "        from the cpds the model predict iteratively from left-to-right.\n",
    "        \n",
    "        In a sequential tagger, we cannot perform this in parallel, instead\n",
    "            we iterate with a for loop making predictions per step i of x.\n",
    "            Onl after we obtain the prediction for step i and make a decision (e.g., draw a sample or\n",
    "            use the mode/argmax), we can continue to the next step.\n",
    "\n",
    "        We impose a maximum number of steps, to avoid infinite loops.\n",
    "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
    "\n",
    "        :param x: batched word-sequences [batch_size, max_length]\n",
    "        :param greedy: use True to replace sampling from each cpd by argmax from each cpd.\n",
    "        :return: a single tag-sequence per word-sequence in x\n",
    "            either this is a sample from the cpds or the argmax from the cpds\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        max_length = x.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # add the beginning we do not know the tag sequence\n",
    "            # but NNs work with fixed dimensional tensors, \n",
    "            # so we allocate a tensor full of BOS codes\n",
    "            y_out = torch.full((batch_size, max_length), self.tag_vocab.pad_id, device=self.device())\n",
    "            bos = torch.full((batch_size, 1), self.tag_vocab.bos_id, device=self.device())\n",
    "            # Per step\n",
    "            for i in range(max_length):\n",
    "                # we parameterise a cpd for Y[i]|X=x\n",
    "                # note that the forward method takes care of not conditioning on y[i] itself\n",
    "                # and only using the ngram_size-1 previous tags\n",
    "                # at this point, the tag y[i] is a dummy code\n",
    "                # the forward method recomputes all cds in the batch, this will include the cpd for Y[i]\n",
    "                # (y_in is as long as x, which is a bit wasteful given that we haven't reached the end yet\n",
    "                #  but that's the simplest strategy, something more efficient\n",
    "                #  would require a lot more code)\n",
    "                y_in = torch.cat([bos, y_out[:,:-1]], 1)\n",
    "                # [batch_size, max_len, C] \n",
    "                cpds = self(x, y_in)\n",
    "                if greedy:\n",
    "                    # we get their modes via argmax\n",
    "                    # [batch_size, max_len]\n",
    "                    y_i = torch.argmax(cpds.probs, -1)[:, i]\n",
    "                else:\n",
    "                    # we draw samples\n",
    "                    # [batch_size, max_len]\n",
    "                    y_i = cpds.sample()[:, i]\n",
    "                \n",
    "                # Here we update the current token to the freshly obtained mode\n",
    "                #  and also replace the token by 0 (pad) in case the sentence is already complete\n",
    "                y_out[:, i] = y_i\n",
    "            # where we had a PAD token in x, we change the y token to PAD too\n",
    "            y_out = torch.where(x != self.word_vocab.pad_id, y_out, torch.zeros_like(y_out) + self.tag_vocab.pad_id)\n",
    "            \n",
    "            return y_out\n",
    "\n",
    "\n",
    "    def sample(self, x, sample_size=None):\n",
    "        \"\"\"\n",
    "        Draws a number of samples from the model, each sample is a complete tag-sequence for each word-sequence.\n",
    "        We impose a maximum number of steps, to avoid infinite loops.\n",
    "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
    "\n",
    "        :param x: batched word-sequences [batch_size, max_length]\n",
    "        :return: sampled tag sequences with shape [batch_size, max_length] if sample_size is None\n",
    "            or shape [sample_size, batch_size, max_length] otherwise\n",
    "        \"\"\"\n",
    "        if sample_size is None:\n",
    "            return self._sample(x)\n",
    "        else:\n",
    "            samples = [self._sample(x) for _ in range(sample_size)]\n",
    "            return torch.stack(samples)\n",
    "\n",
    "    def greedy(self, x):\n",
    "        \"\"\"\n",
    "        For each word-sequence in the batch, compute a tag-sequence made of greedily \n",
    "         choosing the mode/argmax of the cpd for each position, in a sequential manner.\n",
    "         We implement this as a simple modification to _sample, see below.\n",
    "         \n",
    "        :param x: batched word-sequences [batch_size, max_length]\n",
    "        :return: a single tag-sequence per word-sequence in x\n",
    "            [batch_size, max_length]\n",
    "        \"\"\"\n",
    "        return self._sample(x, greedy=True)\n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We coded a few tests so you can debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDecoder(Decoder):\n",
    "    \"\"\"\n",
    "    This class implements a Decoder that always returns a tensor of zeros. \n",
    "    Clearly, this is not useful for any application.\n",
    "    We introduce it here, just so we can test the forward method of our SequentialTagger.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tag_vocab: Vocabulary, output_dim):\n",
    "        \"\"\"        \n",
    "        :param tag_vocab: vocabulary of known tags       \n",
    "        :param output_dim: size of the vector of zeros that's used\n",
    "          as a token representation  (for testing the SequentialTagger)\n",
    "        \"\"\"\n",
    "        # output dim: we return the concatenation of (n-1) vectors of size D\n",
    "        super().__init__(tag_vocab, output_dim)                \n",
    "\n",
    "    def device(self):\n",
    "        \"\"\"Torch device where parameters are stored\"\"\"\n",
    "        raise None    \n",
    "\n",
    "    def forward(self, y_in):\n",
    "        \"\"\"\n",
    "        Encodes batched histories as vectors of zeros (this is not used in practice, \n",
    "         we just use it to test the implementation of the sequential tagger).\n",
    "        \n",
    "        :param y_in: a batch of histories (ie, delayed tag-sequences, those tokenised with add_bos=True)\n",
    "            [batch_size, max_length]\n",
    "        :return: a tensor of 0s\n",
    "            [batch_size, max_length, output_dim]\n",
    "        \"\"\"\n",
    "        # a tensor of zeros with shape [batch_size, max_length, output_dim]\n",
    "        return torch.zeros(y_in.shape + (self.output_dim,), dtype=torch.float, device=y_in.device)\n",
    "\n",
    "\n",
    "def test_sequential_tagger(bidirectional=False, decoder=TestDecoder(tag_vocab, 2)):\n",
    "    seed_all()\n",
    "\n",
    "    # Construct the relevant model\n",
    "    if not bidirectional:\n",
    "        toy_seq_tagger = SequentialTagger(\n",
    "            encoder=BasicEncoder(\n",
    "                word_vocab=word_vocab,\n",
    "                word_embed_dim=32,\n",
    "            ),\n",
    "            decoder=decoder,\n",
    "            hidden_size=50\n",
    "        )\n",
    "    else:\n",
    "        toy_seq_tagger = SequentialTagger(\n",
    "            encoder=BidirectionalEncoder(\n",
    "                word_vocab=word_vocab,\n",
    "                word_embed_dim=32,\n",
    "                cell_size=20,\n",
    "            ),\n",
    "            decoder=decoder,\n",
    "            hidden_size=50\n",
    "        )\n",
    "    print(toy_seq_tagger) \n",
    "    print(f\"Number of parameters: {toy_seq_tagger.num_parameters()}\\n\")\n",
    "\n",
    "\n",
    "    # Here's a batch with 2 sentences\n",
    "    x = torch.from_numpy(word_vocab.batch_encode(training_x[:2], add_eos=True))\n",
    "    y_in = torch.from_numpy(tag_vocab.batch_encode(training_y[:2], add_bos=True))\n",
    "    y_out = torch.from_numpy(tag_vocab.batch_encode(training_y[:2], add_eos=True))\n",
    "    \n",
    "    # Is the forward returning a td.Categorical object?        \n",
    "    assert type(toy_seq_tagger(x, y_in)) is td.Categorical, \"Is the forward returning a td.Categorical object?\"\n",
    "\n",
    "    # Is log_prob returning the right shape?\n",
    "    assert toy_seq_tagger.log_prob(x, y_in, y_out).shape == (2,), \"Is log_prob returning the right shape?\"    \n",
    "\n",
    "    assert toy_seq_tagger.loss(x, y_in, y_out).shape == tuple(), \"Is loss returning the right shape?\"\n",
    "\n",
    "    assert toy_seq_tagger.sample(x).shape == word_vocab.batch_encode(x.tolist()).shape, \"Please don't change anything in sample\"\n",
    "\n",
    "    assert toy_seq_tagger.sample(x, 3).shape == (3,) + word_vocab.batch_encode(x.tolist()).shape, \"Please don't change anything in sample\"\n",
    "\n",
    "    assert toy_seq_tagger.greedy(x).shape == word_vocab.batch_encode(x.tolist()).shape, \"Please don't change anything in sample\"\n",
    "    \n",
    "\n",
    "test_sequential_tagger()\n",
    "test_sequential_tagger(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Markov_Tagger'></a>\n",
    "### Markov Tagger\n",
    "\n",
    "Our first concrete sequential tagger makes a Markov assumption, modelling the tag-sequence using NGrams.\n",
    "This model assigns probability \n",
    "\\begin{align}\n",
    "    P(Y_{1:L}=y_{1:l}|X_{1:L}=x_{1:l}) &= \\prod_{i=1}^l P(Y_i=y_i|X_{1:L}=x_{1:l}, Y_{<i}=y_{<i})\\\\\n",
    "    &\\overset{\\text{ind.}}{=} \\prod_{i=1}^l P(Y_i=y_i|X_{1:L}=x_{1:l}, H_i=y_{i-n+1:i-1})\\\\\n",
    "\\end{align}\n",
    "to the tag-sequence $y_{1:l}$ given the word-sequence $x_{1:l}$, while assuming that each tag $Y_i$ is independent of all but the $n-1$ previous tags $y_{i-n+1:i-1}$.\n",
    "\n",
    "To implement this model we need an NGram decoder:\n",
    "\\begin{align}\n",
    "\\mathrm{decoder}_K(y_{<i}; \\theta_{\\text{dec}}) &= \\mathbf h_i\\\\\n",
    "\\quad\\text{where}&\\\\\n",
    "\\mathbf h_i &= \\mathrm{concat}(\\mathbf t_{i-n+1}, \\ldots, \\mathbf t_{i-1})\\\\\n",
    "\\mathbf t_k &= \\mathrm{embed}_{D_2}(y_k; \\theta_{\\text{tags}}) & k \\in \\{i-n+1, \\ldots, i-1\\}\n",
    "\\end{align}\n",
    "which embeds the $n-1$ tags in the history and returns the concatenation of these embeddings as a representation of the history of the $i$th tag. As we can see, its output dimensionality is $K=(n-1)\\times D$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-3'> **Graded Exercise 3 - NGramDecoder** </a>\n",
    "\n",
    "Complete the `NGramDecoder` below (constructor and forward). We have provided a few tests to help you debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class NGramDecoder(Decoder):\n",
    "\n",
    "    def __init__(self, tag_vocab: Vocabulary, ngram_size, tag_embed_dim: int):\n",
    "        \"\"\"        \n",
    "        :param tag_vocab: vocabulary of known tags       \n",
    "        :param ngram_size: size of the NGrams we are modelling with \n",
    "            (the history will contain ngram_size-1 tokens)\n",
    "        :param tag_embed_dim: dimensionality of tag embeddings (for tags in history)        \n",
    "        \"\"\"\n",
    "        # output dim: we return the concatenation of (n-1) vectors of size D\n",
    "        super().__init__(tag_vocab, (ngram_size - 1) * tag_embed_dim)        \n",
    "        self.ngram_size = ngram_size\n",
    "        self.tag_embed_dim = tag_embed_dim      \n",
    "        \n",
    "        # **EXERCISES**\n",
    "        # 1. Construct an embedding layer for tags        \n",
    "        self._tag_embed = None\n",
    "\n",
    "    def device(self):\n",
    "        \"\"\"Torch device where parameters are stored\"\"\"\n",
    "        raise self._tag_embed.weight.device\n",
    "    \n",
    "    def make_ngrams(self, y_in):\n",
    "        \"\"\"\n",
    "        Return a batch of ngram histories for conditioning the next-tag cpds.\n",
    "\n",
    "        Example with ngram_size=3:\n",
    "\n",
    "        y_in: \n",
    "        [\n",
    "            [BOS, DET,  NOUN, VERB ],\n",
    "            [BOS, VERB, PAD,  PAD],\n",
    "        ]\n",
    "\n",
    "        # here the ngrams associated with y_in\n",
    "        # it's also useful to remember what the corresponding\n",
    "        #  y_out would be (we indicate it after '#')\n",
    "        ngrams:\n",
    "        [\n",
    "            [\n",
    "                [BOS,  BOS],   # DET\n",
    "                [BOS,  DET],   # NOUN\n",
    "                [DET,  NOUN],  # VERB\n",
    "                [NOUN, VERB],  # EOS                \n",
    "            ],\n",
    "            [\n",
    "                [BOS,  BOS],   # VERB\n",
    "                [BOS,  VERB],  # EOS\n",
    "                [VERB, PAD],   # PAD (this position isn't real)\n",
    "                [PAD,  PAD],   # PAD (this position isn't real)                \n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        :param y_in: batched histories (i.e., tag sequences batched with add_bos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "        :return: batched ngram histories\n",
    "            with shape [batch_size, max_length, ngram_size - 1]\n",
    "        \"\"\"\n",
    "        y_shape = y_in.shape\n",
    "        # We might need to add more BOS symbols to y_in\n",
    "        # (remember, y_in already has one)\n",
    "        if self.ngram_size > 2:\n",
    "            # [batch_size, ngram_size - 2]\n",
    "            bos = torch.full((y_in.shape[0], self.ngram_size - 2), self.tag_vocab.bos_id, device=y_in.device)\n",
    "            # [batch_size, max_length + ngram_size - 2]\n",
    "            _y = torch.cat([bos, y_in], 1)\n",
    "        else:\n",
    "            _y = y_in\n",
    "\n",
    "        # For each output step, we will have ngram_size - 1 inputs, so we collect those from y\n",
    "        # [batch_size, max_length, ngram_size - 1]\n",
    "        return torch.cat([_y.unsqueeze(-1)[:,i:i+self.ngram_size-1].reshape(y_shape[0], 1, -1) for i in range(y_shape[1])], 1)\n",
    "\n",
    "    def forward(self, y_in):\n",
    "        \"\"\"\n",
    "        Parameterise the conditional distributions over Y[i] given a truncated history \n",
    "            based on y[<i] and all of x. The truncated history contains ngram_size-1 tokens.\n",
    "\n",
    "        It should execute the steps explained in Section Markov Tagger (see decoder), for each i:\n",
    "\n",
    "            decoder(y[<i]) = h[i]\n",
    "                where \n",
    "                    h[i] = concat(t[i-n+1],..., t[i-1])\n",
    "                    t[k] = embed(y[k]) for k in {i-n+1,...,i-1}\n",
    "\n",
    "        :param x: batched word_sequences (tokenised with add_eos=True)\n",
    "            [batch_size, max_length]\n",
    "        :param y_in: batched histories (tokenised with add_bos=True, so that it's delayed w.r.t. y_out)\n",
    "            [batch_size, max_length]\n",
    "        :return: a td.Categorical object whose logit has shape\n",
    "            [batch_size, max_length, tag_vocab_size]\n",
    "            since this must be a Categorical cpd for each variable Y_i, \n",
    "            and we have one such rv for each step of each and every batched word-sequence.\n",
    "        \"\"\"\n",
    "        # [batch_size, max_length, ngram_size - 1]\n",
    "        history = self.make_ngrams(y_in)        \n",
    "        \n",
    "        # **EXERCISE**\n",
    "        # 2. Compute and return the representation of each history        \n",
    "        raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We coded some test so you can debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_ngram_decoder(ngram_size=2, tag_embed_dim=12):\n",
    "    seed_all()    \n",
    "    \n",
    "    ngram_decoder = NGramDecoder(\n",
    "        tag_vocab=tag_vocab,\n",
    "        ngram_size=ngram_size,            \n",
    "        tag_embed_dim=tag_embed_dim,            \n",
    "    )\n",
    "    print(ngram_decoder)\n",
    "    print(f\"Number of parameters: {ngram_decoder.num_parameters()}\\n\")\n",
    "\n",
    "    # here we test your constructor\n",
    "\n",
    "    # testing shape\n",
    "    output_dim = (ngram_size-1) * tag_embed_dim\n",
    "    assert ngram_decoder.output_dim == output_dim, \"Are you sure you are output the right number of units per token?\"\n",
    "\n",
    "    # testing embedding layer\n",
    "    assert type(ngram_decoder._tag_embed) is nn.Embedding, \"An embedding layer is an instance of nn.Embedding\"\n",
    "    \n",
    "    # here we test the shape of your embedding matrix\n",
    "    matrix_shape = ngram_decoder._tag_embed.weight.shape\n",
    "    assert matrix_shape == (len(tag_vocab), tag_embed_dim), f\"An embedding layer should store one vector per tag in the tagset, got {matrix_shape} instead\"\n",
    "\n",
    "\n",
    "    # Here's a batch with 2 histories (not how they are delayed via add_bos=True)\n",
    "    y_in = torch.from_numpy(tag_vocab.batch_encode(training_y[:2], add_bos=True))    \n",
    "    # let's test make_ngrams (which you need not change)\n",
    "    assert ngram_decoder.make_ngrams(y_in).shape == y_in.shape + (ngram_size - 1,), \"Did you change make_ngrams?\"\n",
    "    \n",
    "    # now we test your forward method\n",
    "    assert ngram_decoder(y_in).shape == y_in.shape + (output_dim,), \"Are you sure you are concatenating all relevant embeddings in the history?\"\n",
    "\n",
    "    # Test number of parameters\n",
    "    n1 = NGramDecoder(\n",
    "        tag_vocab=tag_vocab,\n",
    "        ngram_size=ngram_size,            \n",
    "        tag_embed_dim=tag_embed_dim,            \n",
    "    ).num_parameters()\n",
    "    n2 = NGramDecoder(\n",
    "        tag_vocab=tag_vocab,\n",
    "        ngram_size=ngram_size * 2,            \n",
    "        tag_embed_dim=tag_embed_dim,            \n",
    "    ).num_parameters()\n",
    "    assert n1 == n2, \"The number of parameters of this decoder should be independent of NGram size\"\n",
    "    \n",
    "test_ngram_decoder(2, 12)\n",
    "test_ngram_decoder(3, 12)\n",
    "test_ngram_decoder(4, 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's now test a Markov tagger: that is a sequential tagger that uses the NGram decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ngram_size in [2, 3]:  # let's test different ngram sizes\n",
    "    test_sequential_tagger(\n",
    "        bidirectional=False,  # let's use the basic encoder\n",
    "        # and the NGramDecoder\n",
    "        decoder=NGramDecoder(\n",
    "            tag_vocab=tag_vocab,\n",
    "            ngram_size=ngram_size,            \n",
    "            tag_embed_dim=12,            \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    test_sequential_tagger(\n",
    "        bidirectional=True,  # let's use the bidirectional encoder\n",
    "        # and the NGramDecoder\n",
    "        decoder=NGramDecoder(\n",
    "            tag_vocab=tag_vocab,\n",
    "            ngram_size=ngram_size,            \n",
    "            tag_embed_dim=12,            \n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Autoregressive_Tagger'></a>\n",
    "### Autoregressive Tagger\n",
    "\n",
    "Our second concrete sequential tagger **does not** make a Markov assumption, instead, modelling the tag-sequence autoregressively.\n",
    "This model assigns probability \n",
    "\\begin{align}\n",
    "    P(Y_{1:L}=y_{1:l}|X_{1:L}=x_{1:l}) &= \\prod_{i=1}^l P(Y_i=y_i|X_{1:L}=x_{1:l}, Y_{<i}=y_{<i})\n",
    "\\end{align}\n",
    "to the tag-sequence $y_{1:l}$ given the word-sequence $x_{1:l}$ and the complete history $y_{<i}$. \n",
    "\n",
    "To implement this model we need an autoregressive decoder:\n",
    "\\begin{align}\n",
    "\\mathrm{decoder}_K(y_{<i}; \\theta_{\\text{dec}}) &= \\mathbf h_i\\\\\n",
    "\\quad\\text{where}&\\\\\n",
    "\\mathbf h_i &= \\mathrm{rnnstep}_K(\\mathbf h_{i-1},  \\mathbf t_{i-1}; \\theta_{\\text{rnn}})\\\\\n",
    "\\mathbf t_k &= \\mathrm{embed}_{D}(y_k; \\theta_{\\text{tags}}) & k < i\n",
    "\\end{align}\n",
    "which embeds the tags in the history (all of them) and returns an RNN state that's delayed with respect to the output tag-sequence (that is, the state of an RNN used in _decoder_ mode, you can see how the input $\\mathbf t_{i-1}$ is delayed with respect to the output). This state stands as a representation of the full history to the $i$th tag. \n",
    "\n",
    "As we can see, the output dimensionality $K$ of this decoder the dimensionality of the RNN cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-4'> **Graded Exercise 4 - AutoregressiveDecoder** </a>\n",
    "\n",
    "Complete the `AutoregressiveDecoder` below (constructor and forward). We have provided a few tests to help you debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class AutoregressiveDecoder(Decoder):\n",
    "\n",
    "    def __init__(self, tag_vocab: Vocabulary, tag_embed_dim: int, cell_size:int):\n",
    "        \"\"\"         \n",
    "        :param tag_vocab: Vocabulary of known tags        \n",
    "        :param tag_embed_dim: dimensionality of tag embeddings (for tags in history)\n",
    "        :param cell_size: dimensionality of decoder RNN cell\n",
    "        :param t_drop: tag dropout (for regularisation)\n",
    "        \"\"\"\n",
    "        super().__init__(tag_vocab, cell_size)   \n",
    "        self.tag_embed_dim = tag_embed_dim\n",
    "        self.cell_size = cell_size        \n",
    "\n",
    "        # **EXERCISES**\n",
    "        # 1. Construct an embedding layer for tags\n",
    "        self._tag_embed = None\n",
    "        # 2. Construct an LSTM to be used in decoding mode\n",
    "        self._rnn = None\n",
    "\n",
    "    def device(self):\n",
    "        \"\"\"Torch device where the decoder is stored\"\"\"\n",
    "        raise self._tag_embed.weight.device\n",
    "        \n",
    "    def forward(self, y_in):\n",
    "        \"\"\"\n",
    "        Parameterise the conditional distributions over Y[i] given full history y[<i] and all of x.\n",
    "            \n",
    "        It should execute the steps explained in Section Autoregressive Tagger (see decoder), for each i:\n",
    "\n",
    "            decoder(y[<i]) = h[i]\n",
    "                where \n",
    "                    h[i] = rnnstep(h[i-1], t[i-1])\n",
    "                    t[k] = embed(y[k]) for k < i\n",
    "\n",
    "        :param x: batched word_sequences (tokenised with add_eos=True)\n",
    "            [batch_size, max_length]\n",
    "        :param y_in: batched histories (tokenised with add_bos=True, so that it's delayed w.r.t. y_out)\n",
    "            [batch_size, max_length]\n",
    "        :return: a td.Categorical object whose logit has shape\n",
    "            [batch_size, max_length, tag_vocab_size]\n",
    "            since this must be a Categorical cpd for each variable Y_i, \n",
    "            and we have one such rv for each step of each and every batched word-sequence.\n",
    "        \"\"\"        \n",
    "        # **EXERCISE**\n",
    "        # 3. Compute and return the LSTM representation of the histories\n",
    "        raise NotImplementedError(\"Implement me!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We coded some tests for you to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_ar_decoder(cell_size=10, tag_embed_dim=12):\n",
    "    seed_all()    \n",
    "    \n",
    "    ar_decoder = AutoregressiveDecoder(\n",
    "        tag_vocab=tag_vocab,\n",
    "        tag_embed_dim=tag_embed_dim,            \n",
    "        cell_size=cell_size,        \n",
    "    )\n",
    "    print(ar_decoder)\n",
    "    print(f\"Number of parameters: {ar_decoder.num_parameters()}\\n\")\n",
    "\n",
    "    # here we test your constructor\n",
    "\n",
    "    # testing embedding layer\n",
    "    assert type(ar_decoder._tag_embed) is nn.Embedding, \"An embedding layer is an instance of nn.Embedding\"\n",
    "    # here we test the shape of your embedding matrix\n",
    "    matrix_shape = ar_decoder._tag_embed.weight.shape\n",
    "    assert matrix_shape == (len(tag_vocab), tag_embed_dim), f\"An embedding layer should store one vector per tag in the tagset, got {matrix_shape} instead\"\n",
    "    \n",
    "    # testing LSTM\n",
    "    assert type(ar_decoder._rnn) is nn.LSTM, \"Are you using an LSTM?\"\n",
    "    assert not ar_decoder._rnn.bidirectional, \"Never for decoders!\"\n",
    "    assert ar_decoder._rnn.batch_first, \"In NLP our tensors are batch_first=True, always\"\n",
    "    \n",
    "    # testing shape\n",
    "    output_dim = cell_size\n",
    "    assert ar_decoder.output_dim == output_dim, \"Are you sure you producing the right number of units per token?\"\n",
    "    \n",
    "\n",
    "    # Here's a batch with 2 histories (note how they are delayed via add_bos=True)\n",
    "    y_in = torch.from_numpy(tag_vocab.batch_encode(training_y[:2], add_bos=True))        \n",
    "    \n",
    "    # now we test your forward method\n",
    "    assert ar_decoder(y_in).shape == y_in.shape + (output_dim,), \"Are you sure you are using the LSTM decoder correctly?\"\n",
    "    \n",
    "    \n",
    "test_ar_decoder(10, 20)\n",
    "test_ar_decoder(20, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's now test an Autoregressive tagger: that is a sequential tagger that uses the AutoregressiveDecoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sequential_tagger(\n",
    "    bidirectional=False,  # let's use the basic encoder\n",
    "    # and the AutoregressiveDecoder\n",
    "    decoder=AutoregressiveDecoder(\n",
    "        tag_vocab=tag_vocab,\n",
    "        tag_embed_dim=12,   \n",
    "        cell_size=24\n",
    "    )\n",
    ")\n",
    "\n",
    "test_sequential_tagger(\n",
    "    bidirectional=True,  # let's use the bidirectional encoder\n",
    "    # and the AutoregressiveDecoder\n",
    "    decoder=AutoregressiveDecoder(\n",
    "        tag_vocab=tag_vocab,\n",
    "        tag_embed_dim=12,   \n",
    "        cell_size=24\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "0R1QKLf6WWxT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Training_and_Evaluation'></a>\n",
    "# Training and Evaluation\n",
    "\n",
    "Now we will conduct an experiment with an actual corpus, we better use GPU support for that (on Google Colab you change the runtime to GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "pKFUw21bYCUm",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda:0')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "my_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Perplexity'></a>\n",
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "BZVMkq0GZOqc",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can evaluate sequence models intrinsically, using perplexity (as we did for language modelling, but now we apply to tag-sequences rather than word-sequences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "Foxt9hp0n5J1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perplexity(model: Tagger, dl, device):\n",
    "    \"\"\"\n",
    "    Every sequence model can be evaluated intrinsically in terms of perplexity.\n",
    "    Perplexity is very interpretable, a perplexity value `ppl` means\n",
    "        given the context available (which varies depending on the type of model you use)\n",
    "        the uncertainty of the model about the next token has been narrowed down \n",
    "        to `ppl` possible outputs (out of the C options available for tagging).\n",
    "\n",
    "    :param model: one of our taggers\n",
    "    :param dl: a data loader for the heldout data\n",
    "    :param device: the PyTorch device where the model is stored\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_tokens = 0\n",
    "    total_log_prob = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y_in, batch_y_out in dl:\n",
    "            total_tokens += (batch_x != model.word_vocab.pad_id).float().sum()\n",
    "            total_log_prob = total_log_prob + model.log_prob(batch_x.to(device), batch_y_in.to(device), batch_y_out.to(device)).sum()\n",
    "    return torch.exp(-total_log_prob / total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Decision_rules_and_tagging_accuracy'></a>\n",
    "## Decision rules and tagging accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "8yUftHTPkhMH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Because labelling is a chain of classification decisions, we can also evaluate our tagger in terms of accuracy of tagging decisions. For that we need a **decision rule**. Normally, in NLP, we use the **most probable tag sequence** as a decision. That is, given a sentence $x_{1:l}$ we search in the space $\\{1, \\ldots, C\\}^l$ of all tag sequences of length $l$, for the one sequence that the model assigns highest probability to (i.e., the *mode* of the conditional distribution):\n",
    "\n",
    "\\begin{align}\n",
    "y_{1:l}^\\star &= \\arg\\max_{c_{1:l} \\in \\{1, \\ldots, C\\}^l}~ \\log P(Y_{1:L}=c_{1:l}|X_{1:L}=x_{1:l})\n",
    "\\end{align}\n",
    "\n",
    "This search is defined over an extremely large space and is generally not tractable. For some types of tagger, because of their conditional independence assumptions, this search may be doable in polynomial time (as a function of sequence length), for others this is not at all possible. \n",
    "\n",
    "For the parallel tagger, which treats the tags as independent given the sentence, this search can be done exactly, because greedily maximising each step independently is equivalently to maximising the joint assignment of the entire sequence for that model.\n",
    "\n",
    "**Search for the parallel tagger**\n",
    "\n",
    "We search for the best tag in each position:\n",
    "\\begin{align}\n",
    "y^\\star_i &= \\arg\\max_{c \\in \\{1, \\ldots, C\\}}~ \\log P(Y_i=c|S=x_{1:l})\n",
    "\\end{align}\n",
    "and put them together in a sequence. Assuming the NN computations take 1 unit of time, the total operation takes time $\\mathcal O(l \\times C)$ on CPU (in a GPU the $l$ decisions can be parallelised in hardward, then the computation happens in time $\\mathcal O(C)$).\n",
    "\n",
    "**Search for the Markov tagger**\n",
    "\n",
    "The Markov tagger makes fewer conditional independence assumptions, and the search problem is a bit harder. Solving for each tag independently and concatenating the result will not give us the *true mode* of the conditional distribution. If we do that, we have a *greedy* approximation to the true mode. \n",
    "\n",
    "To search for the exact mode we need a special algorithm called *the Viterbi algorithm*, a type of *dynamic programming* algorithm that can solve the search efficiently. This is not within the scope of this course, but it is covered in the course _machine learning for structured data_ (in year 3).\n",
    "\n",
    "For this tutorial we will use the greedy approximation:\n",
    "\\begin{align}\n",
    "\\hat y_i &= \\arg\\max_{c \\in \\{1, \\ldots, C\\}}~ \\log P(Y_i=c|S=x_{1:l}, H=\\hat y_{i-n1+1:i-1})\n",
    "\\end{align}\n",
    "where we solve the argmax locally per tag in order from left-to-right. For each step $Y_i$ we condition on the already predicted argmax for the $n-1$ preceding steps.\n",
    "\n",
    "Once again, this is an approximation motivated by efficiency, not by correctness.\n",
    "\n",
    "Assuming the NN computations take 1 unit of time, the total operation takes time $\\mathcal O(l \\times C)$ on CPU or GPU (this time the $l$ decisions cannot be parallelised in a GPU because they actually depend on previous decisions being made in a sequential manner).\n",
    "\n",
    "**Search for the autoregressive tagger**\n",
    "\n",
    "The autoregressive tagger makes no conditional independence assumptions, and the search problem is genuinely intractable for this model. Being intractable means there is not efficient algorithm known to be able to handle it. In fact, the current hypothesis is that an efficient (by efficient we mean that it runs in polynomial time as a function of $l$) is actually impossible in standard computer architectures. Problems of this kind are called NP-complete.\n",
    "\n",
    "For this tutorial, we will again use the greedy approximation:\n",
    "\\begin{align}\n",
    "\\hat y_i &= \\arg\\max_{c \\in \\{1, \\ldots, C\\}}~ \\log P(Y_i=c|S=x_{1:l}, H=\\hat y_{<i})\n",
    "\\end{align}\n",
    "where we solve the argmax locally per position in order from left-to-right. For each step $Y_i$ we condition on the already predicted argmax for all the preceding steps.\n",
    "\n",
    "Other approximations do exist, some of them with better properties than this greedy one, but they are algorithmically more complex and not within scope for this course.\n",
    "\n",
    "Assuming the NN computations take 1 unit of time, the total operation takes time $\\mathcal O(l \\times C)$ on CPU or GPU (this time the $l$ decisions cannot be parallelised in a GPU because they actually depend on previous decisions being made in a sequential manner).\n",
    "\n",
    "---\n",
    "\n",
    "Once we have a search algorithm in place to make predictions we can compute accuracy and/or other metrics common for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "Vg4FRel3lv4U",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_untagged_corpus(corpus_x, word_vocab: Vocabulary, tag_vocab: Vocabulary):        \n",
    "    \"\"\"\n",
    "    This lets us use TaggedCorpus for an untagged corpus, by pairing each \n",
    "    token sequence with an equal length PAD-tag sequence.\n",
    "\n",
    "    You can use this to help you tag a new dataset for which you do not have tags (e.g., examples you crete yourself)\n",
    "\n",
    "    :param corpus_x: token sequences (e.g., a test set for which we do not know the tag sequence)\n",
    "    :param word_vocab: vocabulary of known words\n",
    "    :param tag_vocab: vocabulary of known tags\n",
    "\n",
    "    :return: the correspondin TaggedCorpus    \n",
    "    \"\"\"\n",
    "    # we use our typical TaggedCorpus, but the tag-sequences are just PAD (because we have not predicted them yet)\n",
    "    return TaggedCorpus(corpus_x, [[tag_vocab.pad_token] * len(seq) for seq in corpus_x], word_vocab, tag_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "BGzeKX-wv9NT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here we have a predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "zZV5c0WEhdk4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model: Tagger, dl, device, return_targets=False):\n",
    "    \"\"\"\n",
    "    Greedy predictions.\n",
    "    \n",
    "    :param model: one of our taggers\n",
    "    :param dl: a data loader for the heldout data\n",
    "    :param device: the PyTorch device where the model is stored\n",
    "    :param return_targets: also return the targets from the data loader\n",
    "        you can use this when the actual targets are in the dataloader (e.g., for dev set)\n",
    "\n",
    "    :return:\n",
    "        * a list of predictions, each a sequence of tags (already decoded)\n",
    "        * if return_targets=True, additionally return a list of targets, each a sequence of tags (already decoded)\n",
    "    \"\"\"\n",
    "    model.eval()    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y_in, batch_y_out in dl:\n",
    "            # [batch_size, max_len]\n",
    "            preds = model.greedy(batch_x.to(device))\n",
    "            lengths = torch.sum(batch_x != model.word_vocab.pad_id, -1).cpu().numpy()            \n",
    "            all_preds.extend([seq[:l] for l, seq in zip(lengths, model.tag_vocab.batch_decode(preds.cpu(), strip_pad=False))])\n",
    "            if return_targets:                \n",
    "                all_targets.extend([seq[:l] for l, seq in zip(lengths, model.tag_vocab.batch_decode(batch_y_out, strip_pad=False))])\n",
    "\n",
    "    if return_targets:\n",
    "        return all_preds, all_targets\n",
    "    else:\n",
    "        return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "a2atjWtPwF8w",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can use sklearn's classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "OnlLnap0ot3A",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "x7u12R0t3mAa",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here we have the training loop (already fully implemented for you). Do study it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "ZQR5TGAqd-HC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def flatten(seq):\n",
    "    \"\"\"flattens a python list\"\"\"\n",
    "    return list(chain.from_iterable(seq))\n",
    "\n",
    "def train_neural_model(model: Tagger, optimiser, training_corpus: TaggedCorpus, dev_corpus: TaggedCorpus, \n",
    "                       batch_size=200, num_epochs=10, check_every=10, device=torch.device('cuda:0')):\n",
    "    \"\"\"\n",
    "    :param model: pytorch model\n",
    "    :param optimiser: pytorch optimiser\n",
    "    :param training_corpus: a TaggedCorpus for trianing\n",
    "    :param dev_corpus: a TaggedCorpus for dev\n",
    "    :param batch_size: use more if you have more memory\n",
    "    :param num_epochs: use more for improved convergence\n",
    "    :param check_every: use less to check performance on dev set more often\n",
    "    :param device: where we run the experiment\n",
    "\n",
    "    :return: a log of quantities computed during training (for plotting)\n",
    "    \"\"\"\n",
    "    # we use the training data in random order for parameter estimation\n",
    "    batcher = DataLoader(training_corpus, batch_size=batch_size, shuffle=True, collate_fn=pad_to_longest_aligned)\n",
    "    # we use the dev data for evaluation during training (no need for randomisation here)\n",
    "    dev_batcher = DataLoader(dev_corpus, batch_size=batch_size, shuffle=False, collate_fn=pad_to_longest_aligned)\n",
    "\n",
    "    total_steps = num_epochs * len(batcher)\n",
    "    log = defaultdict(list)\n",
    "\n",
    "    # let's assess ppl before training\n",
    "    ppl = perplexity(model, dev_batcher, device=device).item()\n",
    "    log['ppl'].append(ppl)\n",
    "\n",
    "    # and tagging performance before training\n",
    "    preds, targets = predict(\n",
    "        model,         \n",
    "        dev_batcher, \n",
    "        device=device, \n",
    "        return_targets=True)    \n",
    "    acc = classification_report(flatten(targets), flatten(preds), output_dict=True, zero_division=0)['accuracy']\n",
    "    log['acc'].append(acc)\n",
    "    \n",
    "    step = 0\n",
    "\n",
    "    with tqdm(range(total_steps)) as bar:\n",
    "        for epoch in range(num_epochs): # now we train\n",
    "            \n",
    "            for batch_x, batch_y_in, batch_y_out in batcher:\n",
    "                model.train()\n",
    "                optimiser.zero_grad()\n",
    "                # we compute the loss\n",
    "                loss = model.loss(batch_x.to(device), batch_y_in.to(device), batch_y_out.to(device))\n",
    "                # and its gradient\n",
    "                loss.backward()\n",
    "                # then take an optimisation step towards minimising the loss\n",
    "                optimiser.step()\n",
    "\n",
    "                bar.set_postfix({'loss': f\"{loss.item():.2f}\", 'ppl': f\"{ppl:.2f}\", 'acc': f\"{acc:.2f}\"} )\n",
    "                bar.update()  \n",
    "                log['loss'].append(loss.item())\n",
    "\n",
    "                if step % check_every == 0: # every so often, we evaluate again\n",
    "                    # ppl on dev\n",
    "                    ppl = perplexity(model, dev_batcher, device=device).item()\n",
    "                    log['ppl'].append(ppl)\n",
    "                    # and tagging performance on dev\n",
    "                    preds = predict(\n",
    "                        model,                         \n",
    "                        dev_batcher, \n",
    "                        device=device, \n",
    "                        return_targets=False)    \n",
    "                    acc = classification_report(flatten(targets), flatten(preds), output_dict=True, zero_division=0)['accuracy']\n",
    "                    log['acc'].append(acc)                    \n",
    "                \n",
    "                step += 1\n",
    "\n",
    "    # we then evaluate one final time, with the final model\n",
    "    # ppl\n",
    "    ppl = perplexity(model, dev_batcher, device=device).item()\n",
    "    log['ppl'].append(ppl)\n",
    "    # and tagging performance\n",
    "    preds = predict(\n",
    "        model,         \n",
    "        dev_batcher, \n",
    "        device=device, \n",
    "        return_targets=False)    \n",
    "    acc = classification_report(flatten(targets), flatten(preds), output_dict=True, zero_division=0)['accuracy']\n",
    "    log['acc'].append(acc)\n",
    "\n",
    "    # and return a lot of information for inspection\n",
    "    return log            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "RwXhhAUChsKT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Experiment'></a>\n",
    "# Experiment\n",
    "\n",
    "Here we demonstrate how to train and evaluate a model. \n",
    "\n",
    "After that you will conduct an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "sko-agWU5Giv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "On GPU, this should take just about 2 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "hBG_u0XMe_Xb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_all() # reset random number generators before creating your model and training it\n",
    "\n",
    "# our demo will use a ParallelTagger\n",
    "# but all taggers in this notebook (any combination of encoder and decoder)\n",
    "# will run very smoothly on CPU, so long as you use the treebank dataset\n",
    "tagger = ParallelTagger(\n",
    "    encoder=BasicEncoder(\n",
    "        word_vocab=word_vocab,\n",
    "        word_embed_dim=64,\n",
    "    ),\n",
    "    tag_vocab=tag_vocab,    \n",
    "    hidden_size=128, \n",
    ").to(my_device)\n",
    "\n",
    "\n",
    "print(\"Model\")\n",
    "print(tagger)\n",
    "# report number of parameters\n",
    "print(\"Model size:\", tagger.num_parameters())\n",
    "\n",
    "# Train the model\n",
    "log = train_neural_model(\n",
    "    model=tagger, \n",
    "    optimiser=opt.Adam(tagger.parameters(), lr=5e-3), \n",
    "    training_corpus=training, \n",
    "    dev_corpus=dev, \n",
    "    batch_size=200, \n",
    "    num_epochs=10, \n",
    "    check_every=10,\n",
    "    device=my_device\n",
    ")\n",
    "\n",
    "# Plot loss and validation checks\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "_ = axs[0].plot(np.arange(len(log['loss'])), log['loss'])\n",
    "_ = axs[0].set_xlabel('steps')\n",
    "_ = axs[0].set_ylabel('training loss')\n",
    "_ = axs[1].plot(np.arange(len(log['ppl'])), log['ppl'])\n",
    "_ = axs[1].set_xlabel('steps (in 100s)')\n",
    "_ = axs[1].set_ylabel('model ppl given dev')\n",
    "_ = axs[2].plot(np.arange(len(log['acc'])), log['acc'])\n",
    "_ = axs[2].set_xlabel('steps (in 10s)')\n",
    "_ = axs[2].set_ylabel('dev acc')\n",
    "_ = fig.tight_layout(h_pad=2, w_pad=2)\n",
    "plt.show()\n",
    "\n",
    "# Predict for dev set\n",
    "y_, y = predict(\n",
    "    tagger,     \n",
    "    DataLoader(dev, batch_size=200, shuffle=False, collate_fn=pad_to_longest_aligned), \n",
    "    my_device, \n",
    "    return_targets=True\n",
    ")\n",
    "\n",
    "# Compare predictions and targets\n",
    "print(classification_report(flatten(y),flatten(y_), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-5'> **Graded Exercise 5 - Comparison** </a>\n",
    "\n",
    "1. Using the treebank dataset, train all three types of tagers (Parallel, Markov, Autoregressive), each in two versions (with and without the bidirectional encoder). For all models, display plots of training loss, dev perplexity and dev accuracy. Do you observe overfitting (you can use training loss and dev perplexity to study this, after all, dev perplexity is just a slight transformation of dev loss)?\n",
    "\n",
    "2. Now, compare the three types of taggers (using the dev set) **without** the bidirectional encoder. Display their performance in terms of f1-score for each of the tags as well as the macro avg and weighted avg in a table (one column per model type, see the example table below).  Use this to discuss any observed benefits of incorporating more conditional dependences. \n",
    "```\n",
    "Class           f1-score/parallel      f1-score/markov    f1-score/ar\n",
    "------------    -------------------    -----------------  -------------\n",
    "-EOS-                 \n",
    "NOUN                  \n",
    "VERB                  \n",
    ".                     \n",
    "ADP                   \n",
    "DET                   \n",
    "ADJ                   \n",
    "NUM                   \n",
    "ADV                   \n",
    "PRT                   \n",
    "PRON                  \n",
    "CONJ                  \n",
    "X                     \n",
    "macro avg             \n",
    "weighted avg          \n",
    "```\n",
    "\n",
    "\n",
    "3. Then, for each model type, display side by side a comparison of their performance **with and without** the bidirectional encoder, again display their performance in terms of f1-score (assessed on the dev set) for each of the tags as wel as macro avs and weighted avg. You can plot all model types in the same table, or one table per model type (see the example table below, for one of the model types), so long as the resulting visualisation is clear. Discuss whether you observe any benefits from a stronger encoder, and, in particular, if benefits from a stronger encoder can justify using simpler decoders. \n",
    "```\n",
    "Class           f1-score/parallel    f1-score/parallel-bidir    \n",
    "------------    ------------------   -----------------------   \n",
    "-EOS-                 \n",
    "NOUN                  \n",
    "VERB                  \n",
    ".                     \n",
    "ADP                   \n",
    "DET                   \n",
    "ADJ                   \n",
    "NUM                   \n",
    "ADV                   \n",
    "PRT                   \n",
    "PRON                  \n",
    "CONJ                  \n",
    "X                     \n",
    "macro avg             \n",
    "weighted avg     \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNuHBiG349O5I8O42aWP8nQ",
   "collapsed_sections": [],
   "include_colab_link": false,
   "name": "T6.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
