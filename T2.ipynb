{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTTqtr1rkZlR"
   },
   "source": [
    "# Guide\n",
    "\n",
    "Check the guide carefully before starting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMgFqQJLktka"
   },
   "source": [
    "## ILOs\n",
    "\n",
    "After completing this lab you should be able to \n",
    "\n",
    "* implement a Naive Bayes text classifier in Python, this includes parameter estimation and assessment of joint, conditional and marginal probabilities\n",
    "* predict and evaluate models using precision/recall\n",
    "\n",
    "**General notes**\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3.\n",
    "* Use NLTK (3.5) to read annotated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtDuDBZrhgh1"
   },
   "source": [
    "\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* [Labelled data](#data)\n",
    "* [Generative text classification](#textcls)\n",
    "* [Implementation](#code) \n",
    "* [Experiment](#exp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kt-B1p3kePL"
   },
   "source": [
    "## Table of graded exercises\n",
    "\n",
    "Exercises have equal weights.\n",
    "\n",
    "* [NBC](#ex-NBC)\n",
    "* [Grid search](#ex-grid)\n",
    "* [Error analysis](#ex-error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzu5O8fZkhXd"
   },
   "source": [
    "\n",
    "## How to use this notebook\n",
    "\n",
    "* Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
    "* Note that, as always, the notebook recaps theory, and contains solved quizzes. While you should probably make use of this theory recap, be careful not to spend disproportionately more time on this than you should. The theory here is very condensed, and to understand it you need to complete HC2a and the reading listed as preparation to this session.\n",
    "* The last section is optional for this tutorial, but note that it helps connect T1 and T2. So, you might want to work on it once you are done with the graded exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp9RNyyRhgh4"
   },
   "source": [
    "# <a name=\"data\"> Labelled data\n",
    "\n",
    "In this tutorial we will be looking into text classification with labelled data (that is, text that has been categorised or somehow labelled for certain attributes). \n",
    "    \n",
    "This can be binary classification, for example, \n",
    "\n",
    "* `nltk.corpus.sentence_polarity`\n",
    "* `nltk.corpus.movie_reviews`\n",
    "* `nltk.corpus.subjectivity`    \n",
    "    \n",
    "One multiclass classification, for example, \n",
    "* `nltk.corpus.brown`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOcG72wQhgh4"
   },
   "outputs": [],
   "source": [
    "# You might need to install these:\n",
    "\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install seaborn\n",
    "# !pip install nltk\n",
    "# !pip install tabulate\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmV9U9-7hgh5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQ3l6vAPhgh5"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('sentence_polarity')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('subjectivity')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZIMLagahgh6"
   },
   "source": [
    "It's always good to visualise some properties of the data we will be manipulating. Basic properties of interest are the frequency of the classes, the lenght of the documents in it. \n",
    "\n",
    "Visualising the frequency of classes can warn you that your dataset is unballanced, length of documents can warn you that some datasets can be challenging for you personal computer.\n",
    "\n",
    "We could plot these quantities any way we like, but pandas and seaborn together make it really easy to get insightful visualisations, so we will share this trick with you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNaIOY6-hgh6"
   },
   "outputs": [],
   "source": [
    "def get_corpus_df(corpus, categories=None):\n",
    "    \"\"\"\n",
    "    Return a tall dataframe for a nltk text classification corpus.\n",
    "    corpus: an instance of nltk.corpus meant for text classification\n",
    "    categories: None or a list of categories (in case we want to use just a portion of the labelled documents)\n",
    "    \"\"\"\n",
    "    if categories is None:\n",
    "        categories = corpus.categories()\n",
    "    rows = []\n",
    "    for label in categories:\n",
    "        rows.extend((label, len(x)) for x in corpus.sents(categories=[label]))\n",
    "    return pd.DataFrame(rows, columns=['label', 'length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VRbDSMghgh7"
   },
   "source": [
    "**Subjectivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuRNaiUGhgh7"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import subjectivity\n",
    "\n",
    "subjectivity_df = get_corpus_df(subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqD5r3HIhgh7"
   },
   "outputs": [],
   "source": [
    "_ = sns.catplot(y='label', orient='h', kind='count', data=subjectivity_df).set(title=\"subjectivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WVW7wEihgh7"
   },
   "outputs": [],
   "source": [
    "_ = sns.catplot(x='length', y='label', kind='violin', data=subjectivity_df).set(title=\"subjectivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6MyiAVuhgh8"
   },
   "source": [
    "Two types of documents (objective and subjective) that are equally represented, and they are reasonably short documents (mostly 10-40 tokens). The distribution of document length is mostly similar for both classes, but the objective class shows a longer tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF-f_RFAhgh8"
   },
   "source": [
    "It is also a good idea to inspect actual documents in each class. Familiarity with our datasets makes us better researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8nmDtNOhgh8"
   },
   "outputs": [],
   "source": [
    "for x, y in zip(subjectivity.sents(categories=['obj']), ['obj'] * 10):\n",
    "    print(y, ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTLe5VR3hgh8"
   },
   "outputs": [],
   "source": [
    "for x, y in zip(subjectivity.sents(categories=['subj']), ['subj'] * 10):\n",
    "    print(y, ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK02HJynhgh9"
   },
   "source": [
    "**Ungraded exercise with partial solution** \n",
    "    \n",
    "Use the same 3 visualisation/inspections techniques as well as 1 more (which you can choose) on the following three corpora:\n",
    "\n",
    "* `nlt.corpus.sentence_polarity`\n",
    "* `nltk.corpus.movie_reviews`\n",
    "* `nltk.corpus.brown`\n",
    "\n",
    "and like we did, make some remarks about what you see. For `brown` which has many classes, you can inspect fewer documents per class, or pick 3-4 of the classes and inspect documents for those only.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "from nltk.corpus import sentence_polarity\n",
    "\n",
    "# class frequency\n",
    "# length histogram\n",
    "\n",
    "sentence_polarity_df = get_corpus_df(sentence_polarity)\n",
    "_ = sns.catplot(y='label', orient='h', kind='count', data=sentence_polarity_df).set(title=\"sentence_polarity\")\n",
    "_ = sns.catplot(x='length', y='label', kind='violin', data=sentence_polarity_df).set(title=\"sentence_polarity\")\n",
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews_df = get_corpus_df(movie_reviews)\n",
    "_ = sns.catplot(y='label', orient='h', kind='count', data=movie_reviews_df).set(title=\"movie_reviews\")\n",
    "_ = sns.catplot(x='length', y='label', kind='violin', data=movie_reviews_df).set(title=\"movie_reviews\")\n",
    "from nltk.corpus import brown\n",
    "brown_df = get_corpus_df(brown)\n",
    "_ = sns.catplot(y='label', orient='h', kind='count', data=brown_df).set(title=\"brown\")\n",
    "_ = sns.catplot(x='length', y='label', kind='violin', data=brown_df).set(title=\"brown\")\n",
    "\n",
    "\n",
    "# Also list some examples\n",
    "\n",
    "# Additional (examples):\n",
    "# * List the most frequent words\n",
    "# * Plot word vs freq\n",
    "# * Plot rank vs freq\n",
    "\n",
    "# from itertools import compress\n",
    "\n",
    "# # for y in brown.categories():\n",
    "# #     print(\"Label: '{}'\".format(y))\n",
    "# #     for x in compress(brown.sents(categories=[y]), [True] * 2):\n",
    "# #         print(' '.join(x))    \n",
    "```\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJy0vwRIjEue"
   },
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HXQUuAthgh9"
   },
   "source": [
    "In this tutorial, **we will focus on sentiment classification** using the `sentence_polarity` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqtlrqAPhgh-"
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3mcQZNahgh-"
   },
   "source": [
    "# <a name=\"textcls\"> Generative text classification\n",
    "\n",
    "In NLP we often want to design systems that can read a piece of text and categorise it as an instance of one of a finite set of classes. We call those systems *text classifiers* (or text categorisers).\n",
    "    \n",
    "It is seldom the case that a piece of text can be reasonably mapped to *a single category*, for various reasons that we have already discussed: language is ambiguous, we often lack context (about the situation, the writer, the reader), and many implicit attributes of text are subjective by nature (e.g., one's opinion about a product). So, in practice we face text classification as a pipeline: we combine a *statistical model* and a *decision rule*.\n",
    "    \n",
    "The statistical model realises the mapping from a given piece of text to a *conditional probability distribution* (cpd) over the categories available. The decision rule uses the information in this distribution to elect a single category that is shown to the user.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBH0HHE8hgh_"
   },
   "source": [
    "Consider the case of *sentiment classification* with three sentiment levels (i.e., negative, neutral, and positive). \n",
    "\n",
    "A **probabilistic sentiment classifier** maps a document $x \\in \\mathcal X$ to a probability distribution over the sample space $\\mathcal Y = \\{-1, 0, +1\\}$, where -1 stands for negative, 0 stands for neutral, and +1 stands for positive sentiment. Note that we call $x$ a document, but it could be a sentence, a paragraph, or any granularity we like. A document $x$ is a sequence of words $x=\\langle w_{1}, \\ldots, w_{l} \\rangle$, or $w_{1:l}$ for short, where $l = |x|$ is the sequence length. Each word belongs to a vocabulary $\\mathcal W$ of known words, and the vocabulary size is $V$. \n",
    "The space of all possible documents $\\mathcal X$ is the space of all sequences made of words from $\\mathcal W$, and these sequences can be of any size, a set like that is denoted $\\mathcal W^*$.\n",
    "\n",
    "So, formally, we have a random variable $X$ taking on documents in the set $\\mathcal X$, a random variable $Y$ taking on classes in the set $\\mathcal Y$, and a random variable $W$ taking on words in a vocabulary $\\mathcal W$. A statistical model then establishes a map from any one $x \\in \\mathcal X$ to the cpd $P_{Y|X=x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxVbcmOrhgh_"
   },
   "source": [
    "As we said in the course, the probability of a certain sentiment $y \\in \\{-1, 0, 1\\}$ given a piece of text $x$ is not a real attribute of $x$ in the world. Rather, is is a quantification of the uncertainty that a *model of sentiment analysis* can make given text and given all of the design assumptions we made. \n",
    "\n",
    "\n",
    "So, let's **design** one such model. We will pick sentiment analysis as the example, but other text classification problems fit in the same framework as long as the target classes come from a finite set of disjoint categories (e.g., sentiments, or topics, or types of named-entities, or spam vs not spam, or product categorisation in online shops, etc). Technically such sets are called *countably finite sets*.\n",
    "\n",
    "\n",
    "**Goal** We want a conditional distribution $P_{Y|X=x}$ that, given some piece of text $x$ (e.g., a sentence, a paragraph, or a document), quantifies our model's beliefs in each of the classes in the sample space $\\mathcal Y$.\n",
    "\n",
    "**Challenge** At this point in the course we only know how to model Categorical distributions using the so called *tabular representation* whereby a conditional probability distribution (cpd) $Y|X=x$ would require its own set of parameters which are unique to the outcome $x$ we condition on. As documents are really long, and there's no limit to how many different documents there can be in the world, we would have to store infinitely many distributions. Even if we choose to only store those for which we have observed some $x$ in the training data, we would face another problem, namely, we would not be able to classify novel documents (those never seen before). This is a key challenge known as *data sparsity*.\n",
    "\n",
    "**Solution** We will model a joint distribution $P_{YX}$ over $X$ and $Y$ which we will factorise using some crucial conditinal independence assumptions, namely, we will factorise $Y$ first, as in $P_{YX}(y,x)=P_Y(y)P_{X|Y}(x|y)$, and then assume that words in $x$ are independent of one another when conditioned on the class $y$, which leads to $P_{YX}(y,x) \\overset{\\text{ind.}}{=} P_Y(y)\\prod_{i=1}^l P_{W|Y}(w_i|y)$.\n",
    "\n",
    "This is the *naive Bayes classifier*. It's called naive because the independence assumption is unrealistic (in reality words are not independent of one another), but they are useful to get to a feasible model. It's called Bayes because to classify a given document $x$, we need to infer the distribution $P_{Y|X=x}$ from the joint distribution, which we can do via *Bayes rule*, namely, $P_{Y|X}(y|x) = \\frac{P_{YX}(y, x)}{P_X(x)}$. The numerator is the joint probability, which we just defined earlier, and the denominator is the marginal probability, which we obtain via marginalisation $P_X(x) = \\sum_{k \\in \\mathcal Y} P_{YX}(k, x)$.\n",
    "\n",
    "We will then use a Categorical distribution for the prior distribution $P_Y$ and a set of $K$ Categorical distributions, one cpd $P_{W|Y=y}$ per class, for the class-conditioned distributions over the vocabulary. Specifically, this is the generative story:\n",
    "\n",
    "1. Draw a class $Y \\sim \\mathrm{Cat}(\\boldsymbol\\phi)$ with $\\boldsymbol\\phi \\in \\Delta_{K-1}$\n",
    "2. Draw one word a time $W|Y=y \\sim \\mathrm{Cat}(\\boldsymbol\\pi^{(y)})$ with $\\boldsymbol\\pi^{(y)} \\in \\Delta_{V-1}$, until you generate a stop symbol (a special token in the vocabulary, used to denote the end of the document/sentence).\n",
    "\n",
    "Parameter estimation then follows via maximum likelihood estimation (MLE) for Categorical distributions, and to avoid 0 probabilities for unseen words in the future, we use *Laplace smoothing* (or add $\\alpha$ smoothing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyW2efiNhgiG"
   },
   "source": [
    "# <a name=\"code\"> Implementation\n",
    "\n",
    "    \n",
    "We will design a python class that implements NBC. We will start with some helper code to manipulate Categorical cpds, then implement the key quantities in NBC one by one. Finally, when we have all parts together we will put them together in a python class container. \n",
    "    \n",
    "Most exercises have solutions and test cases. We advise you to invest some time trying to develop the solutions yourself (before checking or using our own solutions), the test cases will help you with that. In case your Python skills are not sufficient at the moment, study the solutions carefully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koy2L_kihgiI"
   },
   "outputs": [],
   "source": [
    "def validate_categorical_cpd(params: dict):\n",
    "    \"\"\"\n",
    "    Just to remind you a valid Categorical cpd is a parameterised by a vector of positive \n",
    "    numbers that sum to 1.\n",
    "    \n",
    "    We normally think of the parameters as a sequence or a vector, \n",
    "    but in a programming language, it can be useful to use a dict, \n",
    "    with a dict the indexing does not need to be numerical (we could use the label 'name' \n",
    "    or the word itself to index positions in the dictionary).\n",
    "    \n",
    "    In some cases a dict may be considered slow compared to a list or array, \n",
    "    but for the scale of our experiments in the tutorial, it's fine and \n",
    "    conceptually much easier.\n",
    "    \n",
    "    params: map outcome to probability mass\n",
    "    \"\"\"\n",
    "    return all(0 <= p <= 1 for p in params.values()) and np.isclose(sum(p for p in params.values()), 1., 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unKdU2GBhgiI"
   },
   "outputs": [],
   "source": [
    "assert validate_categorical_cpd({'pos': 0.2, 'neg': 0.8})  # fine\n",
    "assert validate_categorical_cpd({'pos': 0.3, 'neg': 0.7})  # fine\n",
    "assert validate_categorical_cpd({'pos': 0.2, 'neu': 0.1, 'neg': 0.7})  # we can have more than 2, that's fine \n",
    "assert validate_categorical_cpd({'pos': 0.2, 'neu': 0.0, 'neg': 0.8})  # we can have 0 probs inside\n",
    "assert not validate_categorical_cpd({'pos': 0.2, 'neg': 0.7})  # not good\n",
    "assert not validate_categorical_cpd({'pos': 0.2})  # not good\n",
    "assert not validate_categorical_cpd({'pos': -0.2, 'neu': 0.2, 'neg': 0.8})  # we cannot have 'negative probs' inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46wdJpE7hgiI"
   },
   "source": [
    "We will now begin implementing a little piece at a time all functionalities necessary to get NBC off the floor.\n",
    "\n",
    "As we will be manipulating some tables, the first two functions are already implemented for you to show you how you can use python dictionaries to store tables.\n",
    "\n",
    "\n",
    "**Exercise with solution** study and understand the design of `get_prior_parameter` and `get_cond_parameter` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpIQJ7tdhgiI"
   },
   "outputs": [],
   "source": [
    "def get_prior_parameter(y: str, phi: dict):\n",
    "    \"\"\"\n",
    "    Return P(Y=y) under the model Categorical(phi).\n",
    "    \n",
    "    y: the label\n",
    "    phi: a dictionary that maps from the label to its probability mass\n",
    "        even though mathematically we think of phi as a vector, \n",
    "        in code it can be convenient to treat it like a dictionary some times, \n",
    "        for example, as a dictionary we can use labels that are strings (rather than 0-based indices)\n",
    "        \n",
    "        For this implementation assume that phi has already been validated as a Categorical parameter.\n",
    "        \n",
    "    Return: Categorical(y|\\phi)\n",
    "    \"\"\"\n",
    "    # **SOLUTION**\n",
    "    return phi.get(y, 0)  # if an outcome is outside the support, it gets 0 mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ii0IbstlhgiI"
   },
   "outputs": [],
   "source": [
    "assert get_prior_parameter('pos', {'pos': 0.2, 'neg': 0.8}) == 0.2\n",
    "assert get_prior_parameter('neg', {'pos': 0.3, 'neg': 0.7}) == 0.7\n",
    "assert get_prior_parameter('neu', {'pos': 0.3, 'neg': 0.7}) == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfK4RLBvhgiJ"
   },
   "outputs": [],
   "source": [
    "def get_cond_parameter(y: str, w: str, pi: dict, UNK=None):\n",
    "    \"\"\"\n",
    "    Return P(W=w|Y=y) under the model Categorical(\\pi^{(y)}).\n",
    "    \n",
    "    y: the label\n",
    "    w: word\n",
    "    pi: a dictionary of dictionaries \n",
    "        first we can index it using a label to obtain a dict as return, \n",
    "        the latter dict are the parameters for a distribution over the vocabulary given the label\n",
    "        and it can be indexed using a word to obtain a probability mass\n",
    "\n",
    "        even though mathematically we think of pi as a table/matrix\n",
    "        in code it can be convenient to treat it like a dictionary some times, \n",
    "        for example, as a dictionary we can use labels that are strings (rather than 0-based indices)\n",
    "        and words that are strings (rather than 0-based indices to a vocabulary).        \n",
    "        \n",
    "        For this implementation assume that every dict inside of pi\n",
    "         has already been validated as a Categorical parameter.\n",
    "    UNK: reserved for future use\n",
    "        \n",
    "    Return: Categorical(w|\\phi^{(y)})\n",
    "    \"\"\"\n",
    "    # **SOLUTION**\n",
    "    cpd = pi.get(y, None)\n",
    "    if cpd is None:  # the label is not in the support of the model\n",
    "        return 0. \n",
    "    return cpd.get(w, 0.)  # if a word is outside the support, it gets 0 mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4qkhmcchgiJ"
   },
   "outputs": [],
   "source": [
    "# A test case for us, the cpds in it are valid as we can see\n",
    "# (do not change this test case, it will be used in other cells)\n",
    "test_phi_1 = {'pos': 0.6, 'neg': 0.4}\n",
    "test_pi_1 = {\n",
    "    'pos': {'not': 0.05, 'so': 0.1, 'good': 0.5, 'bad': 0.15, 'okay': 0.2 }, \n",
    "    'neg': {'not': 0.05, 'so': 0.1, 'good': 0.1, 'bad': 0.5, 'okay': 0.25 }\n",
    "}\n",
    "assert validate_categorical_cpd(test_phi_1)\n",
    "assert all(validate_categorical_cpd(cpd) for y, cpd in test_pi_1.items())\n",
    "\n",
    "# Fine\n",
    "assert get_cond_parameter('pos', 'good', test_pi_1) == 0.5\n",
    "assert get_cond_parameter('pos', 'not', test_pi_1) == 0.05\n",
    "assert get_cond_parameter('pos', 'so', test_pi_1) == 0.1\n",
    "assert get_cond_parameter('pos', 'bad', test_pi_1) == 0.15\n",
    "assert get_cond_parameter('pos', 'okay', test_pi_1) == 0.2\n",
    "\n",
    "# Fine\n",
    "assert get_cond_parameter('neg', 'good', test_pi_1) == 0.1\n",
    "assert get_cond_parameter('neg', 'not', test_pi_1) == 0.05\n",
    "assert get_cond_parameter('neg', 'so', test_pi_1) == 0.1\n",
    "assert get_cond_parameter('neg', 'bad', test_pi_1) == 0.5\n",
    "assert get_cond_parameter('neg', 'okay', test_pi_1) == 0.25\n",
    "\n",
    "# Not in the support\n",
    "assert get_cond_parameter('neu', 'okay', test_pi_1) == 0.\n",
    "assert get_cond_parameter('pos', 'strange', test_pi_1) == 0.\n",
    "assert get_cond_parameter('neg', 'strange', test_pi_1) == 0.\n",
    "assert get_cond_parameter('neu', 'strange', test_pi_1) == 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-inh4L3qhgiK"
   },
   "source": [
    "**Exercise with solution** Impement a function to return $\\log P_Y(y)$ for a label $y \\in \\mathcal Y$ under this model. Your function should use the functionality `get_prior_parameter` and/or `get_cond_parameter` from earlier. Assume the model parameters are already available. \n",
    "\n",
    "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
    "\n",
    "See API below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5HP0Yj3hgiK"
   },
   "outputs": [],
   "source": [
    "def log_prior_prob(y: str, phi: dict):\n",
    "    \"\"\"Return log P(Y=y) under the model\"\"\"\n",
    "    raise NotImplemented(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biKjsaZFhgiK"
   },
   "source": [
    "<details>\n",
    "    <summary><b>Solution</b></summary>\n",
    "    \n",
    "```python\n",
    "def log_prior_prob(y: str, phi: dict):\n",
    "    \"\"\"Return log P(Y=y) under the model\"\"\"\n",
    "    # **SOLUTION**\n",
    "    return np.log(get_prior_parameter(y, phi))    \n",
    "```\n",
    "    \n",
    "---    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8avNn8mhgiK"
   },
   "outputs": [],
   "source": [
    "# Your code should pass these tests\n",
    "assert log_prior_prob('pos', {'pos': 0.2, 'neg': 0.8}) == np.log(0.2)\n",
    "assert log_prior_prob('neg', {'pos': 0.3, 'neg': 0.7}) == np.log(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EHZYw6NhgiK"
   },
   "source": [
    "**Exercise with solution** Impement a function to return $\\log P_{X|Y}(x|y)$ for a label $y \\in \\mathcal Y$  and document $x \\in \\mathcal X$ under this model. Your function should use the functionality `get_prior_parameter` and/or `get_cond_parameter` from earlier. Assume the model parameters are already available. \n",
    "\n",
    "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
    "\n",
    "See API below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1c4kPCLhgiL"
   },
   "outputs": [],
   "source": [
    "def log_conditional_prob(x: list, y: str, pi: dict):\n",
    "    \"\"\"Return log P(X=x|Y=y)\"\"\"\n",
    "    raise NotImplemented(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xcYEeXRhgiL"
   },
   "source": [
    "<details>\n",
    "    <summary><b>Solution</b></summary>\n",
    "    \n",
    "```python\n",
    "def log_conditional_prob(x: list, y: str, pi: dict):\n",
    "    \"\"\"Return log P(X=x|Y=y) under the model\"\"\"\n",
    "    # **SOLUTION**\n",
    "    # named arguments help us not make mistakes (for example, flipping y and w accidentaly)\n",
    "    return sum(np.log(get_cond_parameter(y=y, w=w, pi=pi)) for w in x)      \n",
    "```\n",
    "    \n",
    "---    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJNBo56nhgiL"
   },
   "outputs": [],
   "source": [
    "# Your code should pass these tests\n",
    "\n",
    "# Fine\n",
    "assert np.isclose(log_conditional_prob(\"not bad\".split(), 'pos', test_pi_1), np.log(0.05 * 0.15), 1e-3)\n",
    "assert np.isclose(log_conditional_prob(\"not good\".split(), 'pos', test_pi_1), np.log(0.05 * 0.5), 1e-3)\n",
    "assert np.isclose(log_conditional_prob(\"good\".split(), 'pos', test_pi_1), np.log(0.5), 1e-3)\n",
    "\n",
    "assert np.isclose(log_conditional_prob(\"not bad\".split(), 'neg', test_pi_1), np.log(0.05 * 0.5), 1e-3)\n",
    "assert np.isclose(log_conditional_prob(\"not good\".split(), 'neg', test_pi_1), np.log(0.05 * 0.1), 1e-3)\n",
    "assert np.isclose(log_conditional_prob(\"good\".split(), 'neg', test_pi_1), np.log(0.1), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYd8sV_ThgiL"
   },
   "source": [
    "A good code should also deal with cases that *are not* in the support of the joint distribution. Those should have 0 probability for now (later we discuss a technique to avoid 0 probs). The log of 0 is -inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZfP5eyMhgiL"
   },
   "outputs": [],
   "source": [
    "# Not in the support (should produce warnings, but pass the test)\n",
    "assert log_prior_prob('neu', {'pos': 0.3, 'neg': 0.7}) == -np.inf\n",
    "assert log_conditional_prob(\"not that bad\".split(), 'pos', test_pi_1) == -np.inf\n",
    "assert log_conditional_prob(\"not that bad\".split(), 'neg', test_pi_1) == -np.inf\n",
    "assert log_conditional_prob(\"not okay\".split(), 'neu', test_pi_1) == -np.inf\n",
    "\n",
    "# you will probably get some RuntimeWarning, this is okay here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvpun40rhgiM"
   },
   "source": [
    "**Exercise with solution** Impement a function to return $\\log P_{YX}(y, x)$ for a label $y \\in \\mathcal Y$  and document $x \\in \\mathcal X$ under this model. Your function should use the functionality `log_prior_prob` and/or `log_conditional_prob` from earlier. Assume the model parameters are already available. \n",
    "\n",
    "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
    "\n",
    "See API below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQ_9K0DnhgiM"
   },
   "outputs": [],
   "source": [
    "def log_joint_prob(y: str, x: list, phi: dict, pi: dict):\n",
    "    \"\"\"Return log P(Y=y, X=x) under the model\"\"\"\n",
    "    raise NotImplemented(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQIuPxwBhgiM"
   },
   "source": [
    "<details>\n",
    "    <summary><b>Solution</b></summary>\n",
    "    \n",
    "```python\n",
    "def log_joint_prob(y: str, x: list, phi: dict, pi: dict):\n",
    "    \"\"\"Return log P(Y=y, X=x) under the model\"\"\"\n",
    "    # **SOLUTION**\n",
    "    # named arguments help us not make mistakes (eg, flipping x and y in the conditional)\n",
    "    return log_prior_prob(y, phi) + log_conditional_prob(x=x, y=y, pi=pi)\n",
    "```\n",
    "    \n",
    "---    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bShgSO8hgiM"
   },
   "outputs": [],
   "source": [
    "# Your code should pass these tests\n",
    "\n",
    "assert np.isclose(\n",
    "    log_joint_prob(y='pos', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log(0.6 * 0.05 * 0.15), \n",
    "    1e-3\n",
    ")\n",
    "assert np.isclose(\n",
    "    log_joint_prob(y='pos', x=\"not good\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log(0.6 * 0.05 * 0.5), \n",
    "    1e-3\n",
    ")\n",
    "\n",
    "\n",
    "assert np.isclose(\n",
    "    log_joint_prob(y='neg', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log(0.4 * 0.05 * 0.5), \n",
    "    1e-3\n",
    ")\n",
    "assert np.isclose(\n",
    "    log_joint_prob(y='neg', x=\"not good\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log(0.4 * 0.05 * 0.1), \n",
    "    1e-3\n",
    ")\n",
    "\n",
    "# NBC is not sensitive to the order of words within the sentence\n",
    "\n",
    "assert np.isclose(\n",
    "    log_joint_prob(y='pos', x=\"bad not\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log(0.6 * 0.05 * 0.15), \n",
    "    1e-3\n",
    ")\n",
    "\n",
    "assert np.isclose(\n",
    "    log_joint_prob(y='neg', x=\"good not\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log(0.4 * 0.05 * 0.1), \n",
    "    1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owmc7QchhgiN"
   },
   "source": [
    "**Exercise with solution** Impement a function to return $\\log P_{X}(x)$ for a document $x \\in \\mathcal X$ under this model. Your function should use the functionality `log_prior_prob` and/or `log_conditional_prob` and/or `log_joint_prob` from earlier. Assume the model parameters are already available. \n",
    "\n",
    "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
    "\n",
    "See API below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNxoCQTWhgiN"
   },
   "outputs": [],
   "source": [
    "def log_marginal_prob(x: list, phi: dict, pi: dict):\n",
    "    \"\"\"Return log P(X=x) under the model\"\"\"    \n",
    "    raise NotImplemented(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twW0gPadhgiN"
   },
   "source": [
    "<details>\n",
    "    <summary><b>Solution</b></summary>\n",
    "    \n",
    "```python\n",
    "def log_marginal_prob(x: list, phi: dict, pi: dict):\n",
    "    \"\"\"Return log P(X=x) under the model\"\"\"\n",
    "    # **SOLUTION**\n",
    "    # Tips:\n",
    "    # * compute the joint probability for x and each of the possible labels\n",
    "    #   marginalise (sum)\n",
    "    #   compute log\n",
    "    # * or, compute all of it in log space using log_joint_prob\n",
    "    #   and np.logaddexp.reduce\n",
    "    \n",
    "    # for the marginalisation, we need the support of the rv Y, \n",
    "    # we can find it within the dictionary phi (ie, the set of keys in it)\n",
    "    support_Y = phi.keys()\n",
    "    # the function logaddexp(a, b) returns log(exp(a) + exp(b))\n",
    "    # the method reduce applies that in a row to a whole list of values \n",
    "\n",
    "    return np.logaddexp.reduce([log_joint_prob(y=y, x=x, phi=phi, pi=pi) for y in support_Y]) \n",
    "```\n",
    "    \n",
    "---    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CBcZstshgiN"
   },
   "outputs": [],
   "source": [
    "# Your code should pass these tests\n",
    "\n",
    "assert np.isclose(\n",
    "    log_marginal_prob(x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log(0.6 * 0.05 * 0.15 + 0.4 * 0.05 * 0.5), \n",
    "    1e-3\n",
    ")\n",
    "assert np.isclose(\n",
    "    log_marginal_prob(x=\"not good\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log(0.6 * 0.05 * 0.5 + 0.4 * 0.05 * 0.1), \n",
    "    1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o28Huus7hgiN"
   },
   "source": [
    "**Exercise with solution** Impement a function to return $\\log P_{Y|X}(y|x)$ for a label $y\\in \\mathcal Y$ and document $x \\in \\mathcal X$ under this model. Your function should use the functionality `log_prior_prob` and/or `log_conditional_prob` and/or `log_joint_prob` and/or `log_marginal_prob` from earlier. Assume the model parameters are already available. \n",
    "\n",
    "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
    "\n",
    "See API below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XFbw3GAhgiO"
   },
   "outputs": [],
   "source": [
    "def log_posterior_prob(y: str, x: list, phi: dict, pi: dict):\n",
    "    \"\"\"Return log P(Y=y|X=x) under the model\"\"\"\n",
    "    raise NotImplemented(\"Implement me !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP8054MShgiO"
   },
   "source": [
    "<details>\n",
    "    <summary><b>Solution</b></summary>\n",
    "    \n",
    "```python\n",
    "def log_posterior_prob(y: str, x: list, phi: dict, pi: dict):\n",
    "    \"\"\"Return log P(Y=y|X=x) under the model\"\"\"    \n",
    "    # The posterior probability is a conditional probability\n",
    "    #  by definition that is P(Y=y,X=x) / P(X=x)\n",
    "    #  and we have already implemented the joint probability in the numerator\n",
    "    log_joint_p = log_joint_prob(y=y, x=x, phi=phi, pi=pi)\n",
    "    #  and the marginal probability in the denominator\n",
    "    log_marginal_p = log_marginal_prob(x, phi=phi, pi=pi)\n",
    "    # As we are in computing it in log space, we can subtract the logs to divide the probs\n",
    "    return log_joint_p - log_marginal_p\n",
    "```\n",
    "    \n",
    "---    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rti572e-hgiO"
   },
   "outputs": [],
   "source": [
    "# Your code should pass these tests\n",
    "\n",
    "assert np.isclose(\n",
    "    log_posterior_prob(y='pos', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log((0.6 * 0.05 * 0.15)/(0.6 * 0.05 * 0.15 + 0.4 * 0.05 * 0.5)), \n",
    "    1e-3\n",
    ")\n",
    "\n",
    "assert np.isclose(\n",
    "    log_posterior_prob(y='neg', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1), \n",
    "    np.log((0.4 * 0.05 * 0.5)/(0.6 * 0.05 * 0.15 + 0.4 * 0.05 * 0.5)), \n",
    "    1e-3\n",
    ")\n",
    "\n",
    "# The posterior is a distribution, thus it must add to one if we sum it over the possible labels:\n",
    "assert np.isclose(\n",
    "    np.exp(log_posterior_prob(y='pos', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1)) \n",
    "    + np.exp(log_posterior_prob(y='neg', x=\"not bad\".split(), phi=test_phi_1, pi=test_pi_1)), \n",
    "    1, \n",
    "    1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2Mcr_YphgiO"
   },
   "source": [
    "**Exercise with solution** Impement a function to return the most probable class under this model for a given document $x \\in \\mathcal X$. Your function should use the functionality `log_prior_prob` and/or `log_conditional_prob` and/or `log_joint_prob` and/or `log_marginal_prob` from earlier. Assume the model parameters are already available. \n",
    "\n",
    "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
    "\n",
    "See API below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pfc9Ozm3hgiO"
   },
   "outputs": [],
   "source": [
    "def classify(x: list, phi: dict, pi: dict):\n",
    "    \"\"\"\n",
    "    Classify x using the most probable label a-posteriori.\n",
    "    \n",
    "    x: a single document to be classified (a list of tokens)\n",
    "    phi: parameters of the prior\n",
    "    pi: parameters of the class-conditioned distributions over vocabulary\n",
    "    \n",
    "    Return the most probable label for the document\n",
    "    \"\"\"\n",
    "    # Tips\n",
    "    # * for each y, evaluate log p(s, y), then select the one that has highest value\n",
    "    #   note that we have provided a helper method self.log_joint_prob(y, x)\n",
    "    raise NotImplemented(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rkeTFBqhgiO"
   },
   "source": [
    "<details>\n",
    "    <summary><b>Solution</b></summary>\n",
    "    \n",
    "```python\n",
    "def classify(x: list, phi: dict, pi: dict):\n",
    "    \"\"\"\n",
    "    Classify x using the most probable label a-posteriori.\n",
    "    \n",
    "    x: a single document to be classified (a list of tokens)\n",
    "    phi: parameters of the prior\n",
    "    pi: parameters of the class-conditioned distributions over vocabulary\n",
    "    \n",
    "    Return the most probable label for the document\n",
    "    \"\"\"\n",
    "    # **EXERCISE**\n",
    "    # Tips\n",
    "    # * for each y, evaluate log p(s, y), then select the one that has highest value\n",
    "    #   note that we have provided a helper method self.log_joint_prob(y, x)\n",
    "\n",
    "    support_Y = list(phi.keys())\n",
    "    best_k = np.argmax([log_joint_prob(y=y, x=x, phi=phi, pi=pi) for y in support_Y])\n",
    "    y_pred = support_Y[best_k]\n",
    "    return y_pred   \n",
    "```\n",
    "    \n",
    "---    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSUR1IqohgiP"
   },
   "outputs": [],
   "source": [
    "# Your code should pass these tests\n",
    "\n",
    "# documents with single word\n",
    "assert classify(\"good\".split(), phi=test_phi_1, pi=test_pi_1) == 'pos'\n",
    "assert classify(\"bad\".split(), phi=test_phi_1, pi=test_pi_1) == 'neg'\n",
    "\n",
    "# documents with multiple words\n",
    "assert classify(\"so good\".split(), phi=test_phi_1, pi=test_pi_1) == 'pos'\n",
    "assert classify(\"so bad\".split(), phi=test_phi_1, pi=test_pi_1) == 'neg'\n",
    "\n",
    "# NBC is not good to handle modifiers, because it sees documents as unordered collections\n",
    "assert classify(\"not good\".split(), phi=test_phi_1, pi=test_pi_1) == 'pos'\n",
    "assert classify(\"not bad\".split(), phi=test_phi_1, pi=test_pi_1) == 'neg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XazzNSrRhgiP"
   },
   "source": [
    "Hopefully it is clear to you that \n",
    "* the model is a probability distribution\n",
    "* the model manipulates simpler probability factors to assign probability to larger documents\n",
    "* the model definition and its parameter estimation are two very different things (so far we have been implementing all sorts of probability queries without ever having to worry about how parameters are estimated -- in fact the parameters have been given to us through `test_phi_1` and `test_pi_1`\n",
    "\n",
    "We can now turn to parameter estimation via MLE, for which I need to give you a dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJguT1BYhgiP"
   },
   "source": [
    "**Exercise with solution** Implement a function that returns a dictionary of prior parameters (something similar to the `test_phi_1` object that we have been using. The numerical values of the parameters should be given by maximum likelihood estimation using a dataset of labelled documents.\n",
    "\n",
    "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
    "\n",
    "See API below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcYy4-4ihgiP"
   },
   "outputs": [],
   "source": [
    "def estimate_phi(data_x, data_y):\n",
    "    \"\"\"\n",
    "    Return the dict that stores the parameters \\phi of the prior.\n",
    "    data_x: list of documents, each document is a list of tokens, each token is a string\n",
    "    data_y: list of labels, each label is a string\n",
    "    \n",
    "        Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
    "        \n",
    "    Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
    "    \"\"\"\n",
    "    raise NotImplemented(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOK38RZxhgiP"
   },
   "source": [
    "<details>\n",
    "    <summary><b>Solution</b></summary>\n",
    "    \n",
    "```python\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "    \n",
    "def estimate_phi(data_x, data_y):\n",
    "    \"\"\"\n",
    "    Return the dict that stores the parameters \\phi of the prior.\n",
    "    data_x: list of documents, each document is a list of tokens, each token is a string\n",
    "    data_y: list of labels, each label is a string\n",
    "    \n",
    "        Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
    "        \n",
    "    Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
    "    \"\"\"\n",
    "    # **SOLUTION**\n",
    "    N = len(data_y)\n",
    "    phi = dict((y, float(n) / N) for y, n in Counter(data_y).items())\n",
    "    return phi \n",
    "```\n",
    "    \n",
    "---    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWCKe71ChgiQ"
   },
   "outputs": [],
   "source": [
    "# Your code should pass these tests\n",
    "\n",
    "test_data_x_1 = [\n",
    "    \"pretty good\".split(),\n",
    "    \"pretty good\".split(),\n",
    "    \"pretty good\".split(),\n",
    "    \n",
    "    \"quite good\".split(),\n",
    "    \"quite okay\".split(),\n",
    "    \n",
    "    \"so good\".split(),\n",
    "    \"so pretty good\".split(),\n",
    "    \n",
    "    \"bad\".split(),\n",
    "    \"bad bad bad\".split(),\n",
    "    \"so very bad\".split(),    \n",
    "    \"just so bad\".split(),\n",
    "    \"bad just really really bad\".split(),\n",
    "]\n",
    "test_data_y_1 = [\n",
    "    'pos', 'pos', 'pos',\n",
    "    'pos', 'pos',\n",
    "    'pos', 'pos',\n",
    "    'neg', 'neg', 'neg', 'neg', 'neg'\n",
    "]\n",
    "\n",
    "test_result_phi_1 = estimate_phi(test_data_x_1, test_data_y_1)\n",
    "\n",
    "# First of all, we get a proper cpd\n",
    "assert validate_categorical_cpd(test_result_phi_1)\n",
    "\n",
    "# And it contains the probs we expect\n",
    "assert np.isclose(test_result_phi_1['pos'], 7./(7+5), 0.01)\n",
    "assert np.isclose(test_result_phi_1['neg'], 5./(7+5), 0.01)\n",
    "\n",
    "# Let's use our helper code\n",
    "assert get_prior_parameter('neu', test_result_phi_1) == 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8KOixOIhgiQ"
   },
   "source": [
    "**Exercise with solution** Implement a function that returns a dictionary of dictionaries for the parameters of the class-conditioned distributions over vocabulary (something similar to the `test_pi_1` object that we have been using). The numerical values of the parameters should be given by maximum likelihood estimation using a dataset of labelled documents.\n",
    "\n",
    "Below you will find some assertions (ie, test cases) for you to verify your implementation.\n",
    "\n",
    "Your code should support Laplace smoothing, but you can implement it first without (see you can pass the first set of tests, which does not depend on smoothing) and then try to modify it to use smoothing (and see if you can pass the second set of tests).\n",
    "\n",
    "\n",
    "We provide some helper code, for determining the vocabulary from data (the vocabulary should contain some special symbols, e.g.  end-of-sequence  symbol, and, if Laplace smoothing is on, also a placeholder for future unseen tokens).\n",
    "\n",
    "**Tip.** Students have found the Laplace smoothing procedure tricky in the past, so if you are spending too much time on it because your new to programming, it's okay to just study the solution. That said, try to get the non-smoothed version right, as that one is relatively simple.\n",
    "\n",
    "See API below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLszzDgFhgiQ"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_labels(data_y):\n",
    "    \"\"\"Return the set of labels in an observed dataset\"\"\"\n",
    "    return set(data_y)\n",
    "\n",
    "def get_vocabulary(data_x, alpha=0., EOS='-EOS-', UNK='-UNK-'):\n",
    "    \"\"\"\n",
    "    Return the set of known words in an observed collection of documents. \n",
    "    To those we add the EOS symbol which needs to be in the support of the distribution, otherwise\n",
    "     the Naive Bayes model does not have a 'stop criterion'.\n",
    "    And, possibly, a placeholder for future unknown words, when we are using Laplace smoothing.\n",
    "    \"\"\"\n",
    "    vocab = set(chain(*data_x))\n",
    "    vocab.add(EOS) # should always be part of the vocabulary\n",
    "    if alpha > 0.:  # we are reserving mass for future unseen words\n",
    "        vocab.add(UNK)\n",
    "    return vocab\n",
    "\n",
    "def estimate_pi(data_x, data_y, alpha=0., EOS='-EOS-', UNK='-UNK-'):\n",
    "    \"\"\"\n",
    "    Return the dict that stores the parameters of the class-conditioned distributions over vocabulary.\n",
    "    \n",
    "    data_x: list of documents, each document is a list of tokens, each token is a string\n",
    "    data_y: list of labels, each label is a string\n",
    "    \n",
    "        Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
    "    alpha: the Laplace smoothing coefficient, \n",
    "     a virtual count that gets added to every outcome that has been seen\n",
    "     including all outcomes that have not been seen \n",
    "     (at this point we cannot know what new tokens will be seen in the future, \n",
    "      so we will be using a placeholder token for that, the special UNK token)\n",
    "    EOS: a special token to be used as the end-of-sentence marker\n",
    "     which you should pretend occurs at the end of every document\n",
    "    UNK: a special token to be used as a placeholder for all unseen tokens\n",
    "     this will only be used if you enable Laplace smoothing  (alpha > 0)  \n",
    "        \n",
    "    Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
    "    \"\"\"\n",
    "    if alpha < 0:\n",
    "        raise ValueError(\"Laplace smoothing requires a positive alpha\")\n",
    "        \n",
    "    support_Y = get_labels(data_y) # all classes in the corpus\n",
    "    vocab = get_vocabulary(data_x, alpha=alpha, EOS=EOS, UNK=UNK)\n",
    "    V = len(vocab)\n",
    "\n",
    "    raise NotImplemented(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqVdrWjThgiQ"
   },
   "source": [
    "<details>\n",
    "    <summary><b>Solution</b></summary>\n",
    "    \n",
    "```python\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def estimate_pi(data_x, data_y, alpha=0., EOS='-EOS-', UNK='-UNK-'):\n",
    "    \"\"\"\n",
    "    Return the dict that stores the parameters of the class-conditioned distributions over vocabulary.\n",
    "    \n",
    "    data_x: list of documents, each document is a list of tokens, each token is a string\n",
    "    data_y: list of labels, each label is a string\n",
    "    \n",
    "        Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
    "    alpha: the Laplace smoothing coefficient, \n",
    "     a virtual count that gets added to every outcome that has been seen\n",
    "     including all outcomes that have not been seen \n",
    "     (at this point we cannot know what new tokens will be seen in the future, \n",
    "      so we will be using a placeholder token for that, the special UNK token)\n",
    "    EOS: a special token to be used as the end-of-sentence marker\n",
    "     which you should pretend occurs at the end of every document\n",
    "    UNK: a special token to be used as a placeholder for all unseen tokens\n",
    "     this will only be used if you enable Laplace smoothing  (alpha > 0)  \n",
    "        \n",
    "    Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
    "    \"\"\"\n",
    "    # **SOLUTION**\n",
    "    \n",
    "    if alpha < 0:\n",
    "        raise ValueError(\"Laplace smoothing requires a positive alpha\")\n",
    "        \n",
    "    support_Y = get_labels(data_y) # all classes in the corpus\n",
    "    vocab = get_vocabulary(data_x, alpha=alpha, EOS=EOS, UNK=UNK)\n",
    "    V = len(vocab)\n",
    "\n",
    "    # Counts for (X,Y)\n",
    "    joint_counts = dict((y, Counter()) for y in support_Y)\n",
    "    for x, y in zip(data_x, data_y):\n",
    "        counter_w_given_y = joint_counts[y]\n",
    "        counter_w_given_y.update(x + [EOS])  # we gotta pad our sentences with EOS        \n",
    "        \n",
    "    # Conditional parameters pi\n",
    "    pi = dict()\n",
    "    \n",
    "    for y in support_Y:\n",
    "        counts_w_given_y = joint_counts[y]\n",
    "        cpd = defaultdict(float)\n",
    "        total_counts = sum(counts_w_given_y.values())\n",
    "        for w in vocab:\n",
    "#         for w, n in counts_w_given_y.items():  # renormalise (dealing with virtual counts as well)\n",
    "            cpd[w] = (counts_w_given_y.get(w, 0) + alpha) / (total_counts + alpha * V)\n",
    "        pi[y] = cpd\n",
    "    \n",
    "    return pi\n",
    "```\n",
    "    \n",
    "---    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp-t2tHWhgiR"
   },
   "source": [
    "These are the tests for non-smoothed models ($\\alpha=0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6zJTSdihgiR"
   },
   "outputs": [],
   "source": [
    "test_result_pi_1 = estimate_pi(test_data_x_1, test_data_y_1)\n",
    "\n",
    "# First of all we get proper cpds\n",
    "assert all(validate_categorical_cpd(cpd) for y, cpd in test_result_pi_1.items())\n",
    "\n",
    "# There are 8 occurrences of 'bad' in negative documents\n",
    "# and putting all negative documents together we have 20 words (already taking EOS tokens into account)\n",
    "assert np.isclose(test_result_pi_1['neg']['bad'], 8/20, 0.01)\n",
    "\n",
    "# There are 2 occurrences of 'so' in negative documents\n",
    "# and putting all negative documents together we have 20 words (already taking EOS tokens into account)\n",
    "assert np.isclose(test_result_pi_1['neg']['so'], 2/20, 0.01)\n",
    "\n",
    "# There are 0 occurrences of 'good' in negative documents\n",
    "# and putting all negative documents together we have 20 words (already taking EOS tokens into account)\n",
    "assert np.isclose(test_result_pi_1['neg']['good'], 0/20, 0.01)\n",
    "\n",
    "# There are 2 occurrences of 'so' in positive documents\n",
    "# and putting all negative documents together we have 20 words (already taking EOS tokens into account)\n",
    "assert np.isclose(test_result_pi_1['pos']['so'], 2/22, 0.01)\n",
    "\n",
    "# EOS artificially occurs once per document\n",
    "assert np.isclose(test_result_pi_1['neg']['-EOS-'], 5/20, 0.01)\n",
    "assert np.isclose(test_result_pi_1['pos']['-EOS-'], 7/22, 0.01)\n",
    "\n",
    "#assert np.isclose(test_result_pi_1['neg']['-EOS-'], 1/((2+4+4+4+6)/5), 0.01)\n",
    "#assert np.isclose(test_result_pi_1['pos']['-EOS-'], 1/((3+3+3+3+3+3+4)/7), 0.01)\n",
    "\n",
    "# No mass reserved for future unseen words\n",
    "assert get_cond_parameter(w=\"alright\", y=\"pos\", pi=test_result_pi_1) == 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoT-vGSBhgiR"
   },
   "source": [
    "For smoothed models (with $\\alpha>0$) we need to change our definition of `get_cond_parameter` so that it uses the probability of the UNK token when it finds an unseen token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4BteR3mhgiR"
   },
   "outputs": [],
   "source": [
    "def get_cond_parameter(y: str, w: str, pi: dict, UNK='-UNK-'):\n",
    "    \"\"\"\n",
    "    Return P(W=w|Y=y) under the model Categorical(\\pi^{(y)}).\n",
    "    \n",
    "    y: the label\n",
    "    w: word\n",
    "    pi: a dictionary of dictionaries \n",
    "        first we can index it using a label to obtain a dict as return, \n",
    "        the latter dict are the parameters for a distribution over the vocabulary given the label\n",
    "        and it can be indexed using a word to obtain a probability mass\n",
    "\n",
    "        even though mathematically we think of pi as a table/matrix\n",
    "        in code it can be convenient to treat it like a dictionary some times, \n",
    "        for example, as a dictionary we can use labels that are strings (rather than 0-based indices)\n",
    "        and words that are strings (rather than 0-based indices to a vocabulary).        \n",
    "        \n",
    "        For this implementation assume that every dict inside of pi\n",
    "         has already been validated as a Categorical parameter.\n",
    "    UNK: we return the probability P(W=UNK|Y=y) in case w is not in the support.\n",
    "        \n",
    "    Return: Categorical(w|\\phi^{(y)})\n",
    "    \"\"\"\n",
    "    cpd = pi.get(y, None)\n",
    "    if cpd is None:  # the label is not in the support of the model\n",
    "        return 0. \n",
    "    # we fetch the probability of the UNK symbol\n",
    "    # and use it as default for when we fetch the probability of the word w\n",
    "    return cpd.get(w, cpd.get(UNK, 0.))  # if a word is outside the support, it gets 0 mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOpumMB3hgiR"
   },
   "source": [
    "Now you can test with Laplace smoothing on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcSkJlddhgiR"
   },
   "outputs": [],
   "source": [
    "test_result_pi_2 = estimate_pi(test_data_x_1, test_data_y_1, alpha=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04BXty0OhgiS"
   },
   "outputs": [],
   "source": [
    "# First of all we get proper cpds\n",
    "assert all(validate_categorical_cpd(cpd) for y, cpd in test_result_pi_2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfFORTPUhgiS"
   },
   "outputs": [],
   "source": [
    "# The first 7 documents are positive:\n",
    "#  number of words in positive docs \n",
    "#  + number of occurrences of EOS \n",
    "#  + one virtual count per word in vocab (which includes UNK and EOS)\n",
    "assert sum(len(x) for x in test_data_x_1[:7]) + 7 + 1 * len(get_vocabulary(test_data_x_1, alpha=1.0)) == 33\n",
    "# Seen word, but not seen with this label\n",
    "assert get_cond_parameter(w=\"really\", y=\"pos\", pi=test_result_pi_2) == 1/33\n",
    "# Unseen word\n",
    "assert get_cond_parameter(w=\"alright\", y=\"pos\", pi=test_result_pi_2) == 1/33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvO8heGVhgiS"
   },
   "outputs": [],
   "source": [
    "# The last 5 documents are negative:\n",
    "#  number of words in neagtive docs \n",
    "#  + number of occurrences of EOS \n",
    "#  + one virtual count per word in vocab (which includes UNK and EOS)\n",
    "assert sum(len(x) for x in test_data_x_1[-5:]) + 5 + 1 * len(get_vocabulary(test_data_x_1, alpha=1.0)) == 31\n",
    "# Seen word, but not seen with this label\n",
    "assert get_cond_parameter(w=\"quite\", y=\"neg\", pi=test_result_pi_2) == 1/31\n",
    "# Unseen word\n",
    "assert get_cond_parameter(w=\"alright\", y=\"neg\", pi=test_result_pi_2) == 1/31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9hlogR2hgiS"
   },
   "source": [
    "**Congratulations! You've implemented every aspect of an NB classifier :D**\n",
    "\n",
    "The rest of this section will just organise the code and put it for you in a single \"package\", so that you can use it in an experiment (which you will conduct in the next section).\n",
    "\n",
    "This section also contains a small demonstration of how you can use the newly developed NB classifier.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "When we have a number of related functionalities (e.g., get parameters, combine them into joint probabilities, marginal probabilities, use them for predictions, code for estimating the parameters themselves, etc.), it is convenient to group all these functionalities into something like a container.\n",
    "\n",
    "This is called a *class* in many programming languages. Think of classes as templates for instantiating objects that store data and code for you to reuse over and over in different situations.\n",
    "\n",
    "You don't need to be able write a class yourself, but it is useful to study the one below. \n",
    "\n",
    "In this course, programming skills *are not* assessed in exams, so studying a class definition and learning to use such a concept is a skill that will help your programming and it is a practical skill of great importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrU-YxBshgiS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain, takewhile\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, labels: list, vocab=[], alpha=0., EOS='</s>', UNK='<unk>', seed=None):\n",
    "        \"\"\"\n",
    "        This constructs a NaiveBayesClassifier built upon a joint distribution over X and Y\n",
    "        where X are documents and Y is a label.\n",
    "        \n",
    "        The set of labels is finite and known ahead of time.\n",
    "\n",
    "        Every document X is a sequence of tokens, a token W is a word in a finite vocabulary.\n",
    "        We can start with a given vocabulary if we already know it, \n",
    "         but in any case, this class offers a training procedure which will use the words in a given corpus\n",
    "         as the set of known words in the vocabulary.\n",
    "         \n",
    "        The NBC needs two special symbols for practical purposes. The end-of-sequence symbol (EOS)\n",
    "         helps the NBC stop generating, when the model is used for generation of new documents.\n",
    "         The UNK token is a placeholder for every unseen word we may encounter in the future.\n",
    "         \n",
    "        Without training, this NBC uses uniform cpds for every cpd in the model\n",
    "\n",
    "        With training, this NBC may uses Laplace smoothing (if you set alpha to something greater than 0.).\n",
    "        \n",
    "        After the class definition you will find a demonstration of how to use it.\n",
    "        \n",
    "        :param labels: a list of classes, each a string or integer\n",
    "        :param alpha: the smoothing coefficient for distributions of the kind X|Y=y\n",
    "        :param EOS: the EOS symbol\n",
    "        :param UNK: the UNK symbol for smoothing\n",
    "        :param seed: random generator seed, fix this for reproducibility\n",
    "        \"\"\"\n",
    "        self._EOS = EOS\n",
    "        self._UNK = UNK\n",
    "        self._alpha = alpha        \n",
    "        self._labels = tuple(labels)\n",
    "        self._rng = rng = np.random.RandomState(seed)  # Good for reproducibility\n",
    "        # The method fit will populate these\n",
    "        self._vocab = set(vocab)\n",
    "        self._vocab.add(EOS)\n",
    "        self._vocab.add(UNK)\n",
    "        # we initialise our distributions with uniform probabilities\n",
    "        #  but we will use a trick, if these objects are None, \n",
    "        #  the methods that read them will return a uniform probability by default\n",
    "        #  this way we do not need to do anything at this point\n",
    "        self._prior_probs = None\n",
    "        # intialise with nothing\n",
    "        #  we will use a trick, whenever we have None, the probabilities are going to be uniform\n",
    "        self._cond_probs = None    \n",
    "        \n",
    "    def get_prior_parameter(self, y: str):\n",
    "        \"\"\"\n",
    "        Return P(Y=y) = phi[y]\n",
    "        \n",
    "        The only difference compared to before is that this time we assume a uniform distribution\n",
    "        when the model is untrained.\n",
    "        \"\"\"\n",
    "        return 1/len(self._labels) if self._prior_probs is None else self._prior_probs[y]    \n",
    "    \n",
    "    def get_cond_parameter(self, y: str, w: str):\n",
    "        \"\"\"\n",
    "        Return P(W=w|Y=y) = pi[y, w]\n",
    "        \n",
    "        If the model is untrained, we assume the probaiblity is uniform over the vocabulary.\n",
    "        \"\"\"\n",
    "        if self._cond_probs is None:\n",
    "            # return uniform probability if the model isn't trained\n",
    "            return 1/len(self._vocab) \n",
    "        else:\n",
    "            # prob of unk|y\n",
    "            unk_prob = self._cond_probs[y].get(self._UNK, 0.0)\n",
    "            # return w|y if w is in the dictionary, if not return unk|y\n",
    "            return self._cond_probs[y].get(w, unk_prob)     \n",
    "        \n",
    "    def log_prior_prob(self, y: str):\n",
    "        \"\"\"Return log P(Y=y)\"\"\"\n",
    "        return np.log(self.get_prior_parameter(y))\n",
    "        \n",
    "    def log_conditional_prob(self, x: list, y: str):\n",
    "        \"\"\"\n",
    "        Return log P(X=x|Y=y)\n",
    "        \"\"\"\n",
    "        return sum(np.log(self.get_cond_parameter(y=y, w=w)) for w in x)  # named arguments help us not make mistakes\n",
    "    \n",
    "    def log_joint_prob(self, y: str, x: list):\n",
    "        \"\"\"Return log P(Y=y) + log P(X=x|Y=y)\"\"\"\n",
    "        return self.log_prior_prob(y) + self.log_conditional_prob(x=x, y=y)  # named arguments help us not make mistakes\n",
    "     \n",
    "    def log_marginal_prob(self, x: list):\n",
    "        \"\"\"Return log P(X=x) = log \\sum_y P(Y=y, X=x)\"\"\"\n",
    "        # the function logaddexp(a, b) returns log(exp(a) + exp(b))\n",
    "        # the method reduce applies that in a row to a whole list of values \n",
    "        return np.logaddexp.reduce([self.log_joint_prob(y=y, x=x) for y in self._labels])  # named arguments help us not make mistakes\n",
    "    \n",
    "    def log_posterior_prob(self, y: str, x: list):\n",
    "        \"\"\"Return log P(Y=y|X=x) under the model\"\"\"\n",
    "\n",
    "        # The posterior probability is a conditional probability\n",
    "        #  by definition that is P(Y=y,X=x) / P(X=x)\n",
    "        #  and we have already implemented the joint probability in the numerator\n",
    "        log_joint = self.log_joint_prob(y=y, x=x)\n",
    "        #  and the marginal probability in the denominator\n",
    "        log_marginal = self.log_marginal_prob(x)\n",
    "        # As we are in computing it in log space, we can subtract the logs to divide the probs\n",
    "        return log_joint - log_marginal\n",
    "    \n",
    "    def predict(self, data_x: list):\n",
    "        \"\"\"\n",
    "        :param data_x: a list of sentences to be classified, each sentence is a list of tokens (each token is a string)\n",
    "\n",
    "        :return: a list of predictions, one per sentence in data_x, \n",
    "            each prediction is the label with highest posterior probability given the input\n",
    "        \"\"\"        \n",
    "        y_pred = []\n",
    "        for x in data_x:            \n",
    "            # the label with highest joint probability with x is also \n",
    "            # the label with the highest posterior probability (since P(X=x) is fixed)\n",
    "            # also, we can do everything in log domain\n",
    "            best_k = np.argmax([self.log_joint_prob(y=y, x=x) for y in self._labels])\n",
    "            y_pred.append(self._labels[best_k])\n",
    "        return y_pred  \n",
    "    \n",
    "    def fit(self, data_x: list, data_y: list):\n",
    "        \"\"\"\n",
    "        Return the dict that stores the parameters of the class-conditioned distributions over vocabulary.\n",
    "\n",
    "        data_x: list of documents, each document is a list of tokens, each token is a string\n",
    "        data_y: list of labels, each label is a string\n",
    "\n",
    "            Documents and labels are paired, that is zip(data_x, data_y) will give you (x,y) pairs.\n",
    "        alpha: the Laplace smoothing coefficient, \n",
    "         a virtual count that gets added to every outcome that has been seen\n",
    "         including all outcomes that have not been seen \n",
    "         (at this point we cannot know what new tokens will be seen in the future, \n",
    "          so we will be using a placeholder token for that, the special UNK token)\n",
    "        EOS: a special token to be used as the end-of-sentence marker\n",
    "         which you should pretend occurs at the end of every document\n",
    "        UNK: a special token to be used as a placeholder for all unseen tokens\n",
    "         this will only be used if you enable Laplace smoothing  (alpha > 0)  \n",
    "\n",
    "        Return a dict that maps from label to its probability mass using MLE as estimation algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make the set of classes\n",
    "        support_Y = self._labels\n",
    "        assert set(self._labels) == set(data_y) # we need observations for all classes \n",
    "        \n",
    "        # Make the vocabulary of words\n",
    "        # Vocabulary (support of W is all known words)\n",
    "        self._vocab = set(chain(*data_x))\n",
    "        # plus the special EOS token\n",
    "        self._vocab.add(self._EOS)\n",
    "        if self._alpha > 0.:\n",
    "            # and the special UNK token\n",
    "            self._vocab.add(self._UNK)\n",
    "        V = len(self._vocab)\n",
    "\n",
    "        # MLE for Y\n",
    "        N = len(data_y)\n",
    "        # count_Y(y) divided by total data points N (no need for smoothing here)\n",
    "        self._prior_probs = dict((y, float(n) / N) for y, n in Counter(data_y).items())\n",
    "        \n",
    "        # MLE for W|Y\n",
    "        # Counts for (X,Y)\n",
    "        joint_counts = dict((y, Counter()) for y in support_Y)\n",
    "        for x, y in zip(data_x, data_y):\n",
    "            counter_w_given_y = joint_counts[y]\n",
    "            counter_w_given_y.update(x + [self._EOS])  # we gotta pad our sentences with EOS        \n",
    "\n",
    "        # Conditional parameters\n",
    "        self._cond_probs = dict()\n",
    "\n",
    "        for y in support_Y:\n",
    "            counts_w_given_y = joint_counts[y]\n",
    "            cpd = defaultdict(float)\n",
    "            total_counts = sum(counts_w_given_y.values())\n",
    "            for w in self._vocab:\n",
    "                cpd[w] = (counts_w_given_y.get(w, 0) + self._alpha) / (total_counts + self._alpha * V)\n",
    "            self._cond_probs[y] = cpd\n",
    "    \n",
    "    def sample_n(self, num_samples=1, y=None, max_length=200):\n",
    "        \"\"\"\n",
    "        Sample a number of times from the joint distribution, or from the conditional\n",
    "        :param num_samples: how many samples do we want to generate\n",
    "        :param y: if given, sample from S|Y=y, otherwise from Y and then from S|Y    \n",
    "        :param max_length: maximum length for s\n",
    "        :return: a generator of samples, each a pair (y, x)\n",
    "        \"\"\"   \n",
    "        \n",
    "        # This algorithm is a bit tricky, \n",
    "        #  you do not need to study it necessarily\n",
    "        # Here's what it does conceptually\n",
    "        # for _ in range(S)\n",
    "        #  draw Y from Cat(phi)\n",
    "        #  draw W[i]|Y=x from Cat(pi[y]) until we draw an EOS symbol\n",
    "        # But, to make it a bit more efficient, we draw\n",
    "        #  S times from Cat(phi)\n",
    "        # Then for each of those draws s=1, ..., S we draw\n",
    "        #  L times from Cat(pi[y[s]]) where L is max length\n",
    "        #  we then find the position of the first EOS and discard from there onwards\n",
    "        \n",
    "        if y is None: # Draw num_samples times from P_Y\n",
    "            # np.random.choice returns an integer in [0, a) and p is the discrete probability distribution it samples from\n",
    "            # we obtain num_samples such samples\n",
    "            ids = self._rng.choice(len(self._labels), p=[self.get_prior_parameter(k) for k in self._labels], size=num_samples)\n",
    "            # these are the sampled labels\n",
    "            sampled_labels = [self._labels[k] for k in ids]\n",
    "        else:  # or repeat the given label num_samples times\n",
    "            # here the sampled_labels are whatever the user chose via argument y\n",
    "            sampled_labels = [y for _ in range(num_samples)]\n",
    "\n",
    "        # here we turn the support of the conditionals into a list for easy access\n",
    "        vocab = [w for w in self._vocab]\n",
    "        vocab_size = len(vocab)        \n",
    "        # here we turn the conditionals into np arrays so it's easier to sample from\n",
    "        probs = dict()        \n",
    "        for label in set(sampled_labels):\n",
    "            probs[label] = np.array([self.get_cond_parameter(y=label, w=w) for w in self._vocab])\n",
    "\n",
    "        # for each sampled label, we also sample a document\n",
    "        for label in sampled_labels:\n",
    "            # I know use numpy to sample max_length tokens following the distribution W|Y=y\n",
    "            # I also map numpy's choices (which are indices) to words using `vocab`\n",
    "            seq = [vocab[v] for v in self._rng.choice(vocab_size, max_length, p=probs[label])]\n",
    "            # Finally, I throw away whatever happens after the first occurrence of EOS\n",
    "            # since the sampler should have stopped at that point\n",
    "            # (this wastes a bit of computation, but it's okay for our demonstration)\n",
    "            yield (label, list(takewhile(lambda w: w != self._EOS, seq)) )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JBBYlrOhgiT"
   },
   "source": [
    "Here is a **toy demonstration** to help you with debugging your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plyvZ6f6hgiT"
   },
   "outputs": [],
   "source": [
    "# an untrained model with \n",
    "toy_nbc = NaiveBayesClassifier(['neg', 'pos'], vocab=['pepper', 'the', 'dog'], alpha=0.01, seed=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q77O2PjhgiT"
   },
   "source": [
    "See that you can sample from an untrained model. That's because an untrained model *is* a probability distribution nonetheless. It's just a not very good one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnSAGIjvhgiT"
   },
   "outputs": [],
   "source": [
    "for y, x in toy_nbc.sample_n(10):\n",
    "    print('{}: {}'.format(y, ' '.join(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgnoP5HYhgiU"
   },
   "source": [
    "Here's a demonstration of how to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BurwPN9MhgiU"
   },
   "outputs": [],
   "source": [
    "toy_nbc.fit(\n",
    "    [\n",
    "        'this is bad'.split(), \n",
    "        'this is really good'.split(),\n",
    "        'this is pretty good'.split(),\n",
    "        'this is great'.split(),\n",
    "    ], ['neg', 'pos', 'pos', 'pos']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IU8l6GUDhgiU"
   },
   "outputs": [],
   "source": [
    "assert toy_nbc._prior_probs == {'neg': 0.25, 'pos': 0.75},  \"Note the toy example has 1 negative and 3 positive instances, so we expect 1/4 and 3/4 as probability, not: {}\".format(toy_nbc._phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iM6lELaChgiU"
   },
   "outputs": [],
   "source": [
    "assert toy_nbc._vocab == {'</s>', '<unk>', 'bad', 'good', 'great', 'is', 'pretty', 'really', 'this'}, \"Did you perhaps forget to create a vocab or to add the UNK and EOS tokens?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqpO4AlnhgiU"
   },
   "outputs": [],
   "source": [
    "assert toy_nbc.get_cond_parameter(y='neg', w='pretty') == toy_nbc.get_cond_parameter(y='neg', w='<unk>'), \"The word 'pretty' was never seen by\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q72AaWQNhgiV"
   },
   "source": [
    "Note that the distributions should normalise over the complete sample space (which includes EOS and UNK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuEUyd95hgiV"
   },
   "outputs": [],
   "source": [
    "assert np.isclose(sum(toy_nbc.get_cond_parameter(y='neg', w=w) for w in toy_nbc._vocab), 1, 1e-3), \"Did you smooth things correctly? Try again with alpha=0.0, then see if you can fix it for alpha > 0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPIVQLg7hgiW"
   },
   "outputs": [],
   "source": [
    "assert np.isclose(sum(toy_nbc.get_cond_parameter(y='pos', w=w) for w in toy_nbc._vocab), 1, 1e-3), \"Did you smooth things correctly? Try again with alpha=0.0, then see if you can fix it for alpha > 0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDsTYGYqhgiW"
   },
   "source": [
    "Now you can use the model for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3WSf-jghgiX"
   },
   "outputs": [],
   "source": [
    "toy_pred = toy_nbc.predict(\n",
    "    [\n",
    "        'this is really bad'.split(), \n",
    "        'this is good'.split(), \n",
    "        'this is pretty'.split()\n",
    "    ]\n",
    ")\n",
    "assert toy_pred == ['neg', 'pos', 'pos'], \"Something seems to have gone wrong with your predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8cALK0chgiX"
   },
   "source": [
    "and for computing probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMDZojhshgiX"
   },
   "outputs": [],
   "source": [
    "assert toy_nbc.log_conditional_prob(x='this is really bad'.split(), y='neg') > toy_nbc.log_conditional_prob(x='this is really bad'.split(), y='pos'), \"The negative LM likes this sentence more\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se2LYfh5hgiY"
   },
   "outputs": [],
   "source": [
    "assert toy_nbc.log_conditional_prob(x='this is really bad'.split(), y='neg') >  toy_nbc.log_marginal_prob(x='this is really bad'.split()) > toy_nbc.log_conditional_prob('this is really bad'.split(), y='pos'), \"The marginal probability is in between\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOM1MxTnhgiY"
   },
   "source": [
    "You can even sample from the generative story of the model. \n",
    "\n",
    "See how about 3/4 of the samples will be positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JN1m-8CVhgiY"
   },
   "outputs": [],
   "source": [
    "assert np.isclose(sum(1 for y, s in toy_nbc.sample_n(1000) if y == 'pos')/1000, 0.75, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uA0ybwRUhgiY"
   },
   "outputs": [],
   "source": [
    "# Random generation:\n",
    "for y, x in toy_nbc.sample_n(10):\n",
    "    print('{}: {}'.format(y, ' '.join(x)))\n",
    "    \n",
    "print('\\nOut of 100 samples we have {} positive.'.format(sum(1 for y, s in toy_nbc.sample_n(100) if y == 'pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP1VbZLuhgiY"
   },
   "source": [
    "We can also choose to sample conditionally, in this case, we ask for samples from the *positive* class only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTbZadrvhgiZ"
   },
   "outputs": [],
   "source": [
    "for y, s in toy_nbc.sample_n(10, y='pos'):\n",
    "    print('{}: {}'.format(y, ' '.join(s)))\n",
    "\n",
    "print('\\nAverage length in 100 positive samples is {:.2f}'.format(sum(len(s) for y, s in toy_nbc.sample_n(100, y='pos'))/100.))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeVshdrRhgiZ"
   },
   "source": [
    "Same can be done for the negative class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unV4PI2AhgiZ"
   },
   "outputs": [],
   "source": [
    "for y, s in toy_nbc.sample_n(10, y='neg'):\n",
    "    print('{}: {}'.format(y, ' '.join(s)))\n",
    "\n",
    "print('\\nAverage length in 100 positive samples is {:.2f}'.format(sum(len(s) for y, s in toy_nbc.sample_n(100, y='neg'))/100.))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8q4ms54hgiZ"
   },
   "source": [
    "And of course, we can check posterior probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjkinbM7hgiZ"
   },
   "outputs": [],
   "source": [
    "np.exp(toy_nbc.log_posterior_prob(x=\"too bad\".split(), y=\"neg\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkGU6YFUhgia"
   },
   "outputs": [],
   "source": [
    "np.exp(toy_nbc.log_posterior_prob(x=\"too bad\".split(), y=\"pos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xupwf5wvhgia"
   },
   "source": [
    "The NB classifier *supports* actual documents, that is, it assigns non-zero probability to actual documents. But it also supports plenty of sequences that are not at all plausible, even after training. This may seem strange at first, but it is actually to be expected, given that it makes too strong assumptions (it is a \"naive\" model after all).\n",
    "\n",
    "This does not mean the model is bad. Generating documents is not its primary *application*, even though being able to generate a document is part of its fundamental design. We call it a \"document\", but for the model it is just a collection of words that are considered related to classes, theses words are not in order, and the model has no clue what these words mean for a human. \n",
    "\n",
    "The primary application of NBC is to represent our uncertainty about the categorisation of a document, so its primary application are inferences we make based on the posterior distribution (or the joint distribution, depending on the purpose).  In the next section we will use the model for classification. \n",
    "\n",
    "We will also play with it as a sampler, just to see what we get, but don't hope for much yet :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKl90gsUhgia"
   },
   "source": [
    "## <a name=\"exp\">  Experiment\n",
    "\n",
    "\n",
    "We will now try our NBC on one of NLTK's datasets.\n",
    "\n",
    "As usual in testing a machine learning model, we will need training data, development data, and test data. Check [Section 4.8](https://web.stanford.edu/~jurafsky/slp3/4.pdf) for a recap of how to train/test a machine learning classifier. There's no need to implement cross-validation (a training/development/test split is sufficient, and we provide helper code for splitting the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnsUjYw4hgia"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def split_corpus(sentences, ratio=0.9):\n",
    "    \"\"\"\n",
    "    Randomly split a list of sentences into two sets according to the given ratio.\n",
    "    \n",
    "    :param sentences: already tokenized sentences (list of strings)\n",
    "    \"\"\"\n",
    "    # This will guarantee that the permutation is the same every time (which is important for reproducibility)\n",
    "    rng = np.random.RandomState(42)    \n",
    "    indices = rng.permutation(len(sentences))\n",
    "    n = int(indices.size * ratio)\n",
    "    return [sentences[i] for i in indices[:n]], [sentences[i] for i in indices[n:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWOX_HfAhgia"
   },
   "source": [
    "Let's attempt to predict sentence polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf3WRmlrhgia"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentence_polarity\n",
    "\n",
    "pos_sents = sentence_polarity.sents(categories='pos')\n",
    "neg_sents = sentence_polarity.sents(categories='neg')\n",
    "print(len(pos_sents), 'positive sentences such as:\\n', ' '.join(pos_sents[0]))\n",
    "print(len(neg_sents), 'negative sentences such as:\\n', ' '.join(neg_sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rmnOq43hgia"
   },
   "source": [
    "And here is a reasonable split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axqTbDvjhgia"
   },
   "outputs": [],
   "source": [
    "training_pos, dev_pos = split_corpus(pos_sents, 0.8)\n",
    "dev_pos, test_pos = split_corpus(dev_pos, 0.5)\n",
    "training_neg, dev_neg = split_corpus(neg_sents, 0.8)\n",
    "dev_neg, test_neg = split_corpus(dev_neg, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nz-HwpbVhgia"
   },
   "outputs": [],
   "source": [
    "rows = [\n",
    "    ['pos', len(training_pos), len(dev_pos), len(test_pos)],\n",
    "    ['neg', len(training_neg), len(dev_neg), len(test_neg)]\n",
    "]\n",
    "print(tabulate(rows, headers=['Label', 'Training', 'Development', 'Test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNODcGWWhgia"
   },
   "source": [
    "<a name=\"ex-NBC\"> **Graded Exercise - NBC for sentence polarity** \n",
    "    \n",
    "Fit NBC, evaluate it on dev set. To recap how you evaluate a classifier, check [Section 4.7](https://web.stanford.edu/~jurafsky/slp3/4.pdf). \n",
    "\n",
    "For this use $\\alpha=1.0$. For evaluation you can use `sklearn.metrics import classification_report`, check its documentation. You can install scikit-learn via `!pip install sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch__6tn0kNAQ"
   },
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5g8JEFQhgib"
   },
   "source": [
    "<a name=\"ex-grid\"> **Graded Exercise - Grid Search** \n",
    "    \n",
    "Grid-search for a good value of $\\alpha$, use the grid shown below. Use performance on *development* set to pick the best value of the hyperparameter. Do report the dev performance for each value in the grid, you may use a table and/or a plot. If your data are not balanced (e.g., brown) you should choose the precise metric carefully (not every metric in scikit's evaluation report is equally good with class imbalance). For the best value of $\\alpha$ then test your NBC on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPMgNPPshgib"
   },
   "outputs": [],
   "source": [
    "grid_alpha = 0.1 + np.arange(0., 1.5, 0.1)\n",
    "grid_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBXLtAFihgib"
   },
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJicuwnvhgib"
   },
   "source": [
    "## Error analysis \n",
    "\n",
    "This is a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), it displays a summary of our predictions in terms of true positives (TP), false positive (FP), false negative (FN), and true negatives (TN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFFrnjolhgib"
   },
   "source": [
    "<a name=\"ex-error\"> **Graded Exercise - Error analysis** \n",
    "    \n",
    "Print the confusion matrix of your classifier using the dev set, use $\\alpha=0.1$ for training. \n",
    "\n",
    "You can build the matrix yourself, or you can use sklearn's `from sklearn.metrics import confusion_matrix`. \n",
    "\n",
    "Then, manually inspect some of the mistakes (FP or FN) and speculate about directions for improvement in the future, in particular, find (and list) examples of **two or more** patterns that the model fails to recognise **because** of its assumptions. Explain how you think the model assumptions are responsible for the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDpeP022kR_K"
   },
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2022/T2_student.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
