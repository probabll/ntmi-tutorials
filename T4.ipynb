{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-28wv8VtwhYL"
   },
   "source": [
    "# Guide\n",
    "\n",
    "Check the guide carefully before starting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2G419Prwmw1"
   },
   "source": [
    "## ILOs\n",
    "\n",
    "After completing this lab you should be able to \n",
    "\n",
    "* implement applications of word embeddings\n",
    "* recognise biases and stereotypes that skip-gram models carry over from the data used to train them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf4ENSjqwtN5"
   },
   "source": [
    "## General notes\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3.\n",
    "\n",
    "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8-BhGlnjaWP"
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "* [Word Embeddings](#emb)\n",
    "    * [Vocabulary](#vocab)\n",
    "    * [Embedding matrix](#matrix)\n",
    "* [Applications of word embeddings](#applications)\n",
    "* [Bias in embeddings](#bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2tQNgfRxrNr"
   },
   "source": [
    "## Table of graded exercises\n",
    "\n",
    "Exercises have equal weights.\n",
    "\n",
    "* [Applications of word embeddings](#ex-applications)\n",
    "* [Bias in embeddings](#ex-bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUKZiCowxxh7"
   },
   "source": [
    "## How to use this notebook\n",
    "\n",
    "Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
    "\n",
    "Note that, as always, the notebook recaps theory, and contains solved quizzes. While you should probably make use of this theory recap, be careful not to spend disproportionately more time on this than you should. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd_T9V21QLA7"
   },
   "source": [
    "## Setting up\n",
    "\n",
    "Here we set up the packages that you will need to install for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO-44bwnjaWR"
   },
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aRjPM4kjaWS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np \n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"emb\"/> Word Embeddings\n",
    "\n",
    "A *word embedding* is a continuous vector representation of a word, typically optimised to capture an aspect of that word's usage in natural language data. \n",
    "\n",
    "Notation:\n",
    "* $\\mathcal W$ is a vocabulary of known words;\n",
    "* $w \\in \\mathcal W$ is a known word;\n",
    "* $\\mathrm{emb}(w; \\theta) \\in \\mathbb R^D$ is the $D$-dimensional representation of $w$ by an embedding function $\\mathrm{emb}(\\cdot; \\theta)$ with parameters $\\theta$;\n",
    "\n",
    "For a finite vocabulary of known words (let's use $V = |\\mathcal W|$ as the vocabulary size), we can store this embedding function using a table $\\mathbf E \\in \\mathbb R^{V\\times D}$, where each row stores the $D$-dimensional vector representation of one of the known words (we also need to store the 1-to-1 mapping between known words and row indices). \n",
    "\n",
    "Assuming this table is stored in memory, we can obtain the word embedding for a known word $w$ in different ways. For example, if we can use a dictionary lookup to obtain the row number associated with a known word, $i = \\mathrm{lookup}(w)$, and then use the row number to retrieve the correct word vector $\\mathbf E_i$.\n",
    "\n",
    "We will start by manipulating embeddings stored this way, for that we will create both the vocabulary dictionary (a python class) and the embedding matrix (a numpy array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"vocab\"/> Vocabulary\n",
    "\n",
    "Let's start by creating a data structure to manage a vocabulary of known words. \n",
    "\n",
    "This data structure can:\n",
    "* encode a token (a string) as a code (an integer)\n",
    "* decode a code (an integer) into a token (a string) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "\n",
    "    def __init__(self, unk_token=\"<unk>\"):\n",
    "        # we reserve position 0 for a placeholder token for words never seen before\n",
    "        self.unk_token = unk_token\n",
    "        self.w2i = {unk_token: 0}\n",
    "        self.i2w = [unk_token]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of known symbols (including any placeholder symbols)\"\"\"\n",
    "        return len(self.i2w)\n",
    "        \n",
    "    def add(self, token: str):\n",
    "        \"\"\"Add a token to the vocabulary (if not there) and return its code.\"\"\"\n",
    "        i = self.w2i.get(token, None)\n",
    "        if i is None:\n",
    "            i = len(self.i2w)\n",
    "            self.i2w.append(token)\n",
    "            self.w2i[token] = i\n",
    "        return i\n",
    "    \n",
    "    def encode(self, token: str):\n",
    "        \"\"\"Return the numerical code of a token. Unknown tokens are mapped to the code of the unk_token string\"\"\"\n",
    "        return self.w2i.get(token, self.w2i[self.unk_token])\n",
    "    \n",
    "    def decode(self, key: int):\n",
    "        \"\"\"Return the string associated with a code. Unknown codes are mapped to unk_token.\"\"\"\n",
    "        return self.i2w[key] if key < len(self.i2w) else self.unk_token\n",
    "    \n",
    "    def __getitem__(self, token: str):\n",
    "        \"\"\"Return the code of a known token (an exception is thrown if the token isn't known)\"\"\"\n",
    "        return self.w2i[token]\n",
    "    \n",
    "    def __contains__(self, token: str):\n",
    "        return token in self.w2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a demonstration of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_vocab = Vocabulary()\n",
    "\n",
    "print(\"Beforing adding tokens to the vocabulary encode will return 0:\")\n",
    "for w in 'the coolest things are also the most rewarding'.split():\n",
    "    print(\"\", w, example_vocab.encode(w))\n",
    "\n",
    "print(\"\\nAs we add tokens, strings are mapped to codes one-to-one:\")    \n",
    "for w in 'the coolest things are also the most rewarding'.split():\n",
    "    print(\"\", w, example_vocab.add(w))\n",
    "    \n",
    "print(\"\\nAnd now encode will return those codes too:\")\n",
    "for w in 'the coolest things are also the most rewarding'.split():\n",
    "    print(\"\", w, example_vocab.encode(w))    \n",
    "    \n",
    "print(\"\\nDecode reverses the mapping:\")\n",
    "for i in range(len(example_vocab)):\n",
    "    print(\"\", i, example_vocab.decode(i)) \n",
    "\n",
    "print(f\"\\nUnknown symbols are mapped to 0, for example here's the code for 'NTMI':\", example_vocab.encode(\"NTMI\"))\n",
    "print(f\"\\nUnknown codes are mapped to unk_token, for example here's the token for 1000:\", example_vocab.decode(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"matrix\"/> Embedding matrix\n",
    "\n",
    "In this section we will download a collection of already trained word embeddings. We use [GloVe](https://nlp.stanford.edu/projects/glove/) vectors which have been trained using text downloaded from the English Wikipedia. GloVe is very similar (though not identical) to skip-gram that we discussed in class. The details aren't important for this tutorial. To keep this demonstration lightweight we are using 50-dimensional vectors. These *aren't* the best word vectors you can find for English Wikipedia, later in this tutorial you will be able to use better ones.\n",
    "\n",
    "We provide helper code to download the embeddings and build an np.array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    \"\"\"\n",
    "    Load word embeddings and update vocab.\n",
    "    :param path: path to word embedding file\n",
    "    :return: a Vocabulary object and an np.array with shape (vocab_size, dim)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise RuntimeError(\"You need to download the word embeddings\")\n",
    "    vocab = Vocabulary()\n",
    "    vectors = []\n",
    "    w2i = {}\n",
    "    i2w = []\n",
    "\n",
    "    # Placeholder vector for the unknown word \n",
    "    # (once we have all word vectors we will set this to the average embedding)\n",
    "    vectors.append(None)\n",
    "\n",
    "    with gzip.open(path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(u' ', 1)\n",
    "            assert word not in vocab.w2i, \"repeated word\"\n",
    "            vocab.add(word)  \n",
    "            v = np.array(vec.split(), dtype=np.float32)\n",
    "            vectors.append(v)\n",
    "            \n",
    "    # fix the embedding of the unknown word to the average embedding\n",
    "    vectors[0] = np.mean(vectors[1:], axis=0)\n",
    "\n",
    "    return vocab, np.array(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download some word embeddings (these have been trained using English Wikipedia data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"glove.6B.50d.txt.gz\"):\n",
    "    !wget -q --show-progress https://raw.githubusercontent.com/probabll/ntmi-tutorials/master/datasets/glove.6B.50d.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNPmtj3BQHSb"
   },
   "outputs": [],
   "source": [
    "vocab, E = load_embeddings('glove.6B.50d.txt.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM08J11JQPpv"
   },
   "outputs": [],
   "source": [
    "print(f\"We have embeddings for {len(vocab)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'person' in vocab, 'ASDASDAperAJSHDKAJSh' in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abQMS0kYZPCL"
   },
   "source": [
    "Here is a helper code to make it easier to embed a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GjweiTCXAI2"
   },
   "outputs": [],
   "source": [
    "def embed(word, vocab, E):\n",
    "    \"\"\"\n",
    "    word: a word (str)\n",
    "    vocab: the Vocabulary object for our corpus and model\n",
    "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
    "\n",
    "    Return the D-dimensional embedding of the word as np.array object.\n",
    "    \"\"\"\n",
    "    if word not in vocab:\n",
    "        raise ValueError(f\"{word} is OOV\")\n",
    "    wid = vocab.w2i[word]\n",
    "    return E[wid]\n",
    "\n",
    "assert np.alltrue(embed('person', vocab, E) == E[vocab['person']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMvbD72OZXYZ"
   },
   "source": [
    "Finally, we can compare words using cosine similarity of their embeddings. Here is how we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TgiW-3kUYVu"
   },
   "outputs": [],
   "source": [
    "def cos_similarity(word1, word2, vocab, E):  \n",
    "    \"\"\"\n",
    "    word1: a word (str)\n",
    "    word2: another word (str)\n",
    "    vocab:  the Vocabulary object for our corpus and model\n",
    "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
    "\n",
    "    Return the cosine similarity (a real number) of the two words in the embedding space of our model.\n",
    "    \"\"\"  \n",
    "    # [D]    \n",
    "    u = embed(word1, vocab, E)\n",
    "    # [D]\n",
    "    v = embed(word2, vocab, E)\n",
    "    return np.sum(u * v) / (np.sqrt(np.sum(u * u)) * np.sqrt(np.sum(v * v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iu8AclU6W3Re"
   },
   "outputs": [],
   "source": [
    "cos_similarity('car', 'truck', vocab, E), cos_similarity('car', 'automobile', vocab, E), cos_similarity('car', 'burger', vocab, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyT1PSBBZ4_p"
   },
   "source": [
    "This is a function to help you whenever you need to find the top-k values in a numpy array. It will be useful in an exercise later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJFsezj5YlrO"
   },
   "outputs": [],
   "source": [
    "def np_topk(array, k=10):\n",
    "    \"\"\"\n",
    "    array: a list or a numpy np.array\n",
    "    k: number of top elements to be returned\n",
    "\n",
    "    Return the top-k elements and their indices in the array.\n",
    "    \"\"\"\n",
    "    array = np.array(array)\n",
    "    ids = np.argsort(-array)  # argsort finds the lowest values, so we use -array to find the highest values\n",
    "    # return top-k values, return the indices of the top-k values\n",
    "    return array[ids][:k], ids[:k]\n",
    "\n",
    "assert np.alltrue(np_topk([10, 20, 30, 40], 2)[0]  == np.array([40, 30]))\n",
    "assert np.alltrue(np_topk([10, 20, 30, 40], 2)[1]  == np.array([3, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"applications\"/> Application of word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HQuHjIpaYHC"
   },
   "source": [
    "<a name=\"ex-applications\"/> **Graded Exercise - Applications of word embeddings**\n",
    "\n",
    "In this exercise you will develop a few nice applications of word embeddings. \n",
    "\n",
    "1. `topk_words`: given a *word* you will find the words that are closest to it in embedding space using cosine similarity.\n",
    "\n",
    "There is a list of words that you should test your function with, you will see it below. For each word in the list, display the 10 words that are nearest in embedding space. Make sure to display the information in a way that's convenient for grading (e.g., using a table from `tabulate` or something similar).\n",
    "\n",
    "2. `doesnt_match`: given a *list of words* you will find the odd word in the list, this \"odd\" word is the one that is on average the least cosine-similar to the other words in the list.\n",
    "\n",
    "Again, there is a list of test cases for you. Make sure to display the information in a convenient format for grading.\n",
    "\n",
    "3. `word_analogy`: given two lists of words make a representation $\\mathbf v$ where the words in the *positive* list contribute positively to $\\mathbf v$, the words in the *negative* list contribute negative to $\\mathbf v$, and then return the 10 words that are cosine-closest to $\\mathbf v$ in embedding space. \n",
    "\n",
    "Again, there is a list of test cases for you. Make sure to print the information in a readable way for grading.\n",
    "\n",
    "\n",
    "**Guidelines** The grade will depend mostly on the correctness of your implementation but also on whether you displayed the requested information in a human-readable way (so the grader does not have to necessarily interact with your code). The grade *will not* depend on whether your model captures meaningful similarities in its embedding space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBvmp7cXbKqc"
   },
   "outputs": [],
   "source": [
    "def topk_words(word, vocab, E, k=10):    \n",
    "    \"\"\"\n",
    "    word: a word (str)\n",
    "    vocab:  the Vocabulary object for our corpus and model\n",
    "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
    "    k: how many nearest neighbours we want to find\n",
    "\n",
    "    Return a python list with the k words (each a string) that are nearest to the input word\n",
    "     in embedding space according to cosine similarity. You can use any of the functions provided earlier, and you\n",
    "     can also create additional ones if you need them.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3D7gHAywdPaS"
   },
   "outputs": [],
   "source": [
    "for word in ['car', 'person', 'woman', 'man']:\n",
    "    print(\"Make sure to test topk_words using:\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Adv0Jm60YbmN"
   },
   "outputs": [],
   "source": [
    "# **CONTRIBUTE YOUR SOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wb3kqL5ecr2I"
   },
   "outputs": [],
   "source": [
    "def doesnt_match(words, vocab, E):\n",
    "    \"\"\"\n",
    "    words: a list of words (each a str)\n",
    "    vocab:  the Vocabulary object for our corpus and model\n",
    "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
    "    \n",
    "    Return the word in the list that is least cosine-similar to every other word in the list on average.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNBsP04aWe8z"
   },
   "outputs": [],
   "source": [
    "for word_list in [['car', 'automobile', 'wall'], ['car', 'bridge', 'wall']]:\n",
    "    print(\"Make sure to test doesnt_match using:\", word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJtapEAvV83o"
   },
   "outputs": [],
   "source": [
    "# **CONTRIBUTE YOUR SOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQck6nJTexkL"
   },
   "outputs": [],
   "source": [
    "def word_analogy(positive: list, negative: list, vocab: Vocabulary, E, k=10):\n",
    "    \"\"\"\n",
    "    positive: a list of words (each a str) that contribute positively to the similarity\n",
    "    negative: a list of words (each a str) that contribute negatively to the similarity\n",
    "    vocab:  the Vocabulary object for our corpus and model\n",
    "    E: our model's embedding matrix as a numpy array of shape [V, D]\n",
    "    k: number of nearest neighbours\n",
    "    \n",
    "    Return the top-k words in terms of cosine similarity with the embedding you obtain by\n",
    "     summing the embedding of the words in the positive list \n",
    "     and subtracting the embedding of the words in the negative list. \n",
    "    That is, you will retrieving the neighbours of the vector:\n",
    "        \\sum_{w in positive} emb(w) - \\sum_{w in negative} emb(w)\n",
    "\n",
    "    The return is a list of neighouring words (each a str).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2IrdKiWe3-W"
   },
   "outputs": [],
   "source": [
    "for pos_list, neg_list in [(['woman', 'president'], ['man']), (['man', 'queen'], ['woman'])]:\n",
    "    print(f\"Make sure to make analogies for: postive={pos_list} negative={neg_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a5wmKiOWxNW"
   },
   "outputs": [],
   "source": [
    "# **CONTRIBUTE YOUR SOLUTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rKnCEQ0wljN"
   },
   "source": [
    "# <a name=\"bias\"/> Bias in embeddings \n",
    "\n",
    "\n",
    "In this section you will experiment with a strong pretrained embedding model that is very similar to skipgram, it's called GloVe. We are not using skipgram because the available models are much too large for this notebook, but GloVe is a **very strong** competitor. \n",
    "\n",
    "Instead of training GloVe, which would be too demanding, we will download a trained one, and interact with it using `gensim`, a very robust python package for word embeddings.  We will experiment with the same applications that you coded above, but this time you will use gensim code, this way if you made mistakes earlier, they won't affect the quality of this experiment.\n",
    "\n",
    "The goal of this exercise is that you visualise biases that embedding models carry over from their training data. A statistical objective (like MLE) is *all about statistics* and not at all about *core human values*. When we download text from the web, it may contain all sorts of stereotypes that are inadequate in many situations. For example, if we download text with gender bias and train our models, those harmful  statistics present in the data will most likely be also present in our models. There's no statistical incentive in their training objective to get rid of correlations that we think are inadequate or outdated or simply harmful. For now, we will not work on fixing the models, we will just investigate them and see that the problem is real. If you are curious to see ways to address the problem, you can check this [excellent paper](https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf), though note that checking it is optional at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTw9SL2RiZEX"
   },
   "source": [
    "First of all, make sure to install [gensim](https://radimrehurek.com/gensim/) by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pp6SxEBZoV7F"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC0Vtu8DikVM"
   },
   "source": [
    "Next, we use gensim's downloader to obtain a trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaO0zq_Prwxv"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRQK3RrOiqKo"
   },
   "source": [
    "This shouldn't take too long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VUahbuYrxMs"
   },
   "outputs": [],
   "source": [
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BT6bC7UiitJP"
   },
   "source": [
    "We can use the model in many ways, we can embed a word (the first time you run this cell it may take a moment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7QRQyFqi0fz"
   },
   "outputs": [],
   "source": [
    "word_vectors.get_vector('person')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Dv5EhRli8yY"
   },
   "source": [
    "We can retrieve similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsZw1YK6jL0S"
   },
   "outputs": [],
   "source": [
    "word_vectors.similar_by_word('person', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ8jEcIxjVOD"
   },
   "source": [
    "We can find words that do not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bu3fWwKqjUJP"
   },
   "outputs": [],
   "source": [
    "word_vectors.doesnt_match(['car', 'wall', 'automobile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cofYWEHajem3"
   },
   "source": [
    "We can make word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1BVIoWpjgUx"
   },
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['man', 'queen'], negative=['woman'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ufe9eDkji1K"
   },
   "source": [
    "And more (you can see some examples [here](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format) if you like)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pj9Rx3LUjs4N"
   },
   "source": [
    "This is a list of occupations that we got from one of the [research papers that initiated this whole investigation](https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf). These occupations are *not* gender-marked words in English, they are gender-neutral. Yet, the statistics of English data used to train GloVe are biased towards making harmful stereotypical associations with words like `woman` and `man`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPfaoQeXjsCk"
   },
   "outputs": [],
   "source": [
    "occupations = [\n",
    "    \"homemaker\",\n",
    "    \"nurse\",\n",
    "    \"receptionist\",\n",
    "    \"librarian\",\n",
    "    \"socialite\",\n",
    "    \"hairdresser\",\n",
    "    \"nanny\",\n",
    "    \"bookkeeper\",\n",
    "    \"stylist\",\n",
    "    \"housekeeper\",\n",
    "    \"maestro\",\n",
    "    \"skipper\",\n",
    "    \"protege\",\n",
    "    \"philosopher\",\n",
    "    \"capitain\",\n",
    "    \"architect\",\n",
    "    \"financier\",\n",
    "    \"warrior\",\n",
    "    \"broadcaster\",\n",
    "    \"magician\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A37-PPqZg_bZ"
   },
   "source": [
    "<a name=\"ex-bias\"> **Graded Exercise - Bias in embeddings**\n",
    "\n",
    "Using gensim functionality (code and model):\n",
    "\n",
    "1. plot the similarity of each occupation word in the list to both `woman` and `man`\n",
    "2. also, plot the difference in similarity towards `woman` with similarity towards `man` and order the occupation words by this difference. \n",
    "\n",
    "Make remarks about what you see in (1) and (2).\n",
    "\n",
    "3. Use algorithms such as `most_similar` (for word analogies), `doesnt_match` and `similar_by_word` to discover additional harmful associations in embedding space. If you want, you can investigate a different type of bias. Be **very** careful here and **very conscious** as you will likely encounter terrible associations. The goal here is not to ridicule the victims of these associations, the goal here is that you grow worried about careless use of NLP, and that you join responsible researchers in a) making careful use of NLP, and b) developing NLP that pushes back from and overcome sources of harm.  \n",
    "\n",
    "**Guideline** We will grade parts 1 and 2 in terms of the quality of your plots and the remarks you make. We will not grade part 3 as a function of how many biases you uncovered, nor as a function of whether we agree with them or not. Instead, we will use the information you display and the remarks you make as a way to assess the effort you put into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lORHSJNI1g-U"
   },
   "outputs": [],
   "source": [
    "# **CONTRIBUTE YOUR SOLUTIONS**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2022/T4_teacher",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
