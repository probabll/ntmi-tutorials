{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open this notebook on Colab](https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T4.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "jzvWfS-ELNzE",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Guide\n",
    "\n",
    "\n",
    "* Before working on this tutorial, you should have worked through the [introduction to PyTorch](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb). If you've never worked with PyTorch before, book about 4 hours to go through that notebook.\n",
    "* Once you are ready, get started with this tutorial. As always, check the entire notebook before you get started, this gives you an idea of what lies ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "KqR7WUDXLeME",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ILOs\n",
    "\n",
    "After completing this lab you should be able to\n",
    "\n",
    "* specify neural text encoders in PyTorch\n",
    "* develop neural text classifiers in PyTorch\n",
    "* estimate parameters via MLE\n",
    "* classify new data points\n",
    "* evaluate classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "YBR2bPwLL9gj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## General Notes\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$.\n",
    "* Use python3.\n",
    "* Use Torch.\n",
    "* This tutorial runs reasonably quickly on CPU. To have GPU support run this notebook on Google Colab (you will find more instructions later).\n",
    "\n",
    "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "CqDZh0QJJsOu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "toc"
    ]
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    "### Topics \n",
    "\n",
    "* [Data](#sec:Data)\n",
    "\t* [Dataset](#sec:Dataset)\n",
    "\t* [Vocabulary](#sec:Vocabulary)\n",
    "\t* [Corpus and Data Loader](#sec:Corpus_and_Data_Loader)\n",
    "* [Text Encoders](#sec:Text_Encoders)\n",
    "* [Neural Text Classifier](#sec:Neural_Text_Classifier)\n",
    "\t* [Torch Categorical](#sec:Torch_Categorical)\n",
    "\t* [Base class](#sec:Base_class)\n",
    "\t* [Average Embedding Classifier](#sec:Average_Embedding_Classifier)\n",
    "\t* [BiLSTM Classifier](#sec:BiLSTM_Classifier)\n",
    "* [Training and Evaluation](#sec:Training_and_Evaluation)\n",
    "* [Graded Experiment](#sec:Graded_Experiment)\n",
    "* [Ungraded Experiment](#sec:Ungraded_Experiment)\n",
    "\n",
    "\n",
    "### Table of ungraded exercises\n",
    "\n",
    "1. [Base class](#ungraded-1)\n",
    "1. [AvgEmbClassifier](#ungraded-2)\n",
    "1. [BiLSTMClassifier](#ungraded-3)\n",
    "1. [Training and evaluation code](#ungraded-4)\n",
    "1. [Experiment with complete dataset](#ungraded-5)\n",
    "\n",
    "\n",
    "### Table of graded exercises\n",
    "\n",
    "*Important:* The grader may re-run your notebook to investigate its correctness, but you must upload your notebook with the cells already run and make sure that all your answers are visible without the need to re-run the notebook.\n",
    "\n",
    "\n",
    "The weight of each exercise is indicated below.\n",
    "\n",
    "\n",
    "1. [Ablations for AvgEmb model](#graded-1) (62.5\\%)\n",
    "1. [Ablations for BiLSTM model](#graded-2) (37.5\\%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "fVnfg0kMLrsi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up\n",
    "\n",
    "We recommend using `jupyter lab`, `jupyter notebook` or `Google Colab`. \n",
    "\n",
    "Before anything, you should make sure to enable widgets for your Jupyter (you might need to restart your notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You should only need to run this the very first time you set up your working environment:\n",
    "\n",
    "# !pip install --upgrade ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have torch installed, or if it's too old, you can run the pip command below (you will need to restart your notebook after that). The notebook was designed on torch `2.2.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmpLiOT3o6FK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzjCfsJDNL1B"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QV4oRuW-XYED",
    "outputId": "7d6f50af-fe99-4028-fa9d-c814299ad965"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "B8qybX6RNDKh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Data'></a>\n",
    "# Data\n",
    "\n",
    "\n",
    "Here we use the labelled documents from NLTK to train text classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfbZPjfaZDdo",
    "outputId": "e0ba3159-a672-4e2c-ad7d-f05a23ba834f"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('subjectivity')\n",
    "nltk.download('sentence_polarity')\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Dataset'></a>\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZuAZ5Il891R"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import subjectivity  # binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAUGN5mVnOQn"
   },
   "source": [
    "Let's work with the _subjectivity_ dataset, it's small enough that you don't need GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdjzsF6anOQn",
    "outputId": "ec487de7-dac4-48f7-a4c0-7c77efff8ea5"
   },
   "outputs": [],
   "source": [
    "corpus = subjectivity\n",
    "labels = tuple(corpus.categories())\n",
    "print(f\"{len(corpus.sents())} labelled documents\")\n",
    "print(\"{}-way classification:\\n{}\".format(len(labels), '\\n'.join(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owiVyu00nOQo"
   },
   "source": [
    "Here we have some helper code to organise and split NLTK corpora into training/dev/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI1O8KL2nOQo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_nltk_corpus(nltk_corpus, categories, seed=23, max_length=None, BOS='-BOS-', EOS='-EOS-', portion=1.):\n",
    "    \"\"\"\n",
    "    Prepare an nltk text categorization corpus in a friendly format.\n",
    "\n",
    "    This function is very similar to what you saw in T2, but here we add BOS tokens in addition to EOS tokens\n",
    "    (while the BOS token has no effect in NBC with unigram conditionals,\n",
    "    it can be useful for some of the feature-richer classifiers we will develop here).\n",
    "\n",
    "    :param nltk_corpus: something like sentence_polarity\n",
    "    :param categories: a list of categories (each a string)\n",
    "    :param seed: for reproducibility\n",
    "    :param BOS: if not None, start every sentence with a single BOS token\n",
    "    :param EOS: if not None, end every sentence with a single EOS token\n",
    "    :return: training, dev, test\n",
    "        each an np.array such that\n",
    "        * array[:, 0] are the inputs (documents, each a string)\n",
    "        * array[:, 1] are the outputs (labels)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    prefix = [BOS] if BOS else []\n",
    "    suffix = [EOS] if EOS else []\n",
    "    for label in categories:  # here we pair doc (as a single string) and label (string)\n",
    "        if max_length is None:\n",
    "            # this time we will concatenate the EOS symbol to the string\n",
    "            pairs.extend((' '.join(prefix + s + suffix), label) for s in nltk_corpus.sents(categories=[label]))\n",
    "        else:\n",
    "            for s in nltk_corpus.sents(categories=[label]):\n",
    "                if len(s) <= max_length:\n",
    "                    pairs.append((' '.join(prefix + s + suffix), label))\n",
    "\n",
    "    # we turn the pairs into a numpy array\n",
    "    # np arrays are very convenient for the indexing tools np provides, as we will see\n",
    "    pairs = np.array(pairs)\n",
    "    # it's good to shuffle the pairs\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(pairs)\n",
    "    # let's split the np array into training (80%), dev (10%), and test (10%)\n",
    "    num_pairs = pairs.shape[0]\n",
    "    # we can use slices to select the first 80% of the rows\n",
    "    training = pairs[0:int(num_pairs * 0.8 * portion),:]\n",
    "    # and similarly for the next 10%\n",
    "    dev = pairs[int(num_pairs * 0.8 * portion):int(num_pairs * 0.9 * portion),:]\n",
    "    # and for the last 10%\n",
    "    test = pairs[int(num_pairs * 0.9 * portion):int(num_pairs * portion),:]\n",
    "    return training, dev, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch9edS3dnOQp"
   },
   "source": [
    "Let's use it on our choice of corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AC2058FsnOQp"
   },
   "outputs": [],
   "source": [
    "training_pairs, dev_pairs, test_pairs = prepare_nltk_corpus(corpus, labels, max_length=50, portion=0.6)\n",
    "training_pairs.shape, dev_pairs.shape, test_pairs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OMFLRZinOQp"
   },
   "source": [
    "Familiriase yourself with how we stored the data points. For example, this is the first document in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "StJzuxTUnOQq",
    "outputId": "c28b1500-1370-4e92-e641-b11cf7917e03"
   },
   "outputs": [],
   "source": [
    "training_pairs[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GE6LasunOQq"
   },
   "source": [
    "and its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "2k_U0Y2PnOQq",
    "outputId": "c4c6b6d8-ec2e-47b5-fd3f-bf7ef0f6e32a"
   },
   "outputs": [],
   "source": [
    "training_pairs[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQTQ4dImnOQq"
   },
   "source": [
    "Some basic data inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JDhNJ4pfnOQq",
    "outputId": "c621cfb8-cce2-4915-81fd-22d6c374929e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "_ = plt.hist([len(x.split()) for x, y in training_pairs], bins=30, label='training')\n",
    "_ = plt.xlabel(\"Length in tokens\")\n",
    "_ = plt.show()\n",
    "\n",
    "_ = plt.hist([y for x, y in training_pairs], bins=30, orientation='horizontal', label='training')\n",
    "_ = plt.xlabel(\"Frequency\")\n",
    "_ = plt.ylabel(\"Class\")\n",
    "_ = plt.show()\n",
    "\n",
    "for x, y in training_pairs[0:6]:\n",
    "    print(f\"label={y}\\tdoc={x}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qmFucRGr1Lh",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Vocabulary'></a>\n",
    "## Vocabulary\n",
    "\n",
    "As always when dealing with NLP models, we need an object to maintain our vocabulary of known tokens.\n",
    "\n",
    "Our vocabulary class will maintain the set of known tokens, and a dictionary to convert tokens to codes and codes back to tokens. The class will also take care of some special symbols (e.g., BOS, EOS, UNK, PAD).\n",
    "\n",
    "Finally, if later on you test your model on sentences that are not word tokenized, you can use `nlt.tokenize.word_tokenize` or any other tokenizer you like (as long as the level of tokenization is similar to the one you used for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcuhA0GEr8W4"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUUct5byIkKu"
   },
   "source": [
    "This is how you can tokenize English sentences (but remember that we don't need to redo this for the training/dev/test data from NLKT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kT2gOTZtr_9w",
    "outputId": "f68e7ad4-a69d-4fef-adc4-8ad354fa9d1e"
   },
   "outputs": [],
   "source": [
    "word_tokenize(\"This is a sentence, and this is another.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt5gPbIDIrx8"
   },
   "source": [
    "We will adapt one of the classes we developed in previous tutorials, and this class will be used for maintaining both the vocabulary of known tokens and the set of known tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMrE8lvlr2aa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, pad_token=\"-PAD-\", bos_token=\"-BOS-\", eos_token=\"-EOS-\", unk_token=\"-UNK-\"):\n",
    "        \"\"\"\n",
    "        Construct a vocabulary with a few reserved tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        # Special tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "        self.pad_id = 0\n",
    "        self.bos_id = 1\n",
    "        self.eos_id = 2\n",
    "        self.unk_id = 3\n",
    "\n",
    "        self.known_words = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n",
    "\n",
    "        # Vocabulary\n",
    "        self.word2id = dict()\n",
    "        self.word2id[self.pad_token] = self.pad_id\n",
    "        self.word2id[self.bos_token] = self.bos_id\n",
    "        self.word2id[self.eos_token] = self.eos_id\n",
    "        self.word2id[self.unk_token] = self.unk_id\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of known words (including reserved tokens)\"\"\"\n",
    "        return len(self.known_words)\n",
    "\n",
    "    def add(self, word: str):\n",
    "        \"\"\"Add a word (if it is unique) and return its index\"\"\"\n",
    "        idx = self.word2id.get(word, None)\n",
    "        if idx is None:\n",
    "            idx = len(self.known_words)\n",
    "            self.known_words.append(word)\n",
    "            self.word2id[word] = idx\n",
    "        return idx\n",
    "\n",
    "    def items(self):\n",
    "        \"\"\"Enumerate word ids and words in order\"\"\"\n",
    "        return enumerate(self.known_words)\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        \"\"\"Return the id (int) of a word (str)\"\"\"\n",
    "        return self.word2id.get(sym, self.unk_id)\n",
    "\n",
    "    def word(self, idx):\n",
    "        \"\"\"Return the word (string) associated with an index\"\"\"\n",
    "        return self.known_words[idx]\n",
    "\n",
    "    def encode(self, doc: list, add_bos=False, add_eos=False, pad_right=0):\n",
    "        \"\"\"\n",
    "        Transform a document into a sequence of integer token identifiers.\n",
    "        doc: list of tokens, each token is a string\n",
    "        add_bos: whether to add the BOS token\n",
    "        add_eos: whether to add the EOS token\n",
    "        pad_right: number of suffix padding tokens\n",
    "\n",
    "        Return: a list of codes (possibly with BOS and EOS added as well as padding)\n",
    "        \"\"\"\n",
    "        return [self.word2id.get(w, self.unk_id) for w in chain([self.bos_token] * int(add_bos), doc, [self.eos_token] * int(add_eos), [self.pad_token] * pad_right)]\n",
    "\n",
    "    def batch_encode(self, docs: list, add_bos=False, add_eos=False):\n",
    "        \"\"\"\n",
    "        Transform a batch of documents into a numpy array of integer token identifiers.\n",
    "        This will pad the shorter documents to the length of the longest document.\n",
    "        docs: a list of documents\n",
    "        add_bos: whether to add the BOS token\n",
    "        add_eos: whether to add the EOS token\n",
    "        pad_right: number of suffix padding tokens\n",
    "\n",
    "        Return: numpy array with shape [len(docs), longest_doc + add_bos + add_eos]\n",
    "        \"\"\"\n",
    "        max_len = max(len(doc) for doc in docs)\n",
    "        return np.array([self.encode(doc, add_bos=add_bos, add_eos=add_eos, pad_right=max_len-len(doc)) for doc in docs])\n",
    "\n",
    "    def decode(self, ids, strip_pad=False):\n",
    "        \"\"\"\n",
    "        Transform a np.array document into a list of tokens.\n",
    "        ids: np.array with shape [num_tokens]\n",
    "        strip_pad: whether PAD tokens should be deleted from the output\n",
    "\n",
    "        Return: list of strings with size [num_tokens - num_padding]\n",
    "        \"\"\"\n",
    "        if strip_pad:\n",
    "            return [self.word(id) for id in ids if id != self.pad_id]\n",
    "        else:\n",
    "            return [self.word(id) for id in ids]\n",
    "\n",
    "    def batch_decode(self, docs, strip_pad=False):\n",
    "        \"\"\"\n",
    "        Transform a np.array collection of documents into a collection of lists of tokens.\n",
    "        ids: np.array with shape [num_docs, max_length]\n",
    "        strip_pad: whether PAD tokens should be deleted from the output\n",
    "\n",
    "        Return: list of documents, each a list of tokens, each token a string\n",
    "        \"\"\"\n",
    "        return [self.decode(doc, strip_pad=strip_pad) for doc in docs]\n",
    "\n",
    "\n",
    "def update_vocabulary_from_corpus(vocab: Vocabulary, corpus: list, min_freq=1):\n",
    "    \"\"\"\n",
    "    Update an existing vocabulary with words observed in a corpus.\n",
    "    \n",
    "    corpus: list of sentences, each sentence a list of tokens, each token a string\n",
    "    min_freq: words less frequent than this will not be added to the vocabulary.\n",
    "    \"\"\"\n",
    "    # Count word occurrences\n",
    "    counter = Counter(chain(*corpus))\n",
    "    # sort them by frequency\n",
    "    sorted_by_freq_tuples = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "    for w, n in sorted_by_freq_tuples:\n",
    "        if n >= min_freq: # discard infrequent words\n",
    "            vocab.add(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxTwRreJI7DC"
   },
   "source": [
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVR6Nfh5skV-",
    "outputId": "8a407338-2df9-4090-8eeb-8995c9f9510b"
   },
   "outputs": [],
   "source": [
    "# we get a vocabulary for words\n",
    "vocab = Vocabulary()\n",
    "list(vocab.items())  # see that it is initialised with some reserved symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_vocabulary_from_corpus(vocab, (xy[0].split() for xy in training_pairs), min_freq=1)\n",
    "# you can see its size V\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM64EcW2JEoo"
   },
   "source": [
    "The `encode` method turns a sequence of (str) symbols into a sequence of (int) codes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5PacbfMnOQs"
   },
   "source": [
    "* document as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gjgxphKinOQs",
    "outputId": "0cc6b154-3a1a-44be-a038-1c35f9499e5b"
   },
   "outputs": [],
   "source": [
    "training_pairs[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_PE3EobnOQs"
   },
   "source": [
    "* document as a token sequence (recall that NLTK corpora are already tokenized, hence string.split() is enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IPiG6_uWnOQs",
    "outputId": "569f8d35-fa6b-46f4-eade-60ed26e6d915"
   },
   "outputs": [],
   "source": [
    "training_pairs[0,0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5jAIpoRnOQt"
   },
   "source": [
    "* document as a sequence of token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNAM6VWCnOQt",
    "outputId": "9a893041-ebaa-4030-b9a7-cc619011f256"
   },
   "outputs": [],
   "source": [
    "vocab.encode(training_pairs[0,0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IOyp9DhnOQt"
   },
   "source": [
    "The _decode_ method of the Vocabulary reverses this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-pokXvpnOQt",
    "outputId": "bd44b2ce-9ced-4ee7-f005-ccec5f83f3bc"
   },
   "outputs": [],
   "source": [
    "vocab.decode(vocab.encode(training_pairs[0,0].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnrRiRkJnOQt"
   },
   "source": [
    "* and string _join_ reverses _split_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "fEeh7E0ynOQt",
    "outputId": "c3e38c69-1191-4d44-87fd-7e543a92ac28"
   },
   "outputs": [],
   "source": [
    "' '.join(vocab.decode(vocab.encode(training_pairs[0,0].split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcGLLsUsKTrM"
   },
   "source": [
    "We can also encode and decode entire batches of sequences. This will use pad symbols/codes to make the sequences in the same batch have the same length.\n",
    "\n",
    "For example, see the first two documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0WrecDnnOQu",
    "outputId": "5e942906-71c1-4ec3-8511-214bbb29beb8"
   },
   "outputs": [],
   "source": [
    "training_pairs[0:2,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBQNvJe0nOQu"
   },
   "source": [
    "note that they have different length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJl3wj6FnOQu",
    "outputId": "d452fe0e-4cd7-41c7-ea12-699d81c844c4"
   },
   "outputs": [],
   "source": [
    "for x in training_pairs[0:2,0]:\n",
    "    print(len(x.split()), \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejd0cgAPnOQu"
   },
   "source": [
    "The `batch_encode` method returns an array where the shortest sequence is padded to the length of the longest one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0tlz8JlnOQu",
    "outputId": "a531562f-faac-489b-df8e-d5e54eb6c902"
   },
   "outputs": [],
   "source": [
    "vocab.batch_encode([x.split() for x in training_pairs[0:2,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QS7F7F9enOQv"
   },
   "source": [
    "The `0`s at the end of the shorter sequence indicate the PAD tokens.\n",
    "\n",
    "We can use `batch_decode` to convert token ids back into string tokens, and, on the way, we can remove the PAD symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mV5DJAJAnOQv",
    "outputId": "23fba1ee-42d6-4dce-fe4f-1a61fd004a46"
   },
   "outputs": [],
   "source": [
    "vocab.batch_decode(vocab.batch_encode([x.split() for x in training_pairs[0:2,0]]), strip_pad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using an enriched vocabulary based on an external resource.\n",
    "This resource, called [GloVe](https://nlp.stanford.edu/projects/glove/), is not only a vocabulary, but also a collection of word-level features, which we will be using to initialise our text encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfALT7ofrNc3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"glove.6B.50d.txt.gz\"):\n",
    "    print(\"Downloading 50-dimensional GloVe embeddings for English\")\n",
    "    # this will download 50-dimensional GloVe embeddings\n",
    "    !wget -q --show-progress https://raw.githubusercontent.com/probabll/ntmi-tutorials/master/datasets/glove.6B.50d.txt.gz\n",
    "else:    \n",
    "    print(\"GloVe embeddings already in disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper code below loads the embeddings from the compressed file into a dictionary mapping words (as python strings) to vectors (as numpy arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNniLbWprTUY"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    \"\"\"\n",
    "    Load word embeddings\n",
    "    :param path: path to word embedding file\n",
    "    :return: a dict mapping words to embedding vectors (np.array)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise RuntimeError(\"You need to download the word embeddings\")\n",
    "\n",
    "    w2v = dict()    \n",
    "\n",
    "    with gzip.open(path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(u' ', 1)            \n",
    "            w2v[word] = np.array(vec.split(), dtype=np.float32)\n",
    "\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the GloVe vectors into a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgdBwirIsCau"
   },
   "outputs": [],
   "source": [
    "glove = load_embeddings('glove.6B.50d.txt.gz')\n",
    "print(f\"We loaded {len(glove)} GloVe vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we extend the vocabulary class using words in GloVe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEUaRb1wxeK2"
   },
   "outputs": [],
   "source": [
    "print(f\"Vocab size before GloVe: {len(vocab)}\")\n",
    "for w in glove.keys():\n",
    "    vocab.add(w)\n",
    "print(f\"Vocab size after GloVe: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "06BMGjPT7BFp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Corpus_and_Data_Loader'></a>\n",
    "## Corpus and Data Loader\n",
    "\n",
    "We will be developing our models in torch, thus we need to wrap our corpus into a `Dataset` and a `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ptZKyBw7FN3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class LabelledCorpus(Dataset):\n",
    "    \"\"\"\n",
    "    Use this to give torch access to a corpus of labelled documents.\n",
    "    This class will also know the vocab object and the set of classes,\n",
    "    and it will take care of coding strings into integers consistently.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_x, corpus_y, vocab: Vocabulary, labels: list):\n",
    "        \"\"\"\n",
    "        In PyTorch we better always manipulate numerical codes, rather than text.\n",
    "        So, our Corpus object will contain a vocab that converts words to codes.\n",
    "\n",
    "        :param corpus_x: docs (token sequences -- you should take care of tokenization outside this class)\n",
    "        :param corpus_y: classes\n",
    "        :param vocab: vocabulary for token sequences\n",
    "        :param labels: possible classes\n",
    "        \"\"\"\n",
    "        self.corpus_x = list(corpus_x)\n",
    "        self.corpus_y = list(corpus_y)\n",
    "        assert len(self.corpus_x) == len(self.corpus_y), \"I need doc-label pairs\"\n",
    "        self.vocab = vocab\n",
    "        self.labels = tuple(labels)\n",
    "        self.label2int = {y: k for k, y in enumerate(labels)}\n",
    "\n",
    "    def labelstr(self, k: int):\n",
    "        \"\"\"Convert from integer to (str) label\"\"\"\n",
    "        return self.labels[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Size of the corpus in number of sequence pairs\"\"\"\n",
    "        return len(self.corpus_x)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Return corpus_x[idx] and corpus_y[idx] converted to codes\"\"\"\n",
    "        x = self.vocab.encode(self.corpus_x[idx], add_bos=False, add_eos=False)\n",
    "        y = self.label2int[self.corpus_y[idx]]\n",
    "        return x, y\n",
    "\n",
    "    @classmethod\n",
    "    def pad_to_longest(cls, pairs, pad_id=0):\n",
    "        \"\"\"\n",
    "        Take a list of coded sequences and returns a torch tensor where\n",
    "        every sentence has the same length (by means of using PAD tokens)\n",
    "        \"\"\"\n",
    "        longest = max(len(x) for x, y in pairs)\n",
    "        batch_x = torch.tensor([x + [pad_id] * (longest - len(x)) for x, y in pairs])\n",
    "        batch_y = torch.tensor([y for x, y in pairs])\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jqOgaYwLPhE"
   },
   "source": [
    "Let's play a bit with our `LabelledCorpus` class and make sure we understand what it does for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVrZjx0R8gYU"
   },
   "outputs": [],
   "source": [
    "training = LabelledCorpus(\n",
    "    # these are the input documents (we use split so they are sequences of tokens)\n",
    "    (x.split() for x in training_pairs[:, 0]),  \n",
    "    training_pairs[:, 1], # these are the documents' labels\n",
    "    vocab,\n",
    "    labels\n",
    ")\n",
    "dev = LabelledCorpus(\n",
    "    # these are the input documents (we use split so they are sequences of tokens)\n",
    "    (x.split() for x in dev_pairs[:, 0]),\n",
    "    dev_pairs[:, 1],\n",
    "    vocab,\n",
    "    labels\n",
    ")\n",
    "test = LabelledCorpus(\n",
    "    # these are the input documents (we use split so they are sequences of tokens)\n",
    "    (x.split() for x in test_pairs[:, 0]),\n",
    "    test_pairs[:, 1],\n",
    "    vocab,\n",
    "    labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-ebPbVxLha8"
   },
   "source": [
    "Here's an example of how we get a `DataLoader` for a corpus, we simply choose the `Dataset` object we want (training/dev/test), the batch size we want, whether we need shuffling (e.g., for training batches in SGD), and how we \"glue\" data points of different length together (i.e., a function such as `pad_to_longest` which `LabelledCorpus` provides for us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhVrqdcI827_",
    "outputId": "33efd97a-2732-499c-b4dd-0c07fc97183a"
   },
   "outputs": [],
   "source": [
    "batcher = DataLoader(training, batch_size=3, shuffle=True, collate_fn=LabelledCorpus.pad_to_longest)\n",
    "for batch_x, batch_y in batcher:\n",
    "    print(\"# This is how the labelled documents in a batch come out of the data loader\\n\")\n",
    "\n",
    "    print(batch_x, batch_y)\n",
    "\n",
    "    print(\"\\nYou can visualise the data using batch_decode:\\n\")\n",
    "\n",
    "    for x, y in zip(training.vocab.batch_decode(batch_x, strip_pad=True), batch_y):\n",
    "        print(f\"{training.labelstr(y)}\\t{' '.join(x)}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "kjldtg5dJW7a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Text_Encoders'></a>\n",
    "# Text Encoders\n",
    "\n",
    "In NLP applications, we often have to *encode* a piece of text, that is, map it to one (or more) vector(s) in some real coordinate space. For example, that is the case in text classification.\n",
    "\n",
    "Check the resource we prepared with the [Encoders](https://github.com/probabll/ntmi-tutorials/blob/main/Encoders.ipynb) relevant for this module. They are:\n",
    "* From Tokens to Vectors\n",
    "\t* One-Hot Encoding\n",
    "\t* Word embeddings\n",
    "* Pooling from multiple vectors\n",
    "\t* Sum pooling\n",
    "\t* Average pooling\n",
    "* Mapping from one real coordinate space to another\n",
    "\t* Linear transformation\n",
    "\t* Nonlinear activation functions\n",
    "* Composing multiple vectors\n",
    "\t* Concatenation\n",
    "\t* Feed forward network\n",
    "\t* Recurrent neural network encoder\n",
    "\n",
    "We will use those NN blocks to i) encode a document (i.e., turn a document into features) and then ii) map that encoding to the parameters of our choice of probability mass function (pmf). In this class, we only work with the Categorical pmf.\n",
    "\n",
    "Whenever a neural network has parameters of its own, these are initialised in some standard way (typically at random). At initialisation, these parameters are uniformative. That is, we can use the NN, but the outputs are not optimised for any specific purpose. We will implement a training procedure later in this notebook.\n",
    "\n",
    "Throughout, we assume a *document* is a sequence $x=\\langle w_1, \\ldots, w_l \\rangle$ of $l$ tokens, each token comes from a vocabulary $\\mathcal V$ of $V$ tokens. The label space $\\mathcal C$ of our text classifier is made of $C$ classes. Hence, our goal is to map from any given $x$ to a $C$-dimensional probability vector $\\boldsymbol \\pi^{(x)} \\in \\Delta_{C-1}$.\n",
    "\n",
    "\n",
    "The rough idea is as follows:\n",
    "* we convert the tokens in a document to fixed-dimensional vectors ;\n",
    "* then, we map these vectors to a single vector representing the entire document (depending on how we design this operation, it may or may not discard information such as the order in which the tokens ocurred);\n",
    "* finally, we map this document encoding to a vector of $C$ logits (and softmax gives us $C$ probabilities), which then is used to parameterise the Categorical pmf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmQMQlr5nOQx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy 2 auxiliary functions from the [Encoders](https://github.com/probabll/ntmi-tutorials/blob/main/Encoders.ipynb) notebook, they implement sum and average pooling, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "si80ThEHnOQx"
   },
   "outputs": [],
   "source": [
    "def sum_pooling(input_sequences, sequence_mask):\n",
    "    \"\"\"\n",
    "    Returns the sum of the vectors along the sequence dimension.\n",
    "\n",
    "    :param input_sequences: [batch_size, max_length, D] a batch of sequences of D-dimensional vectors\n",
    "    :param sequence_mask: [batch_size, max_length] indicates which positions are valid (i.e., not PAD)\n",
    "        we use 1 for valid (not PAD) and 0 for PAD\n",
    "\n",
    "    :return: a tensor with output shape [batch_size, D]\n",
    "    \"\"\"\n",
    "\n",
    "    # here we replace padding positions by D-dimensional vectors of 0s,\n",
    "    #  this way those options won't contribute to the sum\n",
    "    # [batch_size, max_length, D]\n",
    "    masked = torch.where(\n",
    "        # we create an extra axis at the end of the tensor\n",
    "        sequence_mask.unsqueeze(-1),  # this has shape [batch_size, max_length, 1]\n",
    "        input_sequences,  # this has shape [batch_size, max_length, D]\n",
    "        torch.zeros_like(input_sequences)  # this has shape [batch_size, max_length, D]\n",
    "    )\n",
    "\n",
    "    # we sum, along the sequence dimension (second last),\n",
    "    #  the valid vectors (those that are not PAD)\n",
    "    # [batch_size, D]\n",
    "    return torch.sum(masked, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pT54TClInOQy"
   },
   "outputs": [],
   "source": [
    "def average_pooling(input_sequences, sequence_mask):\n",
    "    \"\"\"\n",
    "    Returns the average encoding of each sequence.\n",
    "\n",
    "    :param input_sequences: [batch_size, max_length, D] a batch of sequences of D-dimensional vectors\n",
    "    :param sequence_mask: [batch_size, max_length] indicates which positions are valid (i.e., not PAD)\n",
    "        we use 1 for valid (not PAD) and 0 for PAD\n",
    "\n",
    "    :return: a tensor with output shape [batch_size, D]\n",
    "    \"\"\"\n",
    "\n",
    "    # here we replace padding positions by D-dimensional vectors of 0s,\n",
    "    #  this way those options won't contribute to the sum\n",
    "    # [batch_size, max_length, D]\n",
    "    masked = torch.where(\n",
    "        # we create an extra axis at the end of the tensor\n",
    "        sequence_mask.unsqueeze(-1),  # this has shape [batch_size, max_length, 1]\n",
    "        input_sequences,  # this has shape [batch_size, max_length, D]\n",
    "        torch.zeros_like(input_sequences)  # this has shape [batch_size, max_length, D]\n",
    "    )\n",
    "\n",
    "    # we sum, along the sequence dimension (second last),\n",
    "    #  the valid vectors (those that are not PAD)\n",
    "    # we also divide by sequence length\n",
    "    # [batch_size, D]\n",
    "    avg = torch.sum(masked, dim=-2) / torch.sum(sequence_mask.float(), dim=-1, keepdims=True)\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will be using a pretrained resource, the GloVe embeddings. They will serve as the parameters of our Embedding layer, but before we can use them as such, we need to convert the GloVe dict to an np.array, we do that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYlUtIxLsGQV"
   },
   "outputs": [],
   "source": [
    "def make_embedding_matrix(vocab: Vocabulary, w2v: dict, emb_dim: int, rng=np.random.RandomState(42)):\n",
    "    \"\"\"\n",
    "    Construct an Embedding matrix with shape (vocab_size, emb_dim) from a vocabulary\n",
    "    and a dict of GloVe embeddings.\n",
    "    \n",
    "    :param vocab: a Vocabulary object, our embedding matrix will have a vector for each word \n",
    "        in this object.\n",
    "    :param w2v: a dictionary of word vectors (from GloVe)\n",
    "    :param emb_dim: dimensionality of embedding vectors\n",
    "    :param rng: a random number generator\n",
    "    \n",
    "    :return: an np.array with shape (vocab_size, emb_dim) that can be used to initialise\n",
    "        a torch.nn.Embedding object.\n",
    "    \"\"\"\n",
    "    # We initialise a matrix full of zeros\n",
    "    E = np.zeros((len(vocab), emb_dim))\n",
    "    # Let's keep track of the sum of embeddings (this will be used later to give the -UNK- token its own embedding)\n",
    "    sum_emb = np.zeros(emb_dim)\n",
    "    # and the total number of words for which we found embeddings    \n",
    "    N = 0\n",
    "    # as well as a list of words for which we did not find a GloVe embedding (it can happen)\n",
    "    not_found = []\n",
    "    # for all known symbols\n",
    "    for idx, sym in vocab.items():\n",
    "        v = w2v.get(sym, None)  # try to find a GloVe embedding\n",
    "        if v is None:  # sometimes we cannot find one\n",
    "            v = w2v.get(sym.lower(), None)  # then we try to find the embedding of a lowercase version of the token\n",
    "        if v is None:  # sometimes that's not enough\n",
    "            not_found.append(idx)  # so this word won't have a pretrained embedding\n",
    "        else:  # but, when we do find\n",
    "            if len(v) != emb_dim:  \n",
    "                raise ValueError(f\"I expected {emb_dim}-dimensional vectors, got {len(v)}\")\n",
    "            # we store this embedding in the corresponding row of the table\n",
    "            E[idx] = v\n",
    "            # and update the sum of embeddings and the total count\n",
    "            sum_emb += v\n",
    "            N += 1\n",
    "    # we can use the average of all embeddings that made it to the table\n",
    "    # as a representation of the -UNK- symbol (this is a common heuristic)\n",
    "    avg_emb = sum_emb / N\n",
    "    E[vocab.unk_id] = avg_emb\n",
    "    # and, similarly, as a representation of the symbols that did not get an embedding    \n",
    "    for idx in not_found:\n",
    "        E[idx] = avg_emb\n",
    "        # An alternative to this would be to give these words a _random_ embedding:\n",
    "        ## E[idx] = rng.normal(0., 1., size=emb_dim)\n",
    "        # then we would let these random embeddings be updated by SGD\n",
    "        # For this notebook, we use the simpler average embedding heuristic\n",
    "        # and keep the embedding frozen during training of the classifier.\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYc9X-l5teFV",
    "outputId": "f52890fd-c423-403b-fcae-c740b5920d9c"
   },
   "outputs": [],
   "source": [
    "E = make_embedding_matrix(vocab, glove, 50)\n",
    "E.shape, len(vocab), len(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "JlqKGKLn1FSH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Neural_Text_Classifier'></a>\n",
    "# Neural Text Classifier\n",
    "\n",
    "In this section we will design a complete architecture for our text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_z8N_ZpTNMy"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_all(seed=42):  # this is needed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "seed_all()  # whenever we want to reset random seeds, we run `seed_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pf3iDZ-nhL9E"
   },
   "outputs": [],
   "source": [
    "# various useful packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "LrU5_s6knOQy",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Torch_Categorical'></a>\n",
    "## Torch Categorical\n",
    "\n",
    "Before we continue, let's have a look at PyTorch's implementation of the Categorical distribution.\n",
    "\n",
    "Of course, for binary classification, we could use the Bernoulli distribution, but we will go on with the Categorical for two reasons: a) a Categorical over 2 outcomes is essentially equivalent to the Bernoulli, and b) this way you have a resource that also works with datasets that contain more than 2 categories.\n",
    "\n",
    "In PyTorch, the package `torch.distributions` ships _many_ standard distributions. The implementations there are efficient, tested, and offer an intuitive API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeOHyGqunOQy"
   },
   "outputs": [],
   "source": [
    "import torch.distributions as td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLZ34kXpnOQy"
   },
   "source": [
    "Here we show how you can use a `td.Categorical` object.\n",
    "\n",
    "As standard, we specify a Categorical distribution by specifying the probability masses of the classes in its support. PyTorch infers the support size from the length of the probability vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVUCzp2LnOQz",
    "outputId": "4e699294-97a6-42c5-cfcc-de39ee37692a"
   },
   "outputs": [],
   "source": [
    "# Construct a categorical object from 3 probabilities\n",
    "p = td.Categorical(probs=torch.tensor([0.1, 0.2, 0.7]))\n",
    "p.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61vTBgYAnOQz",
    "outputId": "75b4736c-d79b-40e3-f5fd-98cfb211e906"
   },
   "outputs": [],
   "source": [
    "# Construct a categorical object from 4 probabilities\n",
    "p = td.Categorical(probs=torch.tensor([0.4, 0.2, 0.3, 0.1]))\n",
    "p.probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSU_QpT_nOQ0"
   },
   "source": [
    "Of course, we can also construct a whole _batch_ of Categorical objects, in which case the last dimension indicates the size of the support.\n",
    "\n",
    "In the following example we construct a batch of 2 Categorical distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2a0LzlnnOQ1",
    "outputId": "902844e6-1052-4d8a-a942-f79d15975631"
   },
   "outputs": [],
   "source": [
    "p = td.Categorical(probs=torch.tensor([[0.1, 0.2, 0.7], [0.2, 0.2, 0.6]]))\n",
    "p.probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U8MgC3znOQ1"
   },
   "source": [
    "PyTorch also lets us specify the Categorical parameters in _logit_ space. This is convenient when predicting the logits with neural networks because in this case PyTorch can make numerically efficient tricks whenever probs or log probs are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJeVi2QPnOQ1",
    "outputId": "50314dca-4df6-4412-e481-0f76e274c6e5"
   },
   "outputs": [],
   "source": [
    "# with the `logits` argument we can construct Categorical objects without using softmax ourselves\n",
    "p = td.Categorical(logits=torch.tensor([[-1.1, 0, 2.1], [0.1, -0.2, 1.1]]))\n",
    "p.probs  # but see that internally, torch knows that softmax is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRXOpGYonOQ1"
   },
   "source": [
    "There are two main methods for any one distribution:\n",
    "* `log_prob` to compute the probability mass of an outcome\n",
    "* `sample` to draw outcomes from the distribution\n",
    "\n",
    "Let's test `log_prob` and see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sde1tcCynOQ1",
    "outputId": "c7eb2014-481e-46e2-c25f-96f2d1b65d38"
   },
   "outputs": [],
   "source": [
    "# this returns log prob of outcome 0 under the first Cateogrical distribution\n",
    "# and outcome 1 under the second Categorical distribution\n",
    "p.log_prob(torch.tensor([[0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NL6HCHqwnOQ2",
    "outputId": "e98a946b-4788-47ce-e891-f972bd11590c"
   },
   "outputs": [],
   "source": [
    "# this returns prob of outcome 0 under the first Cateogrical distribution\n",
    "# and outcome 1 under the second Categorical distribution\n",
    "torch.exp(p.log_prob(torch.tensor([[0, 1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_1L0HtxnOQ2"
   },
   "source": [
    "Now we sample from the two Categorical distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8ucH2HbnOQ2",
    "outputId": "719b7510-98ae-4acd-d788-7156b8546160"
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(p.sample())  # this returns one sample from each distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iqsv3S0tnOQ2"
   },
   "source": [
    "We can also obtain multiple samples at once. PyTorch will stack those samples in an object with shape `[sample_size, batch_shape]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-Mnr36GnOQ2",
    "outputId": "ad46ab91-c257-4587-8037-432917a210b1"
   },
   "outputs": [],
   "source": [
    "p.sample(sample_shape=(10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koFASvlwnOQ2"
   },
   "source": [
    "To demonstrate that the sampling algorithm is correct, let's sample many times and compute an MLE for the probability of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ga0em0_9nOQ2",
    "outputId": "a084d050-4e63-4f9f-d7ff-6270b79eca4a"
   },
   "outputs": [],
   "source": [
    "# here we draw 1000 samples from each Categorical, this returns an object of shape [1000, 2];\n",
    "# then we one-hot encode them into 3-dimensional vectors (since this is the number of classes in this example),\n",
    "#  this returns an object of shape [1000, 2, 3];\n",
    "# then we take the mean along the sample dimension, which gives us an object of shape [2, 3]\n",
    "torch.mean(F.one_hot(p.sample(sample_shape=(1000,)), 3).float(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YH7TbH-ZnOQ2",
    "outputId": "fff26e84-4c51-4b64-cc72-cf1756e73c5b"
   },
   "outputs": [],
   "source": [
    "p.probs  # look how similar they are, with more samples they would get ever more similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "vwu9uUjCnOQ2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is it. We will be using `td.Categorical` in our text classifier.\n",
    "\n",
    "And, to enjoy the improved numerical stability of torch code, we will parameterise our Categorical distributions through `logits` as opposed to `probs` (this means, we will let torch take care of the _softmax_ internally to the `td.Categorical` class). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "9GLMX3_Hvmel",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Base_class'></a>\n",
    "## Base class\n",
    "\n",
    "This is a general class, which we will specialise later. \n",
    "\n",
    "The only method that will need specialisation is the `_predict_logits` which will be responsible for the 3 main tasks of our NN:\n",
    "1. produce token encodings\n",
    "2. produce a document encoding\n",
    "3. produce logits (and parameterise a Categorical pmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-1'></a> **Ungraded Exercise 1 - Base class**\n",
    "\n",
    "Study the base Classifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "bZjt4gVDiIRz",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributions as td\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):  # every NN in torch is a subclass of nn.Module\n",
    "    \"\"\"\n",
    "    Statistically a text classifier is a model that maps from some input document x\n",
    "     to a conditional probability distribution over the space of classes.\n",
    "\n",
    "    We achieve this mapping by using a neural network architecture to map from x\n",
    "     to the parameters of a Categorical pmf over C labels.\n",
    "\n",
    "    The parameters of the neural network are initialised at random, hence they\n",
    "     are uninformative unless we train the model.\n",
    "\n",
    "    For training, we typically obtain some labelled data,\n",
    "     and then optimise the parameters to maximise the model's likelihood function.\n",
    "\n",
    "    This class takes care of specifying the model (with randomly initialised parameters).\n",
    "    Later, we will write some helper code for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, num_classes: int, pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "        \"\"\"\n",
    "        :param vocab_size: size of the known vocabulary\n",
    "        :param num_classes: number of classes in the text classification problem\n",
    "        :param pad_id: index of the PAD token in the vocabulary\n",
    "        :param bos_id: index of the BOS token in the vocabulary\n",
    "        :param eos_id: index of the EOS token in the vocabulary\n",
    "        :param unk_id: index of the UNK token in the vocabulary\n",
    "        \"\"\"\n",
    "        super().__init__()  # whenever we develop a torch Module, we need this, it calls the constructor of the class nn.Module\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_classes = num_classes\n",
    "        self._pad = pad_id\n",
    "        self._bos = bos_id\n",
    "        self._eos = eos_id\n",
    "        self._unk = unk_id\n",
    "\n",
    "    # Python properties allow client code to access the property\n",
    "    # without the risk of modifying it\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self._vocab_size\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return self._num_classes\n",
    "\n",
    "    @property\n",
    "    def pad(self):\n",
    "        return self._pad\n",
    "\n",
    "    @property\n",
    "    def bos(self):\n",
    "        return self._bos\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    def num_parameters(self, trainable_only=True):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the model\n",
    "        \n",
    "        :param trainable_only: change to False to count all parameters (even those in frozen layers)\n",
    "        \"\"\"\n",
    "        if trainable_only:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters() if theta.requires_grad)\n",
    "        else:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters())\n",
    "    \n",
    "    def _predict_logits(self, x):\n",
    "        \"\"\"\n",
    "        For each document in the batch of inputs, compute the C-dimensional vetor of probabilities\n",
    "         that should parameterise the conditional Categorical distribution.\n",
    "\n",
    "        This method will encode the tokens in x,\n",
    "         combine the token encodings,\n",
    "         then predict logits\n",
    "\n",
    "        It will do so in a batched way, so that we can use batches of documents.\n",
    "\n",
    "        :param x: [batch_size, max_length] a batch of documents, each document is a sequence of token ids\n",
    "\n",
    "        :return: a tensor with shape [batch_size, num_classes].\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each type of classifier will have a different implementation here\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns a Categorical cpd (see td.Categorical) for each input document.\n",
    "\n",
    "        This method uses _predict_logits to map each document to a vector of C logits,\n",
    "         then parameterises and returns a categorical pmf.\n",
    "\n",
    "        It will do so in a batched way, so that we can use batches of documents.\n",
    "\n",
    "        :param x: [batch_size, max_length] a batch of documents, each document is a sequence of token ids\n",
    "        :return: a td.Categorical object whose logits have shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # [batch_size, num_classes]\n",
    "        s = self._predict_logits(x)\n",
    "        return td.Categorical(logits=s)  # for numerical efficiency reasons,  the call to softmax(s) happens internally to the Categorical object (only when needed)\n",
    "\n",
    "    def log_prob(self, x, y):\n",
    "        \"\"\"\n",
    "        For each (x, y) in the batch, compute the log conditional probability mass\n",
    "         assigned to the observed label y given the observed document x.\n",
    "\n",
    "\n",
    "        :param x: [batch_size, max_length] a batch of documents, each document is a sequence of token ids\n",
    "        :param y: [batch_size] a batch of document labels\n",
    "        :return: a tensor with shape [batch_size]\n",
    "        \"\"\"\n",
    "        # one C-dimensional Categorical cpd for each document in the batch\n",
    "        cpds = self(x)  # in torch, one we call a module like this `module(...)`, torch calls the `module.forward` method\n",
    "        # [batch_size]\n",
    "        logp = cpds.log_prob(y)\n",
    "        return logp\n",
    "\n",
    "    def mode(self, x):\n",
    "        \"\"\"\n",
    "        For each cpd Y|X=x, predicts the mode of the cpd.\n",
    "        :param x: [batch_size, max_length] a batch of documents, each document is a sequence of token ids\n",
    "\n",
    "        :return: a batch of predicted document labels [batch_size]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        max_length = x.shape[1]\n",
    "        with torch.no_grad():\n",
    "            cpds = self(x)  # one Categorical cpd per doc in the batch\n",
    "            # [batch_size]\n",
    "            y_pred = torch.argmax(cpds.probs, -1)  # argmax of each cpd (option -1 means argmax along the last axis of the tensor)\n",
    "            return y_pred\n",
    "\n",
    "    def sample(self, x, sample_size=None):\n",
    "        \"\"\"\n",
    "        Per document in the batch, draws a number of samples from the model,\n",
    "         each sample is a class for the document.\n",
    "\n",
    "        :param x: [batch_size, max_len] a batch of documents, each document is a sequence of token ids\n",
    "        :param sample_size: number of samples\n",
    "            - use None to obtain one sample per element in the batch\n",
    "            - use a tuple to obtain a batch of samples for each and every element in the batch\n",
    "                for example sample_size=(10,) will return 10 samples per element in the batch\n",
    "        :return: a batch of sampled labels with shape [batch_size] if sample_size is None\n",
    "            else with shape [sample_size, batch_size]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        max_length = x.shape[1]\n",
    "        with torch.no_grad():\n",
    "            cpds = self(x) # one Categorical cpd per doc in the batch\n",
    "            # [sample_size, batch_size]\n",
    "            y_pred = cpds.sample(sample_size) \n",
    "            return y_pred\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute a scalar loss from a batch of labelled documents.\n",
    "\n",
    "        The loss is the negative log likelihood of the model estimated on a single batch:\n",
    "            - 1/batch_size * \\sum_{s} log P(y[s]|x[s], theta)\n",
    "\n",
    "        :param x: [batch_size, max_length] a batch of documents, each document is a sequence of token ids\n",
    "        :param y: [batch_size] a batch of document labels\n",
    "        :return: a scalar tensor (i.e., a tensor with shape [])\n",
    "        \"\"\"\n",
    "        return -self.log_prob(x=x, y=y).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Average_Embedding_Classifier'></a>\n",
    "\n",
    "## Average Embedding Classifier\n",
    "\n",
    "This first model encodes a document as the average embedding of the tokens in it, after that, it uses a feed-forward net to map the document encoding to $C$ logits for the Categorical distribution.\n",
    "\n",
    "Here is the model\n",
    "\\begin{align}\n",
    "Y | X=w_{1:l} &\\sim \\mathrm{Categorical}(\\mathbf g(w_{1:l}; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf g$ is the following neural network:\n",
    "\\begin{align}\n",
    "\\mathbf e_i &= \\mathrm{embed}_D(w_i; \\theta_{\\text{glove}})  & i \\in [l]\\\\\n",
    "\\mathbf h &= \\frac{1}{l} \\mathbf e_i \\\\\n",
    "\\mathbf s &= \\mathrm{ffnn}_C(\\mathbf h; \\theta_{\\text{out}})\\\\\n",
    "\\mathbf g(w_{1:l}; \\theta) &= \\mathrm{softmax}(\\mathbf s)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-2'></a> **Ungraded Exercise 2 - AvgEmbClassifier**\n",
    "\n",
    "Study the `AvgEmbClassifier` class below and complete its `_predict_logits` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class AvgEmbClassifier(Classifier):  # this module is a specialisation of Classifier\n",
    "\n",
    "    def __init__(self, vocab_size, num_classes, word_embed_dim: int, hidden_size: int, p_drop=0.2, E=None, pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "        \"\"\"\n",
    "        :param vocab_size: number of known words\n",
    "        :param num_classes: number of classes in the text classification problem\n",
    "        :param word_embed_dim: dimensionality of word embeddings (50 for GloVe embeddings)\n",
    "        :param hidden_size: dimensionality of hidden layer in the FFNN\n",
    "        :param p_drop: dropout rate (for units)\n",
    "            To fight overfitting to small datasets, it is common to use a training trick called Dropout, \n",
    "            whereby we drop some units (i.e., map them to 0) at random.\n",
    "            We typically do that right before linear layers.\n",
    "            This parameter controls the proportion of random units we drop.\n",
    "        :param E: pretrained embeddings (optional), this object should be a torch.tensor of type torch.float32\n",
    "            Using pretrained embeddings helps in 2 ways: \n",
    "            * we can train those embeddings using very large unlabelled datasets, \n",
    "              so they contain features that are more predictive of syntactic and semantic properties of words\n",
    "            * we can keep these parameters frozen (that is, we do not further train them)\n",
    "              which helps against overfitting to the small text classification dataset we have            \n",
    "        \"\"\"\n",
    "        # we always begin by calling the constructor of the parent class\n",
    "        super().__init__(vocab_size=vocab_size, num_classes=num_classes, pad_id=pad_id, bos_id=bos_id, eos_id=eos_id, unk_id=unk_id)\n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Now we construct the necessary NN blocks\n",
    "        \n",
    "        # First, we construct an embedding layer\n",
    "        if E is None:  # here we use a randomly initialised embedding matrix\n",
    "            self.word_embed = nn.Embedding(self.vocab_size, embedding_dim=word_embed_dim)\n",
    "        else:  # here we use the pretrained embeddings, and keep them frozen\n",
    "            if E.shape != (vocab_size, word_embed_dim):\n",
    "                raise ValueError(f\"E should have shape {(vocab_size, word_embed_dim)}, got {E.shape}\")\n",
    "            self.word_embed = nn.Embedding.from_pretrained(E, freeze=True)\n",
    "        \n",
    "        # Next, we construct an FFNN\n",
    "        self.logits_predictor = nn.Sequential(\n",
    "            nn.Dropout(p_drop),  # this is a special operation meant to avoid overfitting\n",
    "            nn.Linear(word_embed_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # You will be using self.word_embed and self.logits_predictor in `_predict_logits` below\n",
    "\n",
    "\n",
    "    def _predict_logits(self, x):\n",
    "        \"\"\"\n",
    "        For each document in the batch of inputs, compute the C-dimensional vetor of probabilities\n",
    "         that should parameterise the conditional Categorical distribution.\n",
    "\n",
    "        This method will encode the tokens in x (using D-dimensinoal embedding vectors),\n",
    "         average the token embeddings         \n",
    "         then predict logits (using an FFNN).\n",
    "\n",
    "        It will do so in a batched way, so that we can use batches of documents.\n",
    "\n",
    "        :param x: [batch_size, max_length] a batch of documents, each document is a sequence of token ids\n",
    "        :return: output has shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # Here you should \n",
    "        # 1. embed the tokens in x\n",
    "        # 2. obtain document encodings by average of word embeddings\n",
    "        # 3. obtain and return logits with shape [batch_size, num_classes]\n",
    "        raise NotImplementedError(\"Implement me!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "class AvgEmbClassifier(Classifier):  # this module is a specialisation of Classifier\n",
    "\n",
    "    def __init__(self, vocab_size, num_classes, word_embed_dim: int, hidden_size: int, p_drop=0.5, E=None, pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "        \"\"\"\n",
    "        :param vocab_size: number of known words\n",
    "        :param num_classes: number of classes in the text classification problem\n",
    "        :param word_embed_dim: dimensionality of word embeddings (50 for GloVe embeddings)\n",
    "        :param hidden_size: dimensionality of hidden layer in the FFNN\n",
    "        :param p_drop: dropout rate (for units)\n",
    "            To fight overfitting to small datasets, it is common to use a training trick called Dropout, \n",
    "            whereby we drop some units (i.e., map them to 0) at random.\n",
    "            We typically do that right before linear layers.\n",
    "            This parameter controls the proportion of random units we drop.\n",
    "        :param E: pretrained embeddings (optional), this object should be a torch.tensor of type torch.float32\n",
    "            Using pretrained embeddings helps in 2 ways: \n",
    "            * we can train those embeddings using very large unlabelled datasets, \n",
    "              so they contain features that are more predictive of syntactic and semantic properties of words\n",
    "            * we can keep these parameters frozen (that is, we do not further train them)\n",
    "              which helps against overfitting to the small text classification dataset we have            \n",
    "        \"\"\"\n",
    "        # we always begin by calling the constructor of the parent class\n",
    "        super().__init__(vocab_size=vocab_size, num_classes=num_classes, pad_id=pad_id, bos_id=bos_id, eos_id=eos_id, unk_id=unk_id)\n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Now we construct the necessary NN blocks\n",
    "        \n",
    "        # First, we construct an embedding layer\n",
    "        if E is None:  # here we use a randomly initialised embedding matrix\n",
    "            self.word_embed = nn.Embedding(self.vocab_size, embedding_dim=word_embed_dim)\n",
    "        else:  # here we use the pretrained embeddings, and keep them frozen\n",
    "            if E.shape != (vocab_size, word_embed_dim):\n",
    "                raise ValueError(f\"E should have shape {(vocab_size, word_embed_dim)}, got {E.shape}\")\n",
    "            self.word_embed = nn.Embedding.from_pretrained(E, freeze=True)\n",
    "        \n",
    "        # Next, we construct an FFNN\n",
    "        self.logits_predictor = nn.Sequential(\n",
    "            nn.Dropout(p_drop),  # this is a special operation meant to avoid overfitting\n",
    "            nn.Linear(word_embed_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # You will be using self.word_embed and self.logits_predictor in `_predict_logits` below\n",
    "\n",
    "\n",
    "    def _predict_logits(self, x):\n",
    "        \"\"\"\n",
    "        For each document in the batch of inputs, compute the C-dimensional vetor of probabilities\n",
    "         that should parameterise the conditional Categorical distribution.\n",
    "\n",
    "        This method will encode the tokens in x (using D-dimensinoal embedding vectors),\n",
    "         average the token embeddings         \n",
    "         then predict logits (using an MLP).\n",
    "\n",
    "        It will do so in a batched way, so that we can use batches of documents.\n",
    "\n",
    "        :param x: [batch_size, max_length] a batch of documents, each document is a sequence of token ids\n",
    "        :return: output has shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # [batch_size, max_length] this is 1 if the token is _not_ a PAD token\n",
    "        valid_positions = x != self.pad\n",
    "        \n",
    "        # [batch_size, max_length, embed_dim]\n",
    "        e = self.word_embed(x) # embed tokens\n",
    "        h = average_pooling(e, valid_positions) # average of token embeddings makes the encoding of each doc\n",
    "        # finally, we predict logits\n",
    "        # [batch_size, num_classes]\n",
    "        s = self.logits_predictor(h)\n",
    "        return s\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy1qnmgtnOQ6"
   },
   "source": [
    "The following bit of code tests the output shape of your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tmcQJQ6nOQ6"
   },
   "outputs": [],
   "source": [
    "def test_avgemb_cls(vocab, labels, data, device=torch.device('cpu')):\n",
    "\n",
    "\n",
    "    model = AvgEmbClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=len(labels),\n",
    "        word_embed_dim=E.shape[-1],\n",
    "        hidden_size=100,\n",
    "        E=torch.tensor(E, dtype=torch.float32, device=device),\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"Model\")\n",
    "    print(model)\n",
    "    # report number of parameters\n",
    "    print(\"Model size (in number of trainable parameters):\", model.num_parameters())\n",
    "    print(\"Model size (including frozen modules):\", model.num_parameters(trainable_only=False))\n",
    "\n",
    "    batcher = DataLoader(data, batch_size=3, shuffle=False, collate_fn=LabelledCorpus.pad_to_longest)\n",
    "\n",
    "    for batch_x, batch_y in batcher:\n",
    "        model.train()\n",
    "        # the output should be C logits for each document in the batch\n",
    "        assert model._predict_logits(batch_x.to(device)).shape == (batch_x.shape[0], model.num_classes)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Isr9WCDnOQ6",
    "outputId": "703a5064-4b22-4e03-f723-8f4f74ce0fe8"
   },
   "outputs": [],
   "source": [
    "test_avgemb_cls(vocab, labels, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "H5tVw_y_zmDl",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:BiLSTM_Classifier'></a>\n",
    "\n",
    "## BiLSTM Classifier\n",
    "\n",
    "Our next model encodes the document using a bidirectional recurrent neural network.\n",
    "\n",
    "Here is the model\n",
    "\\begin{align}\n",
    "Y | X=w_{1:l} &\\sim \\mathrm{Categorical}(\\mathbf g(w_{1:l}; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf g$ is the following neural network:\n",
    "\\begin{align}\n",
    "\\mathbf e_i &= \\mathrm{embed}_D(w_i; \\theta_{\\text{glove}})  & i \\in [l]\\\\\n",
    "\\mathbf u_{1:l} &= \\mathrm{bilstm}_{2H}(\\mathbf e_{1:l}; \\theta_{\\text{enc}}) \\\\\n",
    "\\mathbf h &= \\mathbf u_l\\\\\n",
    "\\mathbf s &= \\mathrm{ffnn}_C(\\mathbf u_l; \\theta_{\\text{out}})\\\\\n",
    "\\mathbf g(w_{1:l}; \\theta) &= \\mathrm{softmax}(\\mathbf s)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before implementing the classifier, we implement a helper function which will help us fight overfitting to small datasets. This function realises a procedure known as _word dropout_. See the documentation for an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_dropout(x, pad, unk, rate=0.):  # this is a helper function we will use to specify a good encoder\n",
    "    \"\"\"\n",
    "    Neural networks have so many parameters that they tend to overfit. \n",
    "    The strategy we saw to counter overfitting in GLMs (i.e., L2 regularisation) isn't sufficient.\n",
    "    A strategy that's more effective for recurrent nets is something called \"word dropout\", \n",
    "        whereby we omit some words from the input at random. This tends to force the RNN\n",
    "        to learn generalisable features.\n",
    "        \n",
    "    :param x: input sequence (batched sequences of token ids)\n",
    "    :param pad: pad idx\n",
    "    :param unk: unk idx\n",
    "    :param rate: rate at which we omit words\n",
    "        we omit a word by replacing its token id by that of the UNK token\n",
    "        \n",
    "    :return: a perturbed version of the input x, with same shape.\n",
    "    \"\"\"\n",
    "    if rate <= 0.:\n",
    "        return x\n",
    "    # 1 if valid\n",
    "    vmask = (x != pad).float()\n",
    "    # 1 if dropped\n",
    "    rmask = (torch.rand(x.shape, device=vmask.device) < rate).float()\n",
    "    # if a position is valid and should be dropped, we replace it by unk\n",
    "    # else, we leave it unchanged\n",
    "    return torch.where(vmask + rmask == 2, torch.full_like(x, unk), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-3'></a> **Ungraded Exercise 3 - BiLSTMClassifier**\n",
    "\n",
    "Study the BiLSTMClassifier class below and complete the implementation of its `_predict_logits` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "gJBMqy60nOQ5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(Classifier):\n",
    "\n",
    "    def __init__(self, vocab_size, num_classes, word_embed_dim: int, hidden_size: int, cell_size: int, p_drop=0.5, w_drop=0.5, num_layers=1, E=None, pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "        \"\"\"\n",
    "        :param vocab_size: number of known words\n",
    "        :param num_classes: number of classes in the text classification problem\n",
    "        :param word_embed_dim: dimensionality of word embeddings (50 for GloVe)\n",
    "        :param hidden_size: dimensionality of hidden layer in the FFNN\n",
    "        :param cell_size: dimensionality of the LSTM cell\n",
    "        :param p_drop: dropout rate (for units)\n",
    "            To fight overfitting to small datasets, it is common to use a training trick called Dropout, \n",
    "                whereby we drop some units (i.e., map them to 0) at random.\n",
    "                We typically do that right before linear layers.\n",
    "                This parameter controls the proportion of random units we drop.\n",
    "        :param w_drop: dropout rate (for words)\n",
    "            This parameter controls a form of dropout that applies to token sequences\n",
    "            as inputs to recurrent layers (see helper function above).\n",
    "        :param num_layers: number of LSTMs to be stacked\n",
    "        :param E: pretrained embeddings (optional), this object needs to be a torch tensor of type torch.float32\n",
    "            Using pretrained embeddings helps in 2 ways: \n",
    "            * we can train those embeddings using very large unlabelled datasets, \n",
    "              so they contain features that are more predictive of syntactic and semantic properties of words\n",
    "            * we can keep these parameters frozen (that is, we do not further train them)\n",
    "              which helps against overfitting to the small text classification dataset we have            \n",
    "        \"\"\"\n",
    "        # we always begin by calling the constructor of the parent class\n",
    "        super().__init__(vocab_size=vocab_size, num_classes=num_classes, pad_id=pad_id, bos_id=bos_id, eos_id=eos_id, unk_id=unk_id)\n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Now we construct the necessary NN blocks\n",
    "        \n",
    "        # First, we construct an embedding layer\n",
    "        if E is None:  # here we use a randomly initialised embedding matrix\n",
    "            self.word_embed = nn.Embedding(self.vocab_size, embedding_dim=word_embed_dim)\n",
    "        else:  # here we use the pretrained embeddings, and keep them frozen\n",
    "            if E.shape != (vocab_size, word_embed_dim):\n",
    "                raise ValueError(f\"E should have shape {(vocab_size, word_embed_dim)}, got {E.shape}\")\n",
    "            self.word_embed = nn.Embedding.from_pretrained(E, freeze=True)\n",
    "\n",
    "        self.w_drop = w_drop\n",
    "\n",
    "        # Next, we construct a BiLSTM\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=word_embed_dim,\n",
    "            hidden_size=cell_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,  # this makes the encoder bidirectional\n",
    "            # don't change the next two arguments\n",
    "            batch_first=True,  # this is important for torch to understand our batches\n",
    "            # this is a form of regularisation that we can use _between_ \n",
    "            # BiLSTM layers, when we have more than 1;\n",
    "            # if used, it drops some output units (map them to 0)  of a BiLSTM layer \n",
    "            # before passing them on as inputs to the next BiLSTM layer\n",
    "            dropout=0. if num_layers == 1 else p_drop, \n",
    "        )\n",
    "\n",
    "        # Last, but not least, we construct an FFNN\n",
    "        # the bidirectional LSTM encoder produces outputs of size 2*hidden_size*num_layers\n",
    "        # thus our linear layer must take 2*hidden_size*num_layers inputs\n",
    "        self.logits_predictor = nn.Sequential(\n",
    "            # again, because NNs overfit easily and L2 regularisation is not sufficient\n",
    "            # to reduce this effect, we use techniques that have been shown more effective\n",
    "            # one such technique is called Dropout, \n",
    "            # whereby we drop some units (map them to 0) at random\n",
    "            # we normally use dropout right _before_ our linear layers\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(num_layers * 2 * cell_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def _predict_logits(self, x):\n",
    "        \"\"\"\n",
    "        For each document in the batch of inputs, compute the C-dimensional vetor of probabilities\n",
    "         that should parameterise the conditional Categorical distribution.\n",
    "\n",
    "        This method will encode the tokens in x (using D-dimensinoal embedding vectors),\n",
    "         compose the token embeddings in bidirectional context (using a BiLSTM),\n",
    "         then predict logits from the last BiLSTM state (using an FFNN).\n",
    "\n",
    "        It will do so in a batched way, so that we can use batches of documents.\n",
    "\n",
    "        :param x: [batch_size, max_length] a batch of documents, each document is a sequence of token ids\n",
    "\n",
    "        :return: tensor with shape [batch_size, num_classes].\n",
    "        \"\"\"\n",
    "\n",
    "        # [batch_size, max_length] this is 1 if the token is _not_ a PAD token\n",
    "        valid_positions = x != self.pad\n",
    "        # counts the number of non-PAD tokens\n",
    "        # [batch_size]\n",
    "        lengths = torch.sum(valid_positions.long(), -1)\n",
    "\n",
    "        # the first thing we do (if the network is being used in training mode)\n",
    "        # is to apply word dropout (as it helps fight overfitting)\n",
    "        if self.training:\n",
    "            # this perturbs some positions of the documents in x\n",
    "            # the result has the same shape and type as x\n",
    "            x = word_dropout(x, self.pad, self.unk, self.w_drop)\n",
    "\n",
    "        # Here you should \n",
    "        # 1. embed the tokens in x\n",
    "        # 2. obtain document encodings via the last state of a BiLSTM\n",
    "        # 3. obtain and return logits with shape [batch_size, num_classes]\n",
    "        raise NotImplementedError(\"Complete me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(Classifier):\n",
    "\n",
    "    def __init__(self, vocab_size, num_classes, word_embed_dim: int, hidden_size: int, cell_size: int, p_drop=0.5, w_drop=0.5, num_layers=1, E=None, pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "        \"\"\"\n",
    "        :param vocab_size: number of known words\n",
    "        :param num_classes: number of classes in the text classification problem\n",
    "        :param word_embed_dim: dimensionality of word embeddings (50 for GloVe)\n",
    "        :param hidden_size: dimensionality of hidden layer in the FFNN\n",
    "        :param cell_size: dimensionality of the LSTM cell\n",
    "        :param p_drop: dropout rate (for units)\n",
    "            To fight overfitting to small datasets, it is common to use a training trick called Dropout, \n",
    "                whereby we drop some units (i.e., map them to 0) at random.\n",
    "                We typically do that right before linear layers.\n",
    "                This parameter controls the proportion of random units we drop.\n",
    "        :param w_drop: dropout rate (for words)\n",
    "            This parameter controls a form of dropout that applies to token sequences\n",
    "            as inputs to recurrent layers (see helper function above).\n",
    "        :param num_layers: number of LSTMs to be stacked\n",
    "        :param E: pretrained embeddings (optional), this object needs to be a torch tensor of type torch.float32\n",
    "            Using pretrained embeddings helps in 2 ways: \n",
    "            * we can train those embeddings using very large unlabelled datasets, \n",
    "              so they contain features that are more predictive of syntactic and semantic properties of words\n",
    "            * we can keep these parameters frozen (that is, we do not further train them)\n",
    "              which helps against overfitting to the small text classification dataset we have            \n",
    "        \"\"\"\n",
    "        # we always begin by calling the constructor of the parent class\n",
    "        super().__init__(vocab_size=vocab_size, num_classes=num_classes, pad_id=pad_id, bos_id=bos_id, eos_id=eos_id, unk_id=unk_id)\n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Now we construct the necessary NN blocks\n",
    "        \n",
    "        # First, we construct an embedding layer\n",
    "        if E is None:  # here we use a randomly initialised embedding matrix\n",
    "            self.word_embed = nn.Embedding(self.vocab_size, embedding_dim=word_embed_dim)\n",
    "        else:  # here we use the pretrained embeddings, and keep them frozen\n",
    "            if E.shape != (vocab_size, word_embed_dim):\n",
    "                raise ValueError(f\"E should have shape {(vocab_size, word_embed_dim)}, got {E.shape}\")\n",
    "            self.word_embed = nn.Embedding.from_pretrained(E, freeze=True)\n",
    "\n",
    "        self.w_drop = w_drop\n",
    "\n",
    "        # Next, we construct a BiLSTM\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=word_embed_dim,\n",
    "            hidden_size=cell_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,  # this makes the encoder bidirectional\n",
    "            # don't change the next two arguments\n",
    "            batch_first=True,  # this is important for torch to understand our batches\n",
    "            # this is a form of regularisation that we can use _between_ \n",
    "            # BiLSTM layers, when we have more than 1;\n",
    "            # if used, it drops some output units (map them to 0)  of a BiLSTM layer \n",
    "            # before passing them on as inputs to the next BiLSTM layer\n",
    "            dropout=0. if num_layers == 1 else p_drop, \n",
    "        )\n",
    "\n",
    "        # Last, but not least, we construct an FFNN\n",
    "        # the bidirectional LSTM encoder produces outputs of size 2*hidden_size*num_layers\n",
    "        # thus our linear layer must take 2*hidden_size*num_layers inputs\n",
    "        self.logits_predictor = nn.Sequential(\n",
    "            # again, because NNs overfit easily and L2 regularisation is not sufficient\n",
    "            # to reduce this effect, we use techniques that have been shown more effective\n",
    "            # one such technique is called Dropout, \n",
    "            # whereby we drop some units (map them to 0) at random\n",
    "            # we normally use dropout right _before_ our linear layers\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(num_layers * 2 * cell_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def _predict_logits(self, x):\n",
    "        \"\"\"\n",
    "        For each document in the batch of inputs, compute the C-dimensional vetor of probabilities\n",
    "         that should parameterise the conditional Categorical distribution.\n",
    "\n",
    "        This method will encode the tokens in x (using D-dimensinoal embedding vectors),\n",
    "         compose the token embeddings in bidirectional context (using a BiLSTM),\n",
    "         then predict logits from the last BiLSTM state (using an FFNN).\n",
    "\n",
    "        It will do so in a batched way, so that we can use batches of documents.\n",
    "\n",
    "        :param x: [batch_size, max_length] a batch of documents, each document is a sequence of token ids\n",
    "\n",
    "        :return: tensor with shape [batch_size, num_classes].\n",
    "        \"\"\"\n",
    "\n",
    "        # [batch_size, max_length] this is 1 if the token is _not_ a PAD token\n",
    "        valid_positions = x != self.pad\n",
    "        # counts the number of non-PAD tokens\n",
    "        # [batch_size]\n",
    "        lengths = torch.sum(valid_positions.long(), -1)\n",
    "\n",
    "        # the first thing we do (if the network is being used in training mode)\n",
    "        # is to apply word dropout (as it helps fight overfitting)\n",
    "        if self.training:\n",
    "            x = word_dropout(x, self.pad, self.unk, self.w_drop)\n",
    "\n",
    "        # We now embed the tokens\n",
    "        # [batch_size, max_length, embed_dim]\n",
    "        e = self.word_embed(x)\n",
    "        # As we explained in the Encoder notebook, \n",
    "        # for torch to deal correctly with batches of sequences of difference length, \n",
    "        # we have to use the auxiliary functions pack_padded_sequence and pad_packed_sequence\n",
    "        packed_seqs = pack_padded_sequence(\n",
    "            e, \n",
    "            lengths.cpu(), # torch needs the lengths to be on CPU\n",
    "            # don't change the next two arguments\n",
    "            batch_first=True, \n",
    "            enforce_sorted=False  \n",
    "        )\n",
    "        u, (hx, cx) = self.encoder(packed_seqs)\n",
    "        u, _ = pad_packed_sequence(u, batch_first=True)\n",
    "\n",
    "        # we will classify from the concatenation of final states\n",
    "        # - permute moves the layer dimension to the end of the tensor, \n",
    "        # - flatten then concatenates the different layers\n",
    "        # the final shape is\n",
    "        # [batch_size, num_layers*2*hidden_dim]\n",
    "        h = torch.flatten(torch.permute(hx, (1, 2, 0)), 1, 2)\n",
    "\n",
    "        # an alternative idea would be to use the average of states u\n",
    "        # like this:\n",
    "        # h = average_pooling(u, valid_positions)        \n",
    "\n",
    "        # finally, we predict logits\n",
    "        # [batch_size, num_classes]\n",
    "        s = self.logits_predictor(h)\n",
    "        return s\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "dy1qnmgtnOQ6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The following bit of code tests the output shape of your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tmcQJQ6nOQ6"
   },
   "outputs": [],
   "source": [
    "def test_bilstm_cls(vocab, labels, data, device=torch.device('cpu')):\n",
    "\n",
    "\n",
    "    model = BiLSTMClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=len(labels),\n",
    "        word_embed_dim=E.shape[-1],\n",
    "        hidden_size=100,\n",
    "        cell_size=50,\n",
    "        E=torch.tensor(E, dtype=torch.float32)\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"Model\")\n",
    "    print(model)\n",
    "    # report number of parameters\n",
    "    print(\"Model size (in number of trainable parameters):\", model.num_parameters())\n",
    "    print(\"Model size (including frozen modules):\", model.num_parameters(trainable_only=False))\n",
    "\n",
    "    batcher = DataLoader(data, batch_size=3, shuffle=False, collate_fn=LabelledCorpus.pad_to_longest)\n",
    "\n",
    "    for batch_x, batch_y in batcher:\n",
    "        model.train()\n",
    "        # the output should be C logits for each document in the batch\n",
    "        assert model._predict_logits(batch_x.to(device)).shape == (batch_x.shape[0], model.num_classes)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "8Isr9WCDnOQ6",
    "outputId": "703a5064-4b22-4e03-f723-8f4f74ce0fe8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_bilstm_cls(vocab, labels, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "0R1QKLf6WWxT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Training_and_Evaluation'></a>\n",
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "BGzeKX-wv9NT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this section we provide all the necessary code to train and test our NN classifiers correctly. \n",
    "\n",
    "We need to prepare a lot of helper code, and here's an overview of what you will find next:\n",
    "* `predict` will use our model to predict labels for the documents in a given data loader;\n",
    "* `train_neural_model` will take a randomly initialised model and estimate its parameters via SGD, for that it requires some training data, it will all use `predict` every so often, in order to find which model checkpoint performs best on dev;\n",
    "* `test_saved_model` will model parameters stored in a file (for example, after training) and test that model in a given corpus;\n",
    "* `train_and_test` will run `train_neural_model` (using training and dev corpora) and, at the end, `test_saved_model` on dev and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-4'></a> **Ungraded Exercise 4 - Training and evaluation code**\n",
    "\n",
    "Study the implementation of the main functions needed for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "zZV5c0WEhdk4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model: Classifier, vocab: Vocabulary, labels: tuple, dl: DataLoader, device, return_targets=False, return_loss=False):\n",
    "    \"\"\"\n",
    "    Mode predictions.\n",
    "\n",
    "    :param model: one of our classifiers\n",
    "    :param vocab:\n",
    "    :param labels: the classe names in this text classification problem\n",
    "    :param dl: a data loader for the heldout data\n",
    "    :param device: the PyTorch device where the model is stored\n",
    "    :param return_targets: also return the targets from the data loader\n",
    "        you can use this when the actual targets are in the dataloader (e.g., for dev set)\n",
    "\n",
    "    :return:\n",
    "        * a list of predictions, each the most probable class\n",
    "        * if return_targets=True, additionally return a list of targets (from the data loader)\n",
    "    \"\"\"\n",
    "    # we first inform torch that we are in eval mode, \n",
    "    # this will switch off certain training-only tricks (e.g., dropout)\n",
    "    model.eval()\n",
    "    # here we will keep information from the various documents\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_losses = []\n",
    "    data_size = 0\n",
    "    # in prediction mode, we do not need to keep track of gradients\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dl:  # for each batch\n",
    "            data_size += batch_x.shape[0]\n",
    "            # [batch_size] \n",
    "            # predict the modes\n",
    "            preds = model.mode(batch_x.to(device))\n",
    "            all_preds.extend((labels[y] for y in preds))\n",
    "            if return_targets:\n",
    "                all_targets.extend((labels[y] for y in batch_y))\n",
    "            if return_loss:\n",
    "                loss = model.loss(batch_x.to(device), batch_y.to(device))\n",
    "                all_losses.append(loss.cpu() * batch_x.shape[0])\n",
    "\n",
    "    r = {'preds': all_preds}\n",
    "\n",
    "    if return_targets:\n",
    "        r['targets'] = all_targets\n",
    "    if return_loss:\n",
    "        r['loss'] = np.sum(all_losses) / data_size\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "a2atjWtPwF8w",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can use sklearn's classification report to compute metrics relevant for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "OnlLnap0ot3A",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "x7u12R0t3mAa",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here we have the training loop (already fully implemented for you). Do study it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "ZQR5TGAqd-HC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_neural_model(exp_name: str,\n",
    "        model: Classifier, optimiser,\n",
    "        vocab: Vocabulary, labels: tuple, \n",
    "        training_corpus: LabelledCorpus, \n",
    "        dev_corpus: LabelledCorpus, \n",
    "        batch_size=200, num_epochs=5, check_every=10, \n",
    "        device=torch.device('cpu'),\n",
    "        ckptdir=\"checkpoints\",\n",
    "        criterion=\"macrof1\"):\n",
    "    \"\"\"\n",
    "    Here we use a training dataset to estimate the trainable parameters of our model\n",
    "     by minimising a loss based on the average negative log probability assigned to the observed data.\n",
    "\n",
    "    :param exp_name: experiment name (used to save model parameters to disk)\n",
    "    :param model: pytorch model\n",
    "    :param optimiser: pytorch optimiser\n",
    "    :param training_corpus: a LabelledCorpus for training\n",
    "    :param dev_corpus: a LabelledCorpus for dev\n",
    "    :param batch_size: use more if you have more memory\n",
    "    :param num_epochs: use more for improved convergence\n",
    "    :param check_every: use less to check performance on dev set more often\n",
    "    :param device: where we run the experiment\n",
    "    :param ckptdir: directory to store checkpoints\n",
    "    :param criterion: either macrof1 or devloss\n",
    "        we implement two criteria for model selection, so you can learn from this code\n",
    "        but, throughout this notebook, we will always use 'macrof1'\n",
    "    :return: a log of quantities computed during training (for plotting)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(ckptdir):  # we will be storing models in this folder\n",
    "        os.makedirs(ckptdir)\n",
    "    if criterion not in ['macrof1', 'devloss']:\n",
    "        raise ValueError(f\"criterion must be macrof1 or devloss, got {criterion}\")\n",
    "        \n",
    "    # we use the training data in random order for parameter estimation\n",
    "    batcher = DataLoader(training_corpus, batch_size=batch_size, shuffle=True, collate_fn=LabelledCorpus.pad_to_longest)\n",
    "    # we use the dev data for evaluation during training (no need for randomisation here)\n",
    "    dev_batcher = DataLoader(dev_corpus, batch_size=batch_size, shuffle=False, collate_fn=LabelledCorpus.pad_to_longest)\n",
    "\n",
    "    # we will train for this many steps\n",
    "    total_steps = num_epochs * len(batcher)\n",
    "    log = defaultdict(list)\n",
    "\n",
    "    # we start by evaluating the untrained model\n",
    "    r = predict(\n",
    "        model,\n",
    "        vocab,\n",
    "        labels,\n",
    "        dev_batcher,\n",
    "        device=device,\n",
    "        return_targets=True,\n",
    "        return_loss=True,\n",
    "    )\n",
    "    report = classification_report(r['targets'], r['preds'], output_dict=True, zero_division=0)\n",
    "    acc, macrof1, devloss = report['accuracy'], report['macro avg']['f1-score'], r['loss']\n",
    "    log['acc'].append(acc)\n",
    "    log['macro-f1'].append(macrof1)\n",
    "    log['dev-loss'].append(r['loss'])\n",
    "\n",
    "    best_devloss, best_macrof1 = devloss, macrof1\n",
    "    # we save this initial model (though hopefully we will find better ones)\n",
    "    torch.save(model.state_dict(), f\"{ckptdir}/{exp_name}_{criterion}.pt\")\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    # and then train the model for a number of steps\n",
    "    with tqdm(range(total_steps)) as bar:        \n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_x, batch_y in batcher:\n",
    "                # we always start by telling torch that we are in training mode\n",
    "                # this enables training-only tricks (e.g., dropout)\n",
    "                model.train()  \n",
    "                # we then set all gradients to 0 \n",
    "                # (if we forget this, the gradients based on the data in this batch\n",
    "                # will be accumulated to the gradients obtained from a previous batch\n",
    "                # which is not what we intend to use for optimisation)\n",
    "                optimiser.zero_grad()\n",
    "                # we evaluate the loss \n",
    "                loss = model.loss(batch_x.to(device), batch_y.to(device))\n",
    "                # and obtain gradients for all trainable parameters\n",
    "                loss.backward()\n",
    "                # now we can take a step towards the direction of steepest descent\n",
    "                optimiser.step()\n",
    "    \n",
    "                # here we update the progress bar\n",
    "                bar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.2f}\",\n",
    "                    'dev-loss': f\"{devloss:.2f}\",\n",
    "                    'acc': f\"{acc:.2f}\",\n",
    "                    'macro-f1': f\"{macrof1:.2f}\",\n",
    "                })\n",
    "                bar.update()\n",
    "                log['loss'].append(loss.item())\n",
    "    \n",
    "                if step % check_every == 0:  # every so often, we evaluate performance on dev set\n",
    "    \n",
    "                    r = predict(\n",
    "                        model,\n",
    "                        vocab,\n",
    "                        labels,\n",
    "                        dev_batcher,\n",
    "                        device=device,\n",
    "                        return_targets=True,\n",
    "                        return_loss=True\n",
    "                    )\n",
    "    \n",
    "                    report = classification_report(r['targets'], r['preds'], output_dict=True, zero_division=0)\n",
    "                    acc, macrof1, devloss = report['accuracy'], report['macro avg']['f1-score'], r['loss']\n",
    "                    log['acc'].append(acc)\n",
    "                    log['macro-f1'].append(macrof1)\n",
    "                    log['dev-loss'].append(r['loss'])\n",
    "    \n",
    "                    # when we find a better model (or at least not worse), we save it\n",
    "                    # what counts as a better model matters, \n",
    "                    # there are two common ways to go about this\n",
    "                    # * either we select models using the validation (dev) loss\n",
    "                    # * or we select models using a measure of classification performance\n",
    "                    # both strategies are relatively common\n",
    "                    # with classification performance being slightly more popular\n",
    "                    # we code both of them, so you can learn from this\n",
    "                    # but, in this notebook, we use macrof1\n",
    "\n",
    "                    if criterion == 'devloss' and devloss <= best_devloss:\n",
    "                        torch.save(model.state_dict(), f\"{ckptdir}/{exp_name}_devloss.pt\")\n",
    "                        best_devloss = devloss                    \n",
    "                    if criterion == 'macrof1' and macrof1 >= best_macrof1:\n",
    "                        torch.save(model.state_dict(), f\"{ckptdir}/{exp_name}_macrof1.pt\")\n",
    "                        best_macrof1 = macrof1\n",
    "    \n",
    "                step += 1\n",
    "\n",
    "    # once we are done with training we evaluate again\n",
    "    r = predict(\n",
    "        model,\n",
    "        vocab,\n",
    "        labels,\n",
    "        dev_batcher,\n",
    "        device=device,\n",
    "        return_targets=True,\n",
    "        return_loss=True\n",
    "    )\n",
    "    report = classification_report(r['targets'], r['preds'], output_dict=True, zero_division=0)\n",
    "    acc, macrof1 = report['accuracy'], report['macro avg']['f1-score']    \n",
    "    log['acc'].append(acc)\n",
    "    log['macro-f1'].append(macrof1)\n",
    "    log['dev-loss'].append(r['loss'])\n",
    "\n",
    "    # If we were selecting on devloss, we would want to save this final check\n",
    "    if criterion == 'devloss' and devloss <= best_devloss:\n",
    "        torch.save(model.state_dict(), f\"{ckptdir}/{exp_name}_devloss.pt\")\n",
    "        best_devloss = devloss\n",
    "    # In this notebook, we are using macrof1:\n",
    "    if criterion == 'macrof1' and macrof1 >= best_macrof1:\n",
    "        torch.save(model.state_dict(), f\"{ckptdir}/{exp_name}_macrof1.pt\")\n",
    "        best_macrof1 = macrof1\n",
    "\n",
    "    return log, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "Di9Vsuv7nOQ8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_saved_model(model: Classifier, vocab, labels, corpus, filename: str, batch_size=100, device=torch.device('cpu')):\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    # Predict for test set\n",
    "    r = predict(\n",
    "        model,\n",
    "        vocab,\n",
    "        labels,\n",
    "        DataLoader(corpus, batch_size=batch_size, shuffle=False, collate_fn=LabelledCorpus.pad_to_longest),\n",
    "        device,\n",
    "        return_targets=True,\n",
    "        return_loss=True\n",
    "    )    \n",
    "    return r\n",
    "\n",
    "def train_and_test(exp_name: str,\n",
    "        model: Classifier, \n",
    "        vocab: Vocabulary,\n",
    "        labels: tuple,\n",
    "        training_corpus: LabelledCorpus, \n",
    "        dev_corpus: LabelledCorpus, \n",
    "        test_corpus=None,                   \n",
    "        lr=1e-3, \n",
    "        weight_decay=1e-4, \n",
    "        batch_size=100, num_epochs=20, check_every=100, \n",
    "        device=torch.device('cpu'),\n",
    "        ckptdir=\"checkpoints\",\n",
    "        criterion=\"macrof1\"):\n",
    "    \"\"\"\n",
    "    Train and test a classifier.\n",
    "\n",
    "    :param exp_name: experiment name (used to save the model weights to disk)\n",
    "    :param model: a classifier model\n",
    "    :param training_corpus: corpus used for training\n",
    "    :param dev_corpus: corpus used for validation (dev)\n",
    "    :param test_corpus: if provided, we test the model at the end on this corpus\n",
    "    :param lr: learning rate for SGD\n",
    "    :param weight_decay: weight of the L2 regulariser\n",
    "        when we worked in Jax we implemented the L2 regulariser ourselves,\n",
    "        in Torch the optimiser object will do it for us, we just need to set this\n",
    "        number to something larger than 0\n",
    "    :param batch_size: documents per batch\n",
    "    :param num_epochs: number of passes over the entire training data\n",
    "    :param check_every: how often we evaluate on dev_corpus    \n",
    "    :param device: torch device\n",
    "    :param ckptdir: directory to store checkpoints\n",
    "    :param criterion: either macrof1 or devloss\n",
    "        we implement two criteria for model selection, so you can learn from this code\n",
    "        but, throughout this notebook, we will always use 'macrof1'\n",
    "    \"\"\"\n",
    "    if not os.path.exists(ckptdir):  # we will be storing models in this folder\n",
    "        os.makedirs(ckptdir)\n",
    "    if criterion not in ['macrof1', 'devloss']:\n",
    "        raise ValueError(f\"criterion must be macrof1 or devloss, got {criterion}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    print(\"Model\")\n",
    "    print(model)\n",
    "    # report number of parameters\n",
    "    print(\"Model size (in number of trainable parameters):\", model.num_parameters())\n",
    "\n",
    "    # construct an Adam optimiser\n",
    "    optimiser = opt.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    print(\"Training...\")\n",
    "    # Train the model\n",
    "    log, model = train_neural_model(\n",
    "        exp_name,\n",
    "        model, optimiser, \n",
    "        vocab, labels,\n",
    "        training_corpus, dev_corpus,\n",
    "        batch_size=batch_size, num_epochs=num_epochs, check_every=check_every,\n",
    "        device=device,\n",
    "        ckptdir=ckptdir,\n",
    "        criterion=criterion\n",
    "    )\n",
    "\n",
    "    # Plot loss and validation checks\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(15, 4))\n",
    "    _ = axs[0].plot(np.arange(len(log['loss'])), log['loss'])\n",
    "    _ = axs[0].set_xlabel('steps')\n",
    "    _ = axs[0].set_ylabel('training loss')\n",
    "    _ = axs[1].plot(np.arange(len(log['dev-loss'])), log['dev-loss'])\n",
    "    _ = axs[1].set_xlabel('steps')\n",
    "    _ = axs[1].set_ylabel('dev loss')\n",
    "    _ = axs[2].plot(np.arange(len(log['acc'])), log['acc'])\n",
    "    _ = axs[2].set_xlabel('steps (in 10s)')\n",
    "    _ = axs[2].set_ylabel('dev acc')\n",
    "    _ = axs[3].plot(np.arange(len(log['macro-f1'])), log['macro-f1'])\n",
    "    _ = axs[3].set_xlabel('steps (in 10s)')\n",
    "    _ = axs[3].set_ylabel('dev macro-f1')\n",
    "\n",
    "    _ = fig.tight_layout(h_pad=2, w_pad=2)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Testing on dev_corpus: selection on {criterion}\")\n",
    "    r = test_saved_model(model, vocab, labels, dev_corpus, f\"{ckptdir}/{exp_name}_{criterion}.pt\", batch_size=batch_size, device=device)\n",
    "    # Compare predictions and targets\n",
    "    print(classification_report(r['targets'], r['preds'], zero_division=0))\n",
    "\n",
    "    if test_corpus is not None:  # if we were given a test set, we should use it to evaluate the model\n",
    "        print(f\"Testing on test_corpus: selection on {criterion}\")\n",
    "        r = test_saved_model(model, vocab, labels, test_corpus, f\"{ckptdir}/{exp_name}_{criterion}.pt\", batch_size=batch_size, device=device)\n",
    "        # Compare predictions and targets\n",
    "        print(classification_report(r['targets'], r['preds'], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "RwXhhAUChsKT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "Here we demonstrate how to train and evaluate a model on NLTK's subjectivity dataset.\n",
    "After that you will conduct an experiment with this dataset.\n",
    "\n",
    "For this corpus, you can continue without GPU support, provided that your laptop has enough memory. For larger corpora, you may need to use Google Colab (change the runtime to GPU). The code base works for classification problems with more than 2 classes (but we will not experiment with those because we want a lightweight experiment that you can run on your own laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "pKFUw21bYCUm",
    "outputId": "11eeea04-1705-4384-f386-322e2b23a927",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda:0')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "my_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About a minute on CPU, it should report about 90 macro F1 on dev:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_all() # reset random number generators before creating your model and training it\n",
    "train_and_test(\n",
    "    \"avgemb_base\",\n",
    "    AvgEmbClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=len(labels),\n",
    "        word_embed_dim=E.shape[1],\n",
    "        hidden_size=100,\n",
    "        p_drop=0.,\n",
    "        E=torch.tensor(E, dtype=torch.float32)\n",
    "    ),\n",
    "    vocab, labels,\n",
    "    training, dev, \n",
    "    lr=1e-3, weight_decay=1e-4,\n",
    "    batch_size=100, num_epochs=50, check_every=200, device=my_device\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 4 minutes on CPU, it should report about 92 macro F1 on dev:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_all() # reset random number generators before creating your model and training it\n",
    "train_and_test(\n",
    "    f\"bilstm_base\",\n",
    "    BiLSTMClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=len(labels),\n",
    "        word_embed_dim=E.shape[1],\n",
    "        hidden_size=100,\n",
    "        cell_size=100,\n",
    "        p_drop=0.5,\n",
    "        w_drop=0.5,\n",
    "        E=torch.tensor(E, dtype=torch.float32)\n",
    "    ),\n",
    "    vocab, labels,\n",
    "    training, dev, \n",
    "    lr=1e-2, weight_decay=1e-4,\n",
    "    batch_size=100, num_epochs=50, check_every=200, device=my_device \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Whenever we train a model we should pay attention to a few things:\n",
    "- Does it look like the optimiser managed to minimise the training loss? You want to see a training loss curve that's generally decreasing, some variance is expected, but there should be a mostly clear downward trend.\n",
    "- Does it look like the model is overfitting? Sometimes we see that the training loss is going down, but the validation/dev loss is going up. This typically suggests that the model is finding spurious features to explain the training data, these features are useful for those data points specifically but they fail to generalise to new data points. \n",
    "- Does it look like the model's classification performance is improving? We should track an appropriate metric, such as accuracy or macro F1. It's not always the case that classification performance improves the longer we train, and that's because likelihood is only a proxy to classification performance. So, we tend to track both things. \n",
    "\n",
    "Sometimes we observe some overfitting and yet accuracy or macro F1 improves. While it's tempting to trust this result (after all, we would like to be able to obtain better classification performance), it is wiser to practice some skepticism: any sign of overfitting is something to be worried about, esp for large and complex models such as NNs. Oftentimes our dev and test sets give us an optimistic idea of how different from training data future data points may be. So, we generally prefer models that exhibit higher accuracy or macro F1 _while_ also exhibiting a reasonable dev loss behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Graded_Experiment'></a>\n",
    "# Graded Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In the next two exercises, you will change a few design choices of your model and/or optimisation algorithm (we call those \"hyper-parameters\") and inspect their effects on the trained model. \n",
    "\n",
    "To make sure that your models are implemented correctly, we recommend you use the solutions of the ungraded quizzes for the missing parts of the two model classes.\n",
    "\n",
    "**Instructions for the graded exercises below**\n",
    "\n",
    "We will present a list of design choices, for each of them we will list some options for the hyperparameters associated with it. You should study the effect of those choices. As there are too many combinations, instead of testing them all jointly, we will use a technique called _ablation_ and study one design choice at a time. In an _ablation_ experiment we have a _base_ configuration which we take as the reference in the comparison, then we inspect the effect of changing 1  design choice, always in relation to that base configuration. We then revert to the _base_ configuration before investigating the next design choice.\n",
    "\n",
    "Besides running the experiment and reporting the plots and classification report that `train_and_test` produces, you must discuss at least the following points for each design choice:\n",
    "\n",
    "* Does your model get bigger?\n",
    "* Does the training loss converge?\n",
    "* Do you observe overfitting? \n",
    "* Do you observe macro F1 improvements?\n",
    "\n",
    "For each of these aspects, your discussion can be brief (e.g., a couple of sentences would do: answer the question, indicate what evidence you used to back your answer).\n",
    "\n",
    "*And don't forget to be organised, the grader has no obligation to try and make sense of a messy notebook.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-1'> **Graded Exercise 1 (62.5\\%) - Ablations for AvgEmb model** </a>\n",
    "\n",
    "For AvgEmb, the base is\n",
    "```python\n",
    "train_and_test(\n",
    "    \"avgemb_base\",\n",
    "    AvgEmbClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=len(labels),\n",
    "        word_embed_dim=E.shape[1],\n",
    "        hidden_size=100,\n",
    "        p_drop=0.,\n",
    "        E=torch.tensor(E, dtype=torch.float32)\n",
    "    ),\n",
    "    vocab, labels,\n",
    "    training, dev, \n",
    "    lr=1e-3, weight_decay=1e-4,\n",
    "    batch_size=100, num_epochs=50, check_every=200, device=my_device\n",
    ") \n",
    "```\n",
    "and you should perform ablations for:\n",
    "\n",
    "1. embedding parameters: randomly initialised (but trained) instead of GloVe (all you need to do is change the constructor's argument to `E=None`).\n",
    "2. number of hidden units in the FFNN: 50 and 200 instead of 100\n",
    "3. learning rate: 1e-2 and 1e-4 instead of 1e-3\n",
    "4. dropout: 0.25 and 0.5 instead of 0.    \n",
    "5. weight decay: 0. and 1e-2 instead of 1e-4\n",
    "\n",
    "Each experiment will run in under a minute on CPU (also on GPU), the experiment without GloVe will take a bit longer (about a minute on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tCONTRIBUTE YOUR SOLUTION/DISCUSSION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-2'> **Graded Exercise 2 (37.5\\%) - Ablations for BiLSTM model** </a>\n",
    "\n",
    "For BiLSTM, the base is\n",
    "\n",
    "```python\n",
    "train_and_test(\n",
    "    f\"bilstm_base\",\n",
    "    BiLSTMClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=len(labels),\n",
    "        word_embed_dim=E.shape[1],\n",
    "        hidden_size=100,\n",
    "        cell_size=100,\n",
    "        p_drop=0.5,\n",
    "        w_drop=0.5,\n",
    "        E=torch.tensor(E, dtype=torch.float32)\n",
    "    ),\n",
    "    vocab, labels,\n",
    "    training, dev, \n",
    "    lr=1e-2, weight_decay=1e-4, \n",
    "    batch_size=100, num_epochs=50, check_every=200, device=my_device \n",
    ") \n",
    "```\n",
    "and you should perform ablations for:\n",
    "\n",
    "1. embedding parameters: randomly initialised (but trained) instead of GloVe (all you need to do is change the constructor's argument to `E=None`)\n",
    "2. learning rate: 1e-1 and 1e-3 instead of 1e-2\n",
    "3. word dropout: 0 and 0.25 instead of 0.5\n",
    "\n",
    "Each experiment will take about 4 minutes on CPU (and under a minute on GPU). The experiment without GloVe will take a bit longer (about 6 minutes on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tCONTRIBUTE YOUR SOLUTION/DISCUSSION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Ungraded_Experiment'></a>\n",
    "# Ungraded Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-5'></a> **Ungraded Exercise 5 - Experiment with complete dataset**\n",
    "\n",
    "Run an experiment with the complete dataset (remember that for that you need to re-run `prepare_nltk_corpus` to get all data points via `portion=1.0`, you also have obtain a fresh Vocabulary object and update it with all known words as well as with the words in GloVe, you should also reconstruct the embedding matrix since the vocabulary changed, and finally reconstruct the `LabelledCorpus` objects for each of the splits), try the base configurations of the two models (they should get about 90 and 94 macro F1 on dev, respectively). Be warned that the BiLSTM can take up to 10 minutes on CPU.\n",
    "\n",
    "For reference, a naive Bayes classifier using bag-of-words features gets about 91 macro F1 on dev. Because this dataset is rather small, NBC is a very strong baseline (meaning, it is difficult to beat it). NNs have many more parameters and they can easily overfit to small datasets (that's why we use dropout, pretrained frozen embeddings, etc). In larger datasets and also especially for more difficult tasks, NNs often have an edge over NBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "### SOLUTION CELL\n",
    "corpus = subjectivity\n",
    "labels = tuple(corpus.categories())\n",
    "print(f\"{len(corpus.sents())} labelled documents\")\n",
    "print(\"{}-way classification:\\n{}\".format(len(labels), '\\n'.join(labels)))\n",
    "\n",
    "print(\"# Prepare NLTK corpus\")\n",
    "training_pairs, dev_pairs, test_pairs = prepare_nltk_corpus(corpus, labels, max_length=50, portion=1.0)\n",
    "print(\"Data size (training, dev, test):\")\n",
    "print(training_pairs.shape, dev_pairs.shape, test_pairs.shape)\n",
    "\n",
    "print(\"# Construct Vocabulary\")\n",
    "vocab = Vocabulary()\n",
    "update_vocabulary_from_corpus(vocab, (xy[0].split() for xy in training_pairs), min_freq=1)\n",
    "print(f\"Vocab size before GloVe: {len(vocab)}\")\n",
    "for w in glove.keys():\n",
    "    vocab.add(w)\n",
    "print(f\"Vocab size after GloVe: {len(vocab)}\")\n",
    "\n",
    "print(\"# Construct embedding matrix\")\n",
    "E = make_embedding_matrix(vocab, glove, 50)\n",
    "print(E.shape)\n",
    "\n",
    "print(\"# Construct LabelledCorpus objects\")\n",
    "training = LabelledCorpus(\n",
    "    # these are the input documents (we use split so they are sequences of tokens)\n",
    "    (x.split() for x in training_pairs[:, 0]),  \n",
    "    training_pairs[:, 1], # these are the documents' labels\n",
    "    vocab,\n",
    "    labels\n",
    ")\n",
    "dev = LabelledCorpus(\n",
    "    # these are the input documents (we use split so they are sequences of tokens)\n",
    "    (x.split() for x in dev_pairs[:, 0]),\n",
    "    dev_pairs[:, 1],\n",
    "    vocab,\n",
    "    labels\n",
    ")\n",
    "test = LabelledCorpus(\n",
    "    # these are the input documents (we use split so they are sequences of tokens)\n",
    "    (x.split() for x in test_pairs[:, 0]),\n",
    "    test_pairs[:, 1],\n",
    "    vocab,\n",
    "    labels\n",
    ")\n",
    "\n",
    "print(\"# Train and test AvgEmb model\")\n",
    "train_and_test(\n",
    "    \"subjectivity_avgemb_base\",\n",
    "    AvgEmbClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=len(labels),\n",
    "        word_embed_dim=E.shape[1],\n",
    "        hidden_size=100,\n",
    "        p_drop=0.,\n",
    "        E=torch.tensor(E, dtype=torch.float32)\n",
    "    ),\n",
    "    vocab, labels,\n",
    "    training, dev, \n",
    "    lr=1e-3, batch_size=100, num_epochs=50, check_every=200, weight_decay=1e-4, device=my_device\n",
    ") \n",
    "\n",
    "      \n",
    "print(\"# Train and test BiLSTM model\")\n",
    "seed_all() # reset random number generators before creating your model and training it\n",
    "train_and_test(\n",
    "    \"subjectivity_bilstm_base\",\n",
    "    BiLSTMClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=len(labels),\n",
    "        word_embed_dim=E.shape[1],\n",
    "        hidden_size=100,\n",
    "        cell_size=100,\n",
    "        p_drop=0.5,\n",
    "        w_drop=0.5,\n",
    "        E=torch.tensor(E, dtype=torch.float32)\n",
    "    ),\n",
    "    vocab, labels,\n",
    "    training, dev, test,\n",
    "    lr=1e-2, batch_size=100, num_epochs=50, check_every=200, weight_decay=1e-4, device=my_device \n",
    ") \n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": false,
   "name": "T4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "009d5f4cfecf46259cdf4d5b7d4f739d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01db01ae82b5421a83145b13db2bcb03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0b99fc6e4d024b3fa04b3afcf6f7331a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ec73b20c9854dca937e897488526266": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f07d3c59d914acba1e5e34876a11b27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10f866d45ae8401caac34a5e02af7dd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dcf6d6a3b6a141e0937ee1270b06d685",
       "IPY_MODEL_941c2990be56492db884fb7a9387cd33",
       "IPY_MODEL_ed81e30d6f9a47a38509143b3aa93a9f"
      ],
      "layout": "IPY_MODEL_86481c6d8c3d4a71a33bf1003898e325"
     }
    },
    "18717b9778ed4427a96302e1441337cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1acb8f65fe8b44cca3ced96a5dc71022": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1ade4dc65b4b48fea6e360f2f6845cc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cec42179bd4427496699365c1e37621": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2145867522284105b18324a4592345bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23a0ce5810ba4e8fab36f8ebbfa9ba88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b534b24b46d345c996f161aa9310560c",
      "max": 147787,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e709b881f8f434e826f9a36a36be505",
      "value": 147787
     }
    },
    "2a1ef97827a242f6a0879a57c94334d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a201baa0e2b4b729279b895d5f60f7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a4adfb814954963b42af8ff8c0c3721": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c38050de05bc446ea5158b258951cf53",
      "placeholder": "​",
      "style": "IPY_MODEL_51cac086592f4b8ba72cef8fc55e2ace",
      "value": " 1821/1821 [00:00&lt;00:00, 55648.61 examples/s]"
     }
    },
    "2be560f8ee8c45869ebd50e2ab1ba37d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c3a0c753cda48b9a298f84c73956ac0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c4b98f8d27e425f8ce8893ef200f474": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2eaf26210d874c78a0154c4025d4c455": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36570ba587704a9291bddd3cf05c64d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38f09c7595bc47fda2c7d28e86ead41e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a6b0f1a276f46778298f484eabff433": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3da3b6503a33472fa9674ecd115354e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ddb631757cb46abb9ae60b2d0496763",
       "IPY_MODEL_d908a484a320473993fa9fc21ddc30a5",
       "IPY_MODEL_f470afaedaf94d9bbfc28a106ca1ea97"
      ],
      "layout": "IPY_MODEL_9415220f7ef149a29c055e28bdd96cb6"
     }
    },
    "3e709b881f8f434e826f9a36a36be505": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ef6945387944042beddcfdf3ddb30cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "40de5ce2a91448cf94f24de90e202b65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ce22a61ac704525bb71c298215e312a",
       "IPY_MODEL_e9c73b9f14ea433bbac1dc9fa7433c01",
       "IPY_MODEL_77bd22e9c20f4341b2b01ce100402a26"
      ],
      "layout": "IPY_MODEL_8215e53545144610a7b47dee0bef4efe"
     }
    },
    "45577013b04141459b6c1ae17f292272": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c553cdaa9d64fbeb9e45c8de7b56157",
      "placeholder": "​",
      "style": "IPY_MODEL_bdbc928e77bd4fc4a20c6e3fcccc0a83",
      "value": "Downloading data: 100%"
     }
    },
    "487e8da034d14e689ce4fb0fc79d3536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57455917a25349c3bf00cf17e4474e5b",
       "IPY_MODEL_612be6b070f84659a5cfcad58ad8d9cd",
       "IPY_MODEL_df577e05dd604729b9d8ccbf7491f9ca"
      ],
      "layout": "IPY_MODEL_4a5cb34df3034c20b789cd49a15887c7"
     }
    },
    "48f22095488e4745ad2c0ee899d17521": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a5cb34df3034c20b789cd49a15887c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c4f7c52ec8e45cb8d8bd76b5a6c4ba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c553cdaa9d64fbeb9e45c8de7b56157": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e260f9a0edd4037a37ba1814b514b22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51cac086592f4b8ba72cef8fc55e2ace": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "569e51ad289e4bdd9c62ec1bf01e55e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57455917a25349c3bf00cf17e4474e5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c3a0c753cda48b9a298f84c73956ac0",
      "placeholder": "​",
      "style": "IPY_MODEL_f844c26592da46e5b16ddf82ef5c7184",
      "value": "Generating train split: 100%"
     }
    },
    "5abf82fa4c9542748a0429250068e652": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "612be6b070f84659a5cfcad58ad8d9cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a6b0f1a276f46778298f484eabff433",
      "max": 67349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6fd49cdcd934907ae4560a3877182b4",
      "value": 67349
     }
    },
    "65891b439c0a4f5ca1031a9ad30b01aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48f22095488e4745ad2c0ee899d17521",
      "placeholder": "​",
      "style": "IPY_MODEL_2145867522284105b18324a4592345bf",
      "value": "Downloading data: 100%"
     }
    },
    "6a12047d2fea4c309024402d89ad567a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c221f6c6e414b7c953b5beaf6fe6699": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_65891b439c0a4f5ca1031a9ad30b01aa",
       "IPY_MODEL_8934a97087f14ced87c29a22b4a4dc47",
       "IPY_MODEL_7ec4c2e715384876b8af721d0ee2fd0a"
      ],
      "layout": "IPY_MODEL_7876d80bd79840dbbcc9ef253fab033e"
     }
    },
    "6f87737ac4f44a9ba999cdf7c76a253f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efcc7ef8af6346f1bb625eb808a587fb",
      "placeholder": "​",
      "style": "IPY_MODEL_18717b9778ed4427a96302e1441337cf",
      "value": " 3.11M/3.11M [00:03&lt;00:00, 996kB/s]"
     }
    },
    "703e6392ac914844b438fe14de6692c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afca4460e64c458c8e23e9560f4ff5a3",
      "max": 1821,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e45d64fada942d2b8e663611966e4ea",
      "value": 1821
     }
    },
    "72ef065c0ecc438ea7fe5bc7c8a02a94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cec42179bd4427496699365c1e37621",
      "placeholder": "​",
      "style": "IPY_MODEL_d3287a0d936f4238aef4bfa917c51fd5",
      "value": "Downloading data: 100%"
     }
    },
    "77bd22e9c20f4341b2b01ce100402a26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38f09c7595bc47fda2c7d28e86ead41e",
      "placeholder": "​",
      "style": "IPY_MODEL_fa0c94d94120476c88558cca813471e1",
      "value": " 4050/4050 [01:02&lt;00:00, 69.76it/s, loss=0.53, dev-loss=0.58, acc=0.69, macro-f1=0.68, lr=[0.001]]"
     }
    },
    "7876d80bd79840dbbcc9ef253fab033e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78dfeafa6add43b1a16ab133a4a7b40c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ce22a61ac704525bb71c298215e312a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6590dd89ae14cd38c95dda633f3f751",
      "placeholder": "​",
      "style": "IPY_MODEL_4c4f7c52ec8e45cb8d8bd76b5a6c4ba6",
      "value": "100%"
     }
    },
    "7e29156188fa45b48c6ea211ed020bdf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ec4c2e715384876b8af721d0ee2fd0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_569e51ad289e4bdd9c62ec1bf01e55e3",
      "placeholder": "​",
      "style": "IPY_MODEL_ceedbd7fe2414448b6e1d8a6103d9b73",
      "value": " 72.8k/72.8k [00:01&lt;00:00, 39.4kB/s]"
     }
    },
    "8215e53545144610a7b47dee0bef4efe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86481c6d8c3d4a71a33bf1003898e325": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87ac978cd93646a6b9f7d9b2dd4bd780": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8934a97087f14ced87c29a22b4a4dc47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f07d3c59d914acba1e5e34876a11b27",
      "max": 72813,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1acb8f65fe8b44cca3ced96a5dc71022",
      "value": 72813
     }
    },
    "896c0b7b40114192b477bb42230e64d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45577013b04141459b6c1ae17f292272",
       "IPY_MODEL_23a0ce5810ba4e8fab36f8ebbfa9ba88",
       "IPY_MODEL_b245108bc9044fe69c4d52e7ca06f32e"
      ],
      "layout": "IPY_MODEL_87ac978cd93646a6b9f7d9b2dd4bd780"
     }
    },
    "8aaab8358631404f89dc122392f25961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e45d64fada942d2b8e663611966e4ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "92138554756a4e4aaca96ca63d9bcd53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9415220f7ef149a29c055e28bdd96cb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "941c2990be56492db884fb7a9387cd33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ec73b20c9854dca937e897488526266",
      "max": 5274,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ef6945387944042beddcfdf3ddb30cc",
      "value": 5274
     }
    },
    "9a9debbf4ff942eea090d6da62f947dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d67b47473f44031bc346ae79945fc9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d913998243c4dde9191ba0fdfe51de9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9ddb631757cb46abb9ae60b2d0496763": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e260f9a0edd4037a37ba1814b514b22",
      "placeholder": "​",
      "style": "IPY_MODEL_2c4b98f8d27e425f8ce8893ef200f474",
      "value": "100%"
     }
    },
    "a4bf7edfaa294755bf464692c7d2e81c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6590dd89ae14cd38c95dda633f3f751": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9481737c3034f1fa5d7830d4cb2ffcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afca4460e64c458c8e23e9560f4ff5a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b15a8927735f47c697452cda15ab94b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a201baa0e2b4b729279b895d5f60f7c",
      "placeholder": "​",
      "style": "IPY_MODEL_7e29156188fa45b48c6ea211ed020bdf",
      "value": "Generating test split: 100%"
     }
    },
    "b245108bc9044fe69c4d52e7ca06f32e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a9debbf4ff942eea090d6da62f947dd",
      "placeholder": "​",
      "style": "IPY_MODEL_a4bf7edfaa294755bf464692c7d2e81c",
      "value": " 148k/148k [00:02&lt;00:00, 69.9kB/s]"
     }
    },
    "b3a6eed41a8e406fa8e6a5f99b08e5b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa841a66f4964c0d9eb478d5a4c6dc62",
      "max": 872,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e915b85cd18c418baa67c00d58f94312",
      "value": 872
     }
    },
    "b534b24b46d345c996f161aa9310560c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdbc928e77bd4fc4a20c6e3fcccc0a83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf4b0f6bb6da46558c7f286f846ded71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c38050de05bc446ea5158b258951cf53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5d0c2f1ee104a67aff548f187e9d32c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf4b0f6bb6da46558c7f286f846ded71",
      "placeholder": "​",
      "style": "IPY_MODEL_db813166d20f44e5a1a0a68242fac55a",
      "value": "Generating validation split: 100%"
     }
    },
    "cb2eb07c36f0427587ee3acab68cd2d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5d0c2f1ee104a67aff548f187e9d32c",
       "IPY_MODEL_b3a6eed41a8e406fa8e6a5f99b08e5b6",
       "IPY_MODEL_dfa3c57253c14ba4b8ccc8de7f021664"
      ],
      "layout": "IPY_MODEL_36570ba587704a9291bddd3cf05c64d4"
     }
    },
    "ceedbd7fe2414448b6e1d8a6103d9b73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d2178c0ca13141c5b9b05937ac937f56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3287a0d936f4238aef4bfa917c51fd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d69d788c9e284632b0c9e543aafaabca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6fd49cdcd934907ae4560a3877182b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d908a484a320473993fa9fc21ddc30a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ade4dc65b4b48fea6e360f2f6845cc9",
      "max": 20250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2178c0ca13141c5b9b05937ac937f56",
      "value": 20250
     }
    },
    "d9ebc454abb2415a82496207cca65841": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8aaab8358631404f89dc122392f25961",
      "max": 3110458,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d913998243c4dde9191ba0fdfe51de9",
      "value": 3110458
     }
    },
    "db813166d20f44e5a1a0a68242fac55a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcf6d6a3b6a141e0937ee1270b06d685": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_009d5f4cfecf46259cdf4d5b7d4f739d",
      "placeholder": "​",
      "style": "IPY_MODEL_0b99fc6e4d024b3fa04b3afcf6f7331a",
      "value": "Downloading readme: 100%"
     }
    },
    "df577e05dd604729b9d8ccbf7491f9ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a12047d2fea4c309024402d89ad567a",
      "placeholder": "​",
      "style": "IPY_MODEL_2eaf26210d874c78a0154c4025d4c455",
      "value": " 67349/67349 [00:00&lt;00:00, 703626.66 examples/s]"
     }
    },
    "dfa3c57253c14ba4b8ccc8de7f021664": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92138554756a4e4aaca96ca63d9bcd53",
      "placeholder": "​",
      "style": "IPY_MODEL_9d67b47473f44031bc346ae79945fc9b",
      "value": " 872/872 [00:00&lt;00:00, 34621.34 examples/s]"
     }
    },
    "e118f8af040248a2806c2f8de5a402eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b15a8927735f47c697452cda15ab94b7",
       "IPY_MODEL_703e6392ac914844b438fe14de6692c7",
       "IPY_MODEL_2a4adfb814954963b42af8ff8c0c3721"
      ],
      "layout": "IPY_MODEL_a9481737c3034f1fa5d7830d4cb2ffcf"
     }
    },
    "e499064ed939412b95e0da8c4ffbe160": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8eb1176d31e41e2990016ce54795fba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72ef065c0ecc438ea7fe5bc7c8a02a94",
       "IPY_MODEL_d9ebc454abb2415a82496207cca65841",
       "IPY_MODEL_6f87737ac4f44a9ba999cdf7c76a253f"
      ],
      "layout": "IPY_MODEL_2be560f8ee8c45869ebd50e2ab1ba37d"
     }
    },
    "e915b85cd18c418baa67c00d58f94312": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e9c73b9f14ea433bbac1dc9fa7433c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78dfeafa6add43b1a16ab133a4a7b40c",
      "max": 4050,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01db01ae82b5421a83145b13db2bcb03",
      "value": 4050
     }
    },
    "ed81e30d6f9a47a38509143b3aa93a9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d69d788c9e284632b0c9e543aafaabca",
      "placeholder": "​",
      "style": "IPY_MODEL_2a1ef97827a242f6a0879a57c94334d3",
      "value": " 5.27k/5.27k [00:00&lt;00:00, 261kB/s]"
     }
    },
    "efcc7ef8af6346f1bb625eb808a587fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f470afaedaf94d9bbfc28a106ca1ea97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5abf82fa4c9542748a0429250068e652",
      "placeholder": "​",
      "style": "IPY_MODEL_e499064ed939412b95e0da8c4ffbe160",
      "value": " 20250/20250 [06:11&lt;00:00, 58.98it/s, loss=0.46, dev-loss=0.53, acc=0.80, macro-f1=0.80, lr=[0.001]]"
     }
    },
    "f844c26592da46e5b16ddf82ef5c7184": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa0c94d94120476c88558cca813471e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa841a66f4964c0d9eb478d5a4c6dc62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
