{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/Rank_frequency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4Nc7q8vdZsS"
   },
   "source": [
    "\n",
    "# ILOs\n",
    "\n",
    "After completing this tutorial the student \n",
    "\n",
    "* can analyse rank frequency and fit a Zipf/Zeta distribution to data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlWB2MZDpeHz"
   },
   "source": [
    "\n",
    "# Table of contents\n",
    "\n",
    "\n",
    "* [Rank-Frequency](#rankfreq)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2grb4yOTpfx1"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5n0ZNs_ptVt"
   },
   "source": [
    "## Packages\n",
    "\n",
    "Everything can be installed with pip, just run in a cell `!pip install numpy`, for example.\n",
    "Some tools might require restarting the notebook's kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_T_JXwfYdZsV"
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install nltk\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pb7ZScT7dZsW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.stats as st\n",
    "import urllib  # sometimes we need to download stuff\n",
    "import gzip    # sometimes the stuff we downloaded is gzipepd\n",
    "import json    # sometimes we download dictionaries stored in json format\n",
    "import pandas as pd    # great for organising tabular data\n",
    "import seaborn as sns  # lots of fancy plotting functions coded for us\n",
    "import nltk\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IisU9zHCdZsf"
   },
   "source": [
    "## <a name=\"nltk\">  NLTK\n",
    "\n",
    "[NLTK](https://www.nltk.org) is a platform for building Python programs to work with human language data. It provides access to corpora and other linguistic resources, as well as a simple interface for developing NLP applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR-lpmJQdZsf"
   },
   "source": [
    "Before you start programming make sure you have installed all necessary packages. You can install packages directly from your jupyter notebook using the command `!pip install <package>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NQFzcoGdZsf"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89HUw40ydZsf"
   },
   "source": [
    "The first time you use nltk, you will have to download some packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mNShrIodZsg"
   },
   "outputs": [],
   "source": [
    "## These are the packages needed for this tutorial:\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('treebank')\n",
    "nltk.download('alpino')\n",
    "nltk.download('floresta')\n",
    "\n",
    "\n",
    "## If you are running this locally, you can also install 'all', \n",
    "##  but it will take a moment though (hence, we don't recommend downloading 'all' on colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNEgljGAdZsg"
   },
   "source": [
    "<details>\n",
    "    <summary> Some people reported an error on macOS <i>SSL: Certificate verify failed</i>, if it happens to you, you can use the following \n",
    "    </summary>\n",
    "    \n",
    "```python\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('all')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8Nbu18KdZsv"
   },
   "source": [
    "## <a name=\"rankfreq\"> Rank-Frequency\n",
    "\n",
    "    \n",
    "***If you choose not to work through this section in week 1, it's still advisable to study the section at a later moment: its content is useful and relevant to this course.***\n",
    "    \n",
    "Natural languages are remarkably productive, an human speakers are very creative. Day after day, the vocabulary of every natural language actively spoken on the planet is continuously changing. New words are created, existing words are reused in novel ways, some words lose their prominence.\n",
    "\n",
    "In most NLP applications the vocabulary of a language is frozen. We consider a \"vocabulary\" the set of all known types at a given time. Here we use the word *type* to distinguish, for example, the unique token `the` from its many occurrences in a corpus, which we usually call *instances*).\n",
    "\n",
    "Entries in a vocabulary are generally referred to as *words*, but in NLP they really are *tokens*, where a *token* is whatever sequence of characters that we treat as a unit (typically a sequence of non-blank characters). For example, linguistically speaking `Oct.` is not a word (it's an abbreviated form of the word `October`), but it may well be a token in our NLP system's vocabulary. Conversely, `camera-ready` is a word in English, but any one occurrence of `camera-ready` may be split into one or more tokens depending on our tokenization strategy (e.g., `camera`, `-`, `ready`). Moreover, while linguistically `Oct.` really is an instance of the word `October`, unless we are explicit about it, a computer cannot tell that. NLP systems won't be able to infer the relationship between these two strings, `Oct.` and `October`, unless we give them the means to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTStN56ydZsv"
   },
   "source": [
    "We will now begin to appreciate one of the most important aspects of written language: *data sparsity*. Data sparsity affects many aspects of NLP systems, and a system's vocabulary is probably the best example.\n",
    "\n",
    "You are probably aware of [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law), an empirical finding that the frequency of any word is inversely proportional to its rank in the frequency table. We will now verify this finding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkVk2GRUdZsv"
   },
   "source": [
    "**Data** NLTK also provides access to corpora. \n",
    "\n",
    "You can check the documentation of the [corpus package](https://www.nltk.org/api/nltk.corpus.html#module-nltk.corpus) online or on your own jupyter notebook using `nltk.corpus?`. Here is a [list of available corpora](http://www.nltk.org/nltk_data/).\n",
    "\n",
    "Corpora in NLTK are mostly already pre-processed at the basic levels (e.g., sentence splitting, and tokenization). \n",
    "\n",
    "Let's have a look at sample from the English PeenTreebank (again, a section of the WSJ corpus). If you are a Dutch speaker, you can also check Alpino (Dutch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trKBqMhTdZsv"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import floresta as pt_floresta\n",
    "from nltk.corpus import alpino as nl_alpino\n",
    "from nltk.corpus import treebank as en_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ws1tlA9vdZsv"
   },
   "outputs": [],
   "source": [
    "len(en_ptb.words()), len(nl_alpino.words()), len(pt_floresta.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_pPHAHtdZsw"
   },
   "source": [
    "To work with rank and frequency we need to determine the number of occurrences of each token in the vocabulary of a given corpus. In python, a `Counter` (from `collections`) can help us achieve that (if you are not familiar with `Counter` but know `dict`, they are very similar, check the python docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7POjkansdZsw"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC9R2OWldZsw"
   },
   "outputs": [],
   "source": [
    "counter = Counter(en_ptb.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teiD9nuxdZsw"
   },
   "source": [
    "The counter stores a dictionary where each key is an observed token and its value is the number of times it occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isdC5ZNhdZsw"
   },
   "outputs": [],
   "source": [
    "# Note that we can use the counter as a dictionary that maps from a token to its count:\n",
    "'day' in counter.keys(), counter['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhn8hmsXdZsw"
   },
   "outputs": [],
   "source": [
    "'NTMI' in counter.keys()  # it looks like our course has not been mentioned in the Penn Treebank yet ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47N-NF6cdZsw"
   },
   "source": [
    "Counters can sort the vocabulary for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGoXyrU7dZsw"
   },
   "outputs": [],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUFF56-VdZsw"
   },
   "source": [
    "Unfortunately, computers do not know that tokens like 'Day' and 'day' refer to the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21UtHI-ldZsw"
   },
   "outputs": [],
   "source": [
    "# Note that for a computer 'Day' and 'day' are different tokens.\n",
    "'Day' in counter.keys(), counter['Day']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiKvrYLZdZsw"
   },
   "source": [
    "**Exercise with solution** One way to deal with that is to lowercase the data as a pre-processing step. This has downsides, can you think of some?\n",
    "\n",
    "<details>\n",
    "    <summary> <b>Click for a solution</b>  </summary>\n",
    "\n",
    "Not every language uses lower/upper case characters (e.g., Chinese, Japanese, Arabic).\n",
    "\n",
    "---\n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk-RX_MkdZsx"
   },
   "source": [
    "This is not the only issue contributing to sparse vocabularies. Morhopological inflection does that too, for example, singular vs plural, gender marking, syntactic case, all these linguistic devices contribute to data sparsity, and in some applications we might want to treat all instances of `day`, `Day`, `days`, and `Days` as if they referred to the same type (the English word `DAY`).\n",
    "\n",
    "One relatively simple way to reduce the vocabulary size by collapsing different variants of a certain base form is to use a [stemmer](https://en.wikipedia.org/wiki/Stemming). NLTK provides options for a few languages including [English, Dutch, and Portuguese](https://www.nltk.org/api/nltk.stem.html). \n",
    "\n",
    "Here is an example of what stemmers do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV8DX9IydZsx"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer, DutchStemmer, PortugueseStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDKwikj3dZsx"
   },
   "outputs": [],
   "source": [
    "en_stemmer = EnglishStemmer()\n",
    "nl_stemmer = DutchStemmer()\n",
    "pt_stemmer = PortugueseStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jU7gBZj2dZsx",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "for i, s in zip(range(3), en_ptb.sents()):\n",
    "    rows = []\n",
    "    rows.extend([(w, en_stemmer.stem(w)) for w in s])\n",
    "    print(tabulate(rows, headers=['word', 'stem']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GalGalhdZsx"
   },
   "source": [
    "Here we use a loglog plot to verify Zipf's law (i.e., if you plot the log of the rank vs the log of the frequency, you should see something close to a straight line). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYme5m8adZsx"
   },
   "outputs": [],
   "source": [
    "def get_ranks(words):\n",
    "    \"\"\"Map a list of words to a np.array of ranks, where the most frequent word is assigned rank 1\"\"\"\n",
    "    counter = Counter(words)\n",
    "    w2r = {word: rank for rank, (word, count) in enumerate(counter.most_common(), 1)}\n",
    "    return np.array([w2r[w] for w in words])\n",
    "\n",
    "def get_rankfreq_pairs(words):\n",
    "    \"\"\"\n",
    "    Map a list of words to an np.array with shape [K, 2] where K is the number of distinct tokens in the input list.\n",
    "    The first column of the array is the rank, the second column of the array is the count.\n",
    "    \"\"\"\n",
    "    counter = Counter(words)\n",
    "    # rank-frequency\n",
    "    rf = np.array([[r, n] for r, (w, n) in enumerate(counter.most_common(), 1)])\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-log plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0558gnxdZsx"
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "palette = cycle(sns.color_palette())\n",
    "\n",
    "for corpus_name, corpus, stemmer in zip(['en_ptb', 'nl_alpino', 'pt_floresta'], [en_ptb, nl_alpino, pt_floresta], [en_stemmer, nl_stemmer, pt_stemmer]):\n",
    "\n",
    "    words = corpus.words()\n",
    "    rf = get_rankfreq_pairs((stemmer.stem(w) for w in words)) # I'll be lowercasing the words, since it makes sense for these languages   \n",
    "    c = next(palette)\n",
    "    _ = plt.loglog(rf[:,0], rf[:,1], '-', c=c, label=corpus_name)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YhI-cJLdZsx"
   },
   "source": [
    "The general tendency indeed looks like a straight line, which in a log-log plot denotes an exponential relationship between rank and frequency, as the line has a negative angle (with the x-axis) we can conclude that frequency decays exponentially fast with an increase in rank. That's the second most frequent word is exponentially less frequent than the first, and so on. \n",
    "\n",
    "The lines are not very straight at the extremes (lowest and highest ranks). The lowest ranks are probably distorted because of the presence of stop words. As for the highest rank, the information is probably distorted because of dataset size. Generally, it does look like Zipf's law is indeed a robust finding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NOxlPrfIlYz"
   },
   "source": [
    "### Zipf and Zeta \n",
    "\n",
    "We will now design a statistical model of the rank. For that, we can use the [Zipf distribution](https://en.wikipedia.org/wiki/Zipf's_law) which predicts  that out of a population of $N > 0$ elements, the probability of the element of rank $k \\ge 1$ is \n",
    "\\begin{align}\n",
    "\\mathrm{Zipf}(k|N, s) = \\frac{\\frac{1}{k^s}}{H_{N,s}}\n",
    "\\end{align}\n",
    "where the normalisation terms is defined as $H_{N,s} = \\sum_{n=1}^N \\frac{1}{k^s}$, and $s>1$ is called the *power parameter*.\n",
    "\n",
    "The Zipf distribution requires a fixed number of draws $N$ and thus supports ranks in $\\{1, \\ldots, N\\}$.\n",
    "\n",
    "A slightly more convenient option is the [Zeta distribution](https://en.wikipedia.org/wiki/Zeta_distribution) which generalises the Zipf distribution removing the need to specify the total number of elements in the population. This is convenient when we are analysing populations (corpora) of different size.\n",
    "\n",
    "The Zeta distribution assigns probability \n",
    "\\begin{align}\n",
    "\\mathrm{Zeta}(k|s) = \\frac{\\frac{1}{k^s}}{\\zeta(s)}\n",
    "\\end{align}\n",
    "to rank $X=k$. The power parameter is $s>1$ as before, and $\\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s}$ is the [Riemann zeta function](https://en.wikipedia.org/wiki/Riemann_zeta_function) which is implemented in `scipy.special.zeta`. The support of the Zeta distribution is $\\mathbb N_1 = \\{1, 2, \\ldots\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Najy4AuTdZsx"
   },
   "source": [
    "**Exercise with solution - Grid search MLE for Zeta** \n",
    "    \n",
    "Perform a grid search to estimate the power parameter of the Zeta distribution for each dataset. Note that power parameters must be greater than 1. A reasonable range of parameters to test is something like `[1.01, 3.0]`. Obtain samples from the MLE Zeta and plot them against the observations (use as many samples as you have observations). Comment on the fit: do you think the model fits the data well, does it fit the data well for most rank values or are there types of values (eg, very low, or very high) for which the model does not do so well?\n",
    "\n",
    "As scipy has a stable implementation of the Riemann zeta function `scipy.special.zeta`, we could implement the pmf of the Zeta distribution yourselves. Generally, however, it is always a good idea to reuse high quality mathematical code. It turns out, the statistics package in scipy has a stable implementation of the Zeta distribution, but, funnily enough, it is named `scipy.stats.zipf`, instead of `scipy.stats.zeta`. For some historical accident, the two terms 'Zipf distribution' and 'Zeta distribution' came to be used somewhat interchangeably in statistics. In doubt, see that if it takes two parameters (total population size $N > 1$ and power $s > 1$) we have the classic Zipf, if it takes one parameter (just the power $s>1$), we have Zipf's generalisation called Zeta.\n",
    "\n",
    "\n",
    "In any case, we will go on with the *Zeta* distribution, you can count on its good implementation from scipy which is called `scipy.stats.zipf`.\n",
    "\n",
    "\n",
    "<details>\n",
    " <summary> Hint </summary>\n",
    "\n",
    "Check the grid search we did for the Poisson case, we first implemented the Poisson likelihood function, and then implemented the search. The strategy here is very similar, but watch out that you use the correct pmf.\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    " <summary> You do not need to use this trick, but you may find it useful. </summary>\n",
    "\n",
    "Check the grid search we did for the Poisson case, we first implemented the Poisson likelihood function, and then implemented the search. The strategy here is very similar, but watch out that you use the correct pmf.\n",
    "\n",
    "For discrete data, sometimes we store the *counts* of the outcomes rather than the outcomes themselves, that is, we store a vector $\\mathbf c$ where $0 \\le c_k \\le N$ is the number of times outcome $k$ occurs in $\\mathcal D$. We can re-express the log-likelihood function in terms of counts:\n",
    "\n",
    "\\begin{align} \n",
    "\\mathcal L_{\\mathcal{D}}(\\theta) &= \\sum_{n=1}^N \\log f_\\theta(x_n) = \\sum_{k \\in \\mathrm{supp}(P_X)} c_k \\log f_\\theta(k) ~,\n",
    "\\end{align}\n",
    "\n",
    "where in practice we only evaluate the terms for which $c_k > 0$ in the dataset.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJeE67NmdZsy"
   },
   "outputs": [],
   "source": [
    "def zeta_log_likelihood(power, rank, freq):\n",
    "    \"\"\"\n",
    "    This function should return a single real value representing the log-likelihood     \n",
    "     \\sum_{r=1}^R count(r) * log Zeta(rank=r|power)\n",
    "    where count(r) is the number of times the rank r occurs in the dataset.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    " <summary> Click for a solution </summary>\n",
    "\n",
    "```python\n",
    "def zeta_log_likelihood(power, rank, freq=None):\n",
    "    pmf = st.zipf(power)   # scipy calls it Zipf, but it is the single parameter version (thus, Zeta) \n",
    "    ll = pmf.logpmf(rank)\n",
    "    if freq is not None and len(freq) == len(rank):\n",
    "         ll *= freq\n",
    "    return ll.sum()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZQNi3cgdZsy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "palette = cycle(sns.color_palette())\n",
    "\n",
    "for corpus_name, corpus, stemmer in zip(['en_ptb', 'nl_alpino', 'pt_floresta'], [en_ptb, nl_alpino, pt_floresta], [en_stemmer, nl_stemmer, pt_stemmer]):\n",
    "    # get all words\n",
    "    words = corpus.words()\n",
    "    # obtain their rank frequencies\n",
    "    rf = get_rankfreq_pairs((stemmer.stem(w) for w in words))\n",
    "    \n",
    "    # plot the data\n",
    "    c = next(palette)\n",
    "    _ = plt.loglog(rf[:,0], rf[:,1], '-', c=c)\n",
    "    \n",
    "    # make a grid\n",
    "    grid = np.linspace(1.01, 3., 300)\n",
    "    # evaluate log-likeilhood for each parameter value in the grid\n",
    "    lls = np.array([zeta_log_likelihood(power, rf[:,0], rf[:,1]) for power in grid])\n",
    "    # find the id of the parameter that maximises the log-likelihood in the grid\n",
    "    k = np.argmax(lls)\n",
    "    # this is the parameter that attains the maximum (in the grid)\n",
    "    mle = grid[k]\n",
    "\n",
    "    # plot samples from the corresponding Zeta distribution\n",
    "    N = len(words)\n",
    "    x_ = st.zipf(mle).rvs(N) # scipy calls it Zipf, but it's the single parameter version (i.e., Zeta)\n",
    "    rf_ = get_rankfreq_pairs(x_)\n",
    "    _ = plt.loglog(rf_[:,0], rf_[:,1], '--', c=c, label=f\"X ~ Zeta({mle:.4f}): {corpus_name}\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RXB5f3ykclx"
   },
   "source": [
    "The models are fairly decent approximation of the data, except towards the extremes (lowest and highest ranks). But, generally, it does look like the data could follow a Zeta distribution."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2022/T1-teacher.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
