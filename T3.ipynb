{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open this notebook on Colab](https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLeLl2YVu4vv"
   },
   "source": [
    "# Guide\n",
    "\n",
    "* Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
    "* Note that, as always, the notebook recaps some theory, and contains solved exercises. While you should probably make use of this theory recap, be careful not to spend disproportionately more time on this than you should. The theory here is more condensed, and should be easier to understand after week 3's reading and after the highlights discussed in class (HC3a).\n",
    "* There are various excerpts of code that should be studied so that you can adapt them in graded exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APh_4TfPRQ13"
   },
   "source": [
    "## ILOs\n",
    "\n",
    "After completing this lab you should be able to \n",
    "\n",
    "* develop generalised linear models of text classification and text regression in Python using sklearn and jax\n",
    "* estimate parameters via gradient-based optimisation\n",
    "\n",
    "**General notes**\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SasxJwnHRrUq",
    "tags": [
     "toc"
    ]
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Topics \n",
    "\n",
    "* [Setting up](#sec:Setting_up)\n",
    "* [Generalised Linear Models (GLMs)](#sec:Generalised_Linear_Models_(GLMs))\n",
    "* [Parameter estimation](#sec:Parameter_estimation)\n",
    "* [Implementation](#sec:Implementation)\n",
    "\t* [Binary classifier](#sec:Binary_classifier)\n",
    "\t* [Baseline: NBC on Features](#sec:Baseline:_NBC_on_Features)\n",
    "\t* [Bernoulli GLM on Jax](#sec:Bernoulli_GLM_on_Jax)\n",
    "* [Python/JAX GLM Class](#sec:Python/JAX_GLM_Class)\n",
    "\t* [Binary classification experiment](#sec:Binary_classification_experiment)\n",
    "\t* [Ordinal regression experiment](#sec:Ordinal_regression_experiment)\n",
    "\n",
    "\n",
    "### Table of ungraded exercises\n",
    "\n",
    "1. [Space and time complexity of the linear model](#ungraded-1)\n",
    "1. [Analysing the loss for the Bernoulli GLM](#ungraded-2)\n",
    "1. [GLM class](#ungraded-3)\n",
    "1. [Code for parameter estimation](#ungraded-4)\n",
    "1. [Change the feature function](#ungraded-5)\n",
    "1. [Further analysis](#ungraded-6)\n",
    "\n",
    "\n",
    "### Table of graded exercises\n",
    "\n",
    "*Important:* The grader may re-run your notebook to investigate its correctness, but you must upload your notebook with the cells already run and make sure that all your answers are visible without the need to re-run the notebook.\n",
    "\n",
    "\n",
    "Exercises have equal weights.\n",
    "\n",
    "\n",
    "1. [Subjectivity classifier](#graded-1)\n",
    "1. [Poisson regression](#graded-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqxvaCboSFNN",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Setting_up'></a>\n",
    "# Setting up\n",
    "\n",
    "Make sure you have colab configure to use 4 spaces for TAB (Tools/Settings/Editor/Indentation). If you change your Runtime to GPU (Runtime/Change runtime type) model training will be faster, but for this tutorial a GPU is not strictly necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SuKXLEzu9xJ"
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install jax\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGS20mlRvCvW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set(font_scale=1.3)\n",
    "\n",
    "import jax\n",
    "from jax import device_put\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, mean_absolute_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('sentence_polarity')\n",
    "nltk.download('subjectivity')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfOTsJZdxEX-",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Generalised_Linear_Models_(GLMs)'></a>\n",
    "# Generalised Linear Models (GLMs)\n",
    "\n",
    "GLMs are used to prescribe conditional distributions of the kind $P_{Y|X=x}$. In our application domain, $x \\in \\mathcal X$ is typically one or more pieces of text, and $y \\in \\mathcal Y$ can be nominal or numerical. \n",
    "    \n",
    "In a nutshell, a GLM maps $x$ to the parameter of our choice of pmf (or pdf) for the distribution $P_{Y|X=x}$. It does so via a parametric function $g(x; \\boldsymbol\\theta)$ with trainable parameters $\\boldsymbol\\theta$. This function involves:\n",
    "* a predefined feature function $\\mathbf h(x)$, which maps $x$ to a $D$-dimensional vector of numerical features;\n",
    "* a linear transformation of this vector (here is where the trainable parameters $\\boldsymbol \\theta$ play a role), whose result we refer to as the linear predictor;\n",
    "* and, depending on the pmf/pdf we choose, a non-linear activation function that constrains the so-called linear predictor to the space of values that is coherent with our choice of pmf/pdf.\n",
    "    \n",
    "    \n",
    "To build GLMs:\n",
    "    \n",
    "1. Start with choosing  the family you will model with. This depends on the type of data you have (e.g., binary means Bernoulli, $K$-way classification usually means Categorical, ordinal regression usually means Poisson, but it could be some other distribution over natural numbers or a subset thereof, like the Binomial, continuous regression might require a Normal distribution, regressing to vectors of proportions might require a Dirichlet distribution, etc.). You don't need to know all these distributions by heart, when needed, we will give you information about them that will help you judge their relevance in context.\n",
    "\n",
    "2. The input is text, but GLMs operate with inputs in the real coordinate space, so you need a vector-valued feature function $\\mathbf h(x)$. Define your feature function. Common choices are based on ideas such as bag-of-words and tf-idf.\n",
    "\n",
    "3. In a GLM, the input $\\mathbf h(x)$ and the parameters $\\boldsymbol\\theta$ interact linearly. This constrains us to either operations like dot product, matrix multiplication and scalar or vector addition. Whether we have \"dot product plus scalar\" or \"matrix multiplication plus vector\" depends exclusively on the dimensionality we need for the linear predictor. If we need a single scalar, we wil use the former. If we need a vector, we will use the latter.\n",
    "\n",
    "4. Example 1: the Poisson parameter is a single scalar, thus we know that we need to map from $\\mathbf h(x)$ to a single scalar, as we achieve with \"dot product plus scalar\". Example 2: the Categorical parameter is a vector, thus we know that we need to map from $\\mathbf h(x)$ to a $K$-dimensional vector, as we achieve with \"matrix multiplication plus vector\". Example 3: the Normal distribution has 2 parameters, a location and a scale, thus we use two linear transformations of the kind 'dot product plus scalar', one for each parameter.\n",
    "\n",
    "5. Finally, the statistical parameter is generally constrained to a subset of the real numbers, so we need an activation function that constrains the linear predictor accordingly. Example 1: the Poisson parameter is *strictly positive* by definition, so we need to wrap the linear function around something whose output is *never negative* and *never 0*, no matter which real-valued input we give it. The exponential function does that for us. There are other activation functions that achieve the same result, but the exponential is convenient for certain reasons (e.g., it's logarithm is linear). In our implementation below we will see other options. Example 2: the Categorical parameter must be a probability vector, the softmax function can realise that constraint for us. Example 3: the Normal location is unconstrained, we don't need an activation for it (or, equivalently, we use the 'identity function' which simply returns the linear predictor without any change), the scale however is constrained to being strictly positive, for that we can for example exponentiate the linear predictor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtvPFOeI8lpl",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Parameter_estimation'></a>\n",
    "# Parameter estimation\n",
    "\n",
    "Given a training set $\\mathcal D = \\{(x^{(n)}, y^{(n)})\\}_{n=1}^N$ of $N$ observed input-target pairs we would ideally choose the parameter vector $\\boldsymbol\\theta$ that maximises the log-likelihood of the model. Let $f(y; \\phi=g(x;\\boldsymbol\\theta))$ denote the pmf (or pdf) that prescribes the cpd $P_{Y|X=x}$, then:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) &= \\sum_{n=1}^N \\log f(y^{(n)}; \\phi_n) \\quad \\text{where } \\phi_n = g(x^{(n)}; \\boldsymbol\\theta)\\\\\n",
    "\\boldsymbol\\theta^\\star &= \\arg\\max_{\\boldsymbol\\theta}~\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) \n",
    "\\end{align}\n",
    "\n",
    "Unlike tabular CPDs, there is no simple expression for the MLE of a GLM. But, we can employ a gradient-based search. This search uses the gradient $\\nabla_{\\boldsymbol \\theta}\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta)$ to iteratively update an existing $\\boldsymbol \\theta$, starting from an initial guess $\\boldsymbol \\theta^{(0)}$, which is typically a random initialisation of the parameters.\n",
    "\n",
    "At iteration $t$, the update rule is\n",
    "\\begin{equation}\n",
    "\\boldsymbol \\theta^{(t+1)} = \\boldsymbol \\theta^{(t)}  + \\gamma_t \\nabla_{\\boldsymbol \\theta^{(t)}}\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta^{(t)})\n",
    "\\end{equation}\n",
    "where the log-likelihood is assessed using the current parameters, we obtain the gradient for it, and combine it with the current parameters to get the next iterate. The quantity $\\gamma_t > 0$ is called a *learning rate*.\n",
    "\n",
    "<details>\n",
    "    <summary><b>Maximisation vs minimisation</b></summary>\n",
    "\n",
    "If you have seen this formula before with a *minus* rather than a *plus* for the gradient, don't worry, it is the same notion. You *sum* the gradient if you are maximising the log-likelihood, and you *subtract* the gradient if you are minimising the negative log-likelihood. The two procedures yield the exact same optimum.\n",
    "\n",
    "---    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-y2IiYGU-NaW"
   },
   "source": [
    "Assessing the log-likelihood of a certain value of the parameter vector $\\boldsymbol\\theta$ requires assessing the probability mass (or density, for continuous variables) of each one of our observations under the current value of the parameter. Each one such assessment on its own is not at all challenging, but assessing all the $N$ terms can be challenging for large $N$.\n",
    "\n",
    "Fortunately, we can use a stochastic gradient procedure, which still has the same guarantees as the deterministic procedure.\n",
    "\n",
    "At each iteration $t$, we compute an approximation to the gradient using $S < N$ data points uniformly sampled from $\\mathcal D$.\n",
    "\n",
    "We can obtain this gradient by essentially pretending, at each iteration $t$, that the log-likelihood function depends only on a small *batch* of $S$ observations drawn from the training set:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal L_{\\mathcal B}(\\boldsymbol\\theta^{(t)}) = \\frac{1}{S}\\sum_{s=1}^S \\log f(y^{(s)}; g(x^{(s)}; \\boldsymbol \\theta^{(t)}))\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QrGXlj-Bj4Y"
   },
   "source": [
    "Oftentimes, we have **many** features, and thus **many parameters**. This gives models the capacity to discover correlations that have no real predictive power (e.g., that [capivaras](https://en.wikipedia.org/wiki/Capybara) implies a *negative* sentiment, simply because the 1 time that word was seen in the data was in a document labelled with the negative class, for example the document might have been \"I loved the capivaras but overall the horrible weather ruined the trip\"). \n",
    "\n",
    "These are called **spurious correlations** and we would rather not be mislead by them. \n",
    "\n",
    "In an attempt to get rid of them, we employ a so called **regulariser**. This is a penalty on the objective based on the norm of the parameter vector, and usually employ the L2 norm.\n",
    "\n",
    "The L2 norm of a vector is:\n",
    "\\begin{equation}\n",
    "\\mathcal R(\\mathbf v) = \\sqrt{\\sum_{d=1}^D v_d^2}\n",
    "\\end{equation}\n",
    "For a collection of parameter vectors we sum the L2 norms of each vector. \n",
    "\n",
    "Thus, for each iteration $t$, this is the objective we optimise:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal L_{\\mathcal B}(\\boldsymbol\\theta^{(t)}) - \\lambda \\mathcal R(\\boldsymbol\\theta^{(t)})\n",
    "\\end{equation}\n",
    "where $\\lambda \\ge 0$ is a hyperparameter used to control the importance of the regulariser. There's no way to choose the value of $\\lambda$ directly from the training data, we have to try various values and test the model's performance on heldout data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Implementation'></a>\n",
    "# Implementation\n",
    "    \n",
    "We will implement GLMs using JAX, a software package for differentiable programming. We use a *binary text classifier* as a running example to show you how to develop the key GLM operations in JAX, including an algorithm for parameter estimation.\n",
    "    \n",
    "Study the code well. \n",
    "    \n",
    "After that, we pack everything nicely in a class for you so that you can create GLMs for different types of data (e.g., binary, ordinal) and perform experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7IUWL0aln_b",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Binary_classifier'></a>\n",
    "## Binary classifier\n",
    "\n",
    "Here we use the `subjectivity` corpus from NLTK to train binary classifiers. We will train a Naive Bayes classifier using scikitlearn and a Bernoulli GLM which we will develop in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u09CRt2Hu4vz"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import subjectivity  # binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5KbhWdhu4v0"
   },
   "outputs": [],
   "source": [
    "labels = subjectivity.categories()\n",
    "C = len(labels)\n",
    "print(\"{}-way classification:\\n{}\".format(C, '\\n'.join(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnMGDKpPu4v1"
   },
   "source": [
    "As usual, we will split our observations in three disjoint sets, 80% for training, 10% for whatever development purposes we have, and 10% for testing the generalisation of our classifier at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8qvqgPVu4v1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_nltk_corpus(nltk_corpus, categories, seed=23, BOS='<s>', EOS='</s>'):\n",
    "    \"\"\"\n",
    "    Prepare an nltk text categorization corpus in a sklearn friendly format.\n",
    "    \n",
    "    This function is very similar to what you saw in T2, but here we add BOS tokens in addition to EOS tokens \n",
    "    (while the BOS token has no effect in NBC with unigram conditionals, \n",
    "    it can be useful for some of the feature-richer classifiers we will develop here).\n",
    "    \n",
    "    :param nltk_corpus: something like sentence_polarity\n",
    "    :param categories: a list of categories (each a string), \n",
    "        sklearn will treat categories as 0-based integers, thus we will map the ith element in this list to y=i\n",
    "    :param seed: for reproducibility\n",
    "    :param BOS: if not None, start every sentence with a single BOS token\n",
    "    :param EOS: if not None, end every sentence with a single EOS token\n",
    "    :return: training, dev, test\n",
    "        each an np.array such that \n",
    "        * array[:, 0] are the inputs (documents, each a string)\n",
    "        * array[:, 1] are the outputs (labels)\n",
    "    \"\"\"\n",
    "    pairs = []    \n",
    "    prefix = [BOS] if BOS else []\n",
    "    suffix = [EOS] if EOS else []\n",
    "    for label in categories:  # here we pair doc (as a single string) and label (string)\n",
    "        # this time we will concatenate the EOS symbol to the string\n",
    "        pairs.extend((' '.join(prefix + s + suffix), label) for s in nltk_corpus.sents(categories=[label]))\n",
    "    # we turn the pairs into a numpy array\n",
    "    # np arrays are very convenient for the indexing tools np provides, as we will see\n",
    "    pairs = np.array(pairs)\n",
    "    # it's good to shuffle the pairs\n",
    "    rng = np.random.RandomState(seed)    \n",
    "    rng.shuffle(pairs)\n",
    "    # let's split the np array into training (80%), dev (10%), and test (10%)\n",
    "    num_pairs = pairs.shape[0]\n",
    "    # we can use slices to select the first 80% of the rows\n",
    "    training = pairs[0:int(num_pairs * 0.8),:]\n",
    "    # and similarly for the next 10%\n",
    "    dev = pairs[int(num_pairs * 0.8):int(num_pairs * 0.9),:]\n",
    "    # and for the last 10%\n",
    "    test = pairs[int(num_pairs * 0.9):,:] \n",
    "    return training, dev, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MHE7lMyu4v1"
   },
   "source": [
    "Separate our corpus into training/dev/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwmUaINGu4v2"
   },
   "outputs": [],
   "source": [
    "so_training, so_dev, so_test = prepare_nltk_corpus(subjectivity, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HInVHrPNu4v2"
   },
   "outputs": [],
   "source": [
    "so_training.shape, so_dev.shape, so_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSNp929mdFPG"
   },
   "source": [
    "As always, begin by inspecting your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTz7Fha-dK16"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist([len(x.split()) for x, y in so_training], bins=30)\n",
    "_ = plt.xlabel(\"Length in tokens\")\n",
    "_ = plt.show()\n",
    "\n",
    "_ = plt.hist([y for x, y in so_training], bins=30)\n",
    "_ = plt.ylabel(\"Frequency\")\n",
    "_ = plt.xlabel(\"Class\")\n",
    "_ = plt.show()\n",
    "\n",
    "print(tabulate(so_training[4:6], headers=['doc', 'label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcYKb1SGU9dS"
   },
   "source": [
    "Helper code to map named labels to 0-based integers and back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PugoF4SY2MbF"
   },
   "outputs": [],
   "source": [
    "def label_as_string(y, vocab={True: 'subj', False: 'obj', 1: 'subj', 0: 'obj'}):\n",
    "    \"\"\"Map from boolean or integer to a string label\"\"\"\n",
    "    return [vocab[b] for b in y]\n",
    "\n",
    "def label_as_int(y, vocab={'subj': 1, 'obj': 0}):\n",
    "    \"\"\"Map from string to integer\"\"\"\n",
    "    return np.array([vocab[b] for b in y], dtype=int)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UteCynrnVQdL"
   },
   "outputs": [],
   "source": [
    "label_as_string([True, False]), label_as_int(['subj', 'obj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8Klxu7hlyCX",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Baseline:_NBC_on_Features'></a>\n",
    "## Baseline: NBC on Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcbnc40xVfXJ"
   },
   "source": [
    "Let's start with a classifier we already know, this will also help us get comfortable with feature functions. \n",
    "\n",
    "The NBC can be represented in terms of feature functions. Suppose a feature function $\\mathbf h(x)$ maps $x$ to a space where each coordinate $d$ corresponds to a word in the vocabulary, and $h_d(x)$ is the number of times that word occurs in $x$. This would make $\\mathbf h(x) \\in \\mathbb R^V$ a $V$-dimensional vector, but most of its coordinates would be in fact 0, since only up to $l = |x|$ words occur in $x$. If we use the notation $(f, n) \\in \\mathbf h(x)$ to denote all feature-count pairs for which the feature is a word and the count is not zero, we can rewrite the joint probability of the NB classifier as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    P_{YX}(y, x) = P_Y(y)\\prod_{(f, n) \\in \\mathbf h(x)} P_{F|Y}(f|y)^n\n",
    "\\end{equation}\n",
    "\n",
    "In class we discussed how this view of NBC can be used to generalise it to feature types are not just words, but in this tutorial we will continue working with word counts.\n",
    "\n",
    "To represent feature functions efficiently sklearn uses *vectorizers*. These classes turn a string into a sparse vector of coded features. Internally this builds a vocabulary of features and a data structure that is sparse like nested dicts, but much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yM6fpsXVfKw"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od8mvtCfVj8h"
   },
   "source": [
    "Take a moment to read the documentation of `CountVectorizer?` and play a bit with some examples.\n",
    "\n",
    "The count vectorizer can be used to implement NBC. We simply need to count the unigrams, which in sklearn style is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pocCtTmVpX4"
   },
   "outputs": [],
   "source": [
    "toy_vectorizer = CountVectorizer(ngram_range=(1,1), min_df=2)  # count all words that occur at least twice\n",
    "# let's start with the first 5 sentences in the training set, just to see what this does\n",
    "toy_vectorizer.fit(so_training[:5, 0])  \n",
    "print(tabulate(so_training[:5], headers=['doc', 'label']))\n",
    "print()\n",
    "print(f\"The feature space contains {len(toy_vectorizer.get_feature_names_out())} features.\\nThey are:\")\n",
    "for fname in toy_vectorizer.get_feature_names_out():\n",
    "    print(f\" F={fname}\")\n",
    "print()\n",
    "print(\"Note that sklearn is pre-processing the text for us, getting rid of some punctuation marks and English stopwords. We also chose to keep only words that occurred at least twice (with min_df=2).\")\n",
    "print(\"To apply the feature function to text we use the method *transform*:\")\n",
    "h_x05 = toy_vectorizer.transform(so_training[:5, 0])\n",
    "print(f\" which gives us an object of shape {h_x05.shape}, as expected.\")\n",
    "print(h_x05.shape)\n",
    "print(\"But that object is a sparse array (the zeros are not stored in it):\")\n",
    "print(h_x05)\n",
    "print(\"In some occasions we may need a dense numpy array, when that's the case we can use *toarray*:\")\n",
    "print(h_x05.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OapzoU0vevE2"
   },
   "source": [
    "With count features we can implement the NB classifier. In sklearn it is called `MultinomialNB`. \n",
    "\n",
    "Here are the necessary steps:\n",
    "1. Train a CountVectorizer with `min_df=1` using all of the training data. \n",
    "2. Then use it to train a MultinomialNB. We will set the alpha (the Laplace smoothing coefficient) to 0.7.\n",
    "3. Finally, we the performance on the *test set*, reporting the results via sklearn's `classification_report` as well as `ConfusionMatrixDisplay.from_predictions`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfAzpStdu4v3"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ectNpZEufg_N"
   },
   "outputs": [],
   "source": [
    "nb_vectorizer = CountVectorizer(ngram_range=(1,1), min_df=1)\n",
    "nb_vectorizer.fit(so_training[:, 0])\n",
    "nb_cls = MultinomialNB(alpha=0.7)\n",
    "nb_cls.fit(nb_vectorizer.transform(so_training[:,0]), so_training[:, 1])\n",
    "\n",
    "print(\"Classification report\")\n",
    "y_pred = nb_cls.predict(nb_vectorizer.transform(so_test[:, 0]))\n",
    "print(classification_report(so_test[:, 1], y_pred))\n",
    "print(\"Confusion matrix\")\n",
    "_ = ConfusionMatrixDisplay.from_predictions(so_test[:,1], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tsBuB23f84K"
   },
   "source": [
    "\n",
    "<details>\n",
    "    <summary><b>Curious how we got to alpha=0.7?</b></summary>\n",
    "\n",
    "We got sklearn to perform a grid search using cross validation on the training set for us:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cls_nb = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "        ('clf', MultinomialNB(alpha=0.)),                 \n",
    "    ]\n",
    ")\n",
    "# See the hyperparameters of the model (stuff like ngram_range and Laplace coefficient)\n",
    "print(cls_nb.get_params())\n",
    "# We can search for values of the hyperparameters of interest using cross validation\n",
    "nb_grid = GridSearchCV(cls_nb, param_grid={'clf__alpha': np.linspace(0.1, 1., 10)}, cv=3)\n",
    "nb_grid.fit(so_training[:, 0], so_training[:, 1])\n",
    "print(nb_grid.best_params_)\n",
    "print(classification_report(so_dev[:,1], nb_grid.predict(so_dev[:, 0])))\n",
    "```\n",
    "\n",
    "---    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELy5fJlAeL92",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Bernoulli_GLM_on_Jax'></a>\n",
    "## Bernoulli GLM on Jax\n",
    "\n",
    "To train GLMs we need gradient-based optimisation. Nowadays we do not compute derivatives by hand, nor implement automatic differentiation, we use software that does that for us, efficiently and realiably. \n",
    "\n",
    "In this tutorial you will use JAX, for you've already seen it in ML.\n",
    "\n",
    "Because we need JAX's automatic differentiation, we will need to express our model's computations using building blocks available in JAX. Luckily for us, JAX implements something very much like numpy and something very much like scipy, so all we've been using so far in the course is likely already available in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgU8HDoje43F"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "from jax import grad, value_and_grad, random\n",
    "from jax.nn import softplus  # useful to predict a strictly positive value\n",
    "from jax.nn import sigmoid  # useful to predict a probability value\n",
    "from jax.nn import softmax  # useful to predict a probability vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Curious about softplus, sigmoid and softmax?</b></summary>\n",
    "\n",
    "In case you want to get a visual idea of what these functions look like, you can plot them:\n",
    "   \n",
    "```python\n",
    "    \n",
    "def np_sigmoid(x):\n",
    "    \"\"\"Our own numpy implementation of sigmoid\"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "_ = plt.plot(np.linspace(-10, 10, 1000), np_sigmoid(np.linspace(-10, 10, 1000)))\n",
    "_ = plt.title(\"Sigmoid function\")\n",
    "_ = plt.xlabel(r'$s$')\n",
    "_ = plt.ylabel(r'sigmoid($s$) = $\\frac{1}{(1+\\exp(-s))}$')\n",
    "plt.show()\n",
    "\n",
    "def np_softplus(x):\n",
    "    \"\"\"This is our own numpy implementation of the softplus function\"\"\"\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "_ = plt.plot(np.linspace(-10, 10, 1000), np_softplus(np.linspace(-10, 10, 1000)))\n",
    "_ = plt.title(\"Softplus function\")\n",
    "_ = plt.xlabel(r'$s$')\n",
    "_ = plt.ylabel(r'softplus($x$) = $\\log(1+\\exp(x))$')\n",
    "plt.show()\n",
    "\n",
    "# Compare exp and softplus\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "_ = axs[0].plot(np.linspace(-10, 10, 1000), np.exp(np.linspace(-10, 10, 1000)))\n",
    "_ = axs[0].set_xlabel('s')\n",
    "_ = axs[0].set_ylabel('exp(s)')\n",
    "_ = axs[1].plot(np.linspace(-10, 10, 1000), np_softplus(np.linspace(-10, 10, 1000)))\n",
    "_ = axs[1].set_xlabel('s')\n",
    "_ = axs[1].set_ylabel('softplus(s)')\n",
    "fig.suptitle(\"Compare exp and softplus\")\n",
    "fig.tight_layout(h_pad=1, w_pad=1)\n",
    "plt.show()\n",
    "\n",
    "def np_softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "# let's get 1000 values for s0\n",
    "s1 = np.linspace(-3.0, 3.0, 1000)\n",
    "# for s2 and s3 we will have 1s and 0s (for the sake of illustration)\n",
    "s = np.vstack([s1, np.ones_like(s1), np.zeros_like(s1)]).T\n",
    "\n",
    "_ = plt.plot(s1, np_softmax(s)[:, 0], linewidth=2, label='output for k=1')\n",
    "_ = plt.plot(s1, np_softmax(s)[:, 1], linewidth=2, label='output for k=2')\n",
    "_ = plt.plot(s1, np_softmax(s)[:, 2], linewidth=2, label='output for k=3')\n",
    "_ = plt.plot(s1, np_softmax(s).sum(-1), ':', c='black', linewidth=2, label='elementwise sum')\n",
    "_ = plt.xlabel(r'$s_1$')\n",
    "_ = plt.ylabel('softmax output')\n",
    "_ = plt.title(\"Softmax function\")\n",
    "_ = plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "```\n",
    "\n",
    "---    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fk5xAxDfF4X"
   },
   "outputs": [],
   "source": [
    "# we need a random generator in order to create arrays in JAX\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiUeDXY4nbW6"
   },
   "source": [
    "Here we will \n",
    "\n",
    "1. describe the necessary functions to parameterise a Bernoulli GLM\n",
    "2. implement a stochastic gradient-based procedure for parameter estimation.\n",
    "\n",
    "Roughly this is what we need to do:\n",
    "\n",
    "1. obtain a feature function $\\mathbf h$ and compute it for all our data points\n",
    "2. obtain some initial parameters for the linear function\n",
    "3. implement the linear function that outputs the linear predictor\n",
    "4. constrain the linear predictor to being a probability value\n",
    "5. write a training loop, where we use some subset of the data to estimate the value of the log-likelihood function, use the gradient of that value with respect to model parameters to improve our parameter estimates\n",
    "\n",
    "So, let's get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eIeN0fDlVxX"
   },
   "source": [
    "## Feature function\n",
    "\n",
    "For feature function we are going to start with word counts (ie, `CountVectorizer`) and then we will normalise those into `tf-idf` features (see `TfidfTransformer`), which will help us take frequency information into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khMgBavjlbeP"
   },
   "outputs": [],
   "source": [
    "so_ff = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(ngram_range=(1,1), min_df=5)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "    ]\n",
    ")\n",
    "so_ff.fit(so_training[:, 0], so_training[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPqLYEUIl1sY"
   },
   "outputs": [],
   "source": [
    "so_training_h_sparse = so_ff.transform(so_training[:, 0])\n",
    "so_dev_h_sparse = so_ff.transform(so_dev[:, 0])\n",
    "so_test_h_sparse = so_ff.transform(so_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRZcw4hGf7C4"
   },
   "outputs": [],
   "source": [
    "# The number of input features is the $D$ in the formulas above.\n",
    "num_features = so_training_h_sparse.shape[1]\n",
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKooRFs5mK-y"
   },
   "source": [
    "### Computing the Bernoulli parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHTyXRWanSwp"
   },
   "source": [
    "To compute the Bernoulli parameter associated with each text input $x$ we start with a linear function of the features $\\mathbf h(x)$, for which we needs weight and a bias. So here we obtain those.\n",
    "\n",
    "Let's obtain $\\boldsymbol\\theta$ for binary classification using `num_features` input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJ8Nntm2fnCm"
   },
   "outputs": [],
   "source": [
    "def init_params_linear1(num_features, key):\n",
    "    \"\"\"\n",
    "    num_features: dimensionality of the feature space\n",
    "    key: a JAX random generator\n",
    "\n",
    "    Return w with shape [num_features] and bias with shape []\n",
    "    \"\"\"\n",
    "    w = random.uniform(key, shape=(num_features,))\n",
    "    b = random.uniform(key)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-pJ4zsX3ktu"
   },
   "outputs": [],
   "source": [
    "key, param_key = random.split(key, 2) \n",
    "w, b = init_params_linear1(num_features, param_key)\n",
    "assert w.shape == (num_features,)\n",
    "assert b.shape == ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ex-header": "Linear function in JAX",
    "id": "T-m_V3Jkngva"
   },
   "source": [
    "Below, we implement a real-valued linear function $\\mathbf w^\\top \\mathbf h(x) + b$ with $\\mathbf w\\in \\mathbb R^D$ and $b\\in \\mathbb R$ in JAX.\n",
    "\n",
    "Study our solution below for a bit before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jH_PdRkkgon5"
   },
   "outputs": [],
   "source": [
    "def linear1(inputs, *, w, b):\n",
    "    \"\"\"\n",
    "    inputs: a collection of inputs [batch_size, num_features]\n",
    "      we normally program our models supporting the ability to perform the same operations\n",
    "      for multiple documents at once\n",
    "      we do so by \"batching\" documents together, so our first dimension is used \n",
    "      to iterate over different documents\n",
    "    w: parameters (vector of size num_features)\n",
    "      see that the parameters do not depend on batch size\n",
    "      that's because parameters are a property of the model and batch size isn't\n",
    "    b: parameters (single scalar)\n",
    "\n",
    "    Return the following computation\n",
    "        \\sum_{d=1}^D x[d] * w[d] + b\n",
    "    for each document x[n] amongst the inputs.\n",
    "    \"\"\"   \n",
    "    # SOLUTION\n",
    "    # elementwise product\n",
    "    # [batch_size, num_features]\n",
    "    out = w * inputs  # by default this multiplication happens elementwise along the last dimension of the tensor\n",
    "    # reduce the feature dimension via sum \n",
    "    # [batch_size]\n",
    "    out = jnp.sum(out, axis=-1) \n",
    "    # add bias\n",
    "    out = out + b\n",
    "    # [batch_size]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-1'></a> **Ungraded Exercise 1 - Space and time complexity of the linear model**\n",
    "\n",
    "Study the implementation above, and answer:\n",
    "\n",
    "* What's the cost (in units of memory) to store our linear model? Represent it as a function of $D$.\n",
    "* Express the time to compute the linear predictor for a data point $x$, assume the feature function takes time $\\mathcal O(D)$ to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tCONTRIBUTE YOUR SOLUTION/DISCUSSION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phAZVIVHm4FJ"
   },
   "source": [
    "As the dev set is not too large, we will convert it to a dense numpy array, so that we can use it within JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyArQ6Qfm1hM"
   },
   "outputs": [],
   "source": [
    "so_dev_h = so_dev_h_sparse.toarray()\n",
    "\n",
    "# DO NOT use .toarray for the training data, you could run out of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db8znaGZKMnv"
   },
   "source": [
    "Unlike sklearn, JAX does not understand string labels, so we must convert our labels to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtULSvoaKEuZ"
   },
   "outputs": [],
   "source": [
    "so_training_y = label_as_int(so_training[:, 1])\n",
    "so_dev_y = label_as_int(so_dev[:, 1])\n",
    "so_test_y = label_as_int(so_test[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3nN3VdEm9zd"
   },
   "source": [
    "Now we can test our `linear1` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VlrUDrdoa_z"
   },
   "outputs": [],
   "source": [
    "# We can give it a single document, then we get a single output\n",
    "assert linear1(so_dev_h[0:1], w=w, b=b).shape == (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7GbD_YHonXa"
   },
   "outputs": [],
   "source": [
    "# or we can give it 10 documents, then we get 10 outputs\n",
    "assert linear1(so_dev_h[0:10], w=w, b=b).shape == (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Og0fDl2gpXv"
   },
   "outputs": [],
   "source": [
    "# or we can give it the entire dev set, and get one output per doc in the dev set\n",
    "assert linear1(so_dev_h, w=w, b=b).shape == (so_dev_h.shape[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN7ZE69lpbeO"
   },
   "source": [
    "For this statistical model, a binary classifier, we need to map $x$ to a probability value (the Bernoulli parameter), but the linear function is not constrained accordingly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GS3_pcASpUXr"
   },
   "outputs": [],
   "source": [
    "assert jnp.alltrue(linear1(so_dev_h, w=w, b=b) >= 0.) and jnp.alltrue(linear1(so_dev_h, w=w, b=b) <= 1.) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LnmksvKo_DM"
   },
   "source": [
    "For that, jax offers the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS8jeQkgpJLQ"
   },
   "outputs": [],
   "source": [
    "def make_prob(linear_predictor):\n",
    "    \"\"\"\n",
    "    Properly constrains the linear predictor in order to parameterise a Bernoulli distribution.\n",
    "\n",
    "    linear_predictor: an array of linear predictors with shape [batch_size]\n",
    "    \"\"\"\n",
    "    return sigmoid(linear_predictor)  # this a JAX function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MlPa1Bippwp"
   },
   "source": [
    "which constrains everything as we wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03YQX7V6pM6N"
   },
   "outputs": [],
   "source": [
    "assert jnp.alltrue(make_prob(linear1(so_dev_h, w=w, b=b)) >= 0.) and jnp.alltrue(make_prob(linear1(so_dev_h, w=w, b=b)) <= 1.) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MCEn-h03umS"
   },
   "source": [
    "Next we assess the Bernoulli log pmf using JAX differentiable code. Luckily for us JAX has a [`jax.scipy.sats`](https://jax.readthedocs.io/en/latest/jax.scipy.html#jax-scipy-stats) module, which contains many of our favourite distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_vjIxrp3wPj"
   },
   "outputs": [],
   "source": [
    "def bernoulli_logpmf(y, p):\n",
    "    \"\"\"\n",
    "    y: a batch of binary values with shape [batch_size]\n",
    "    p: a batch of probability values with shape [batch_size]\n",
    "\n",
    "    Return \\log Bernoulli(y[n]|p[n]) for each element y[n] in the batch.\n",
    "    \"\"\"\n",
    "    # this is \\log p when y is 1 and \\log(1-p) when y is 0\n",
    "    # we could implement it ourselves, for example:\n",
    "    #   y * jnp.log(p) + (1-y) * jnp.log(1-p)\n",
    "    # but it's always better to use an existing stress-tested implementation : )\n",
    "    return jax.scipy.stats.bernoulli.logpmf(y, p=p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjsX6S6eYsGK"
   },
   "outputs": [],
   "source": [
    "assert jnp.allclose(jnp.exp(bernoulli_logpmf(np.array([1, 1, 1]), np.array([0.9, 0.5, 0.1]))), np.array([0.9, 0.5, 0.1]), 1e-6)\n",
    "assert jnp.allclose(jnp.exp(bernoulli_logpmf(np.array([0, 0, 0]), np.array([0.9, 0.5, 0.1]))), np.array([0.1, 0.5, 0.9]), 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aImk2GCgptF8"
   },
   "source": [
    "<details>\n",
    "    <summary><b>Check this, if you'd like to draw samples from Bernoulli in JAX</b></summary>\n",
    "\n",
    "```python\n",
    "y_sample = random.bernoulli(key, p=make_prob(linear1(bin_dev_h, w=w, b=b)))\n",
    "```\n",
    "---    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viCOZ-_m4FOC"
   },
   "source": [
    "We can also check the performance of the most probable class (for an untrained model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MozIzzP34JDi"
   },
   "outputs": [],
   "source": [
    "def bernoulli_mode(p):\n",
    "    return jnp.where(p > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-H7GsSwAyRO"
   },
   "outputs": [],
   "source": [
    "assert jnp.alltrue(bernoulli_mode(np.array([0.51, 0.1, 0.8])) == np.array([1, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ixmfQp1oI-1"
   },
   "source": [
    "Finally, let's test our untrained Bernoulli GLM on the dev set. For predictions, we will use the mode of the predicted Bernoulli distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QyUpsUX4XU3"
   },
   "outputs": [],
   "source": [
    "y_best = bernoulli_mode(make_prob(linear1(so_dev_h, w=w, b=b)))\n",
    "print(classification_report(so_dev[:,1], label_as_string(np.array(y_best)), zero_division=0))\n",
    "_ = ConfusionMatrixDisplay.from_predictions(so_dev[:,1], label_as_string(np.array(y_best)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgQKb1UXaJuH"
   },
   "source": [
    "The next piece of code will plot the Bernoulli pmfs that our GLM predicts for the 16 first documents in the dev set. You will be able to reuse this code later to analyse a trained model.\n",
    "\n",
    "This model is not trained yet, so don't be surprised if the plots aren't interesting yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdSDtF-KlWRq"
   },
   "outputs": [],
   "source": [
    "import textwrap \n",
    "\n",
    "fig, axs = plt.subplots(4, 4, sharex=True, figsize=(16, 16))\n",
    "\n",
    "print(\"The plots show samples from the conditional model Y|X=x.\\nThe red vertical line shows the observation Y=y for X=x.\")\n",
    "\n",
    "for i in range(16):\n",
    "    x, y = so_dev[i, 0], so_dev[i, 1]\n",
    "    h = so_ff.transform([x]).toarray()\n",
    "    prob = make_prob(linear1(h, w=w, b=b)).item()\n",
    "\n",
    "    _ = axs[i//4,i%4].set_title(\"\\n\".join(textwrap.wrap(x, 30)))\n",
    "    _ = axs[i//4,i%4].plot(label_as_string([0, 1]), [1-prob, prob], 'x') \n",
    "    _ = axs[i//4,i%4].axvline(y, c='red')\n",
    "    _ = axs[i//4,i%4].set_ylabel(r\"$P(y|x)$\")\n",
    "    _ = axs[i//4,i%4].set_xlabel(\"class (y)\")\n",
    "    \n",
    "fig.tight_layout(h_pad=1, w_pad=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEw5kIsb40Pp"
   },
   "source": [
    "### Parameter estimation\n",
    "\n",
    "For a Bernoulli GLM, our likelihood function is:\n",
    "\\begin{equation}\n",
    "\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) = \\sum_{n=1}^N \\log \\mathrm{Bernoulli}(y^{(n)}|g(x^{(n)}; \\boldsymbol \\theta)) \n",
    "\\end{equation}\n",
    "\n",
    "And for stochastic gradient-based optimisation we will approximate it using mini batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sj7LhAPfsAlY"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to regularise our model to try and avoid overfitting, for now we do so via L2-regularisation (i.e., a penalty on the L2-norm of the parameter vector). We implement L2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zj61xunDBc2x"
   },
   "outputs": [],
   "source": [
    "def l2_regulariser(w, b):\n",
    "    \"\"\"Here we implement the L2 regulariser for you\"\"\"\n",
    "    return jax.numpy.linalg.norm(w.flatten(), 2) + jax.numpy.linalg.norm(b.flatten(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0CiRH5YDPzm"
   },
   "outputs": [],
   "source": [
    "assert jnp.isclose(l2_regulariser(np.array([1., 2., 3.]), np.array(4.)), np.sqrt(1**2 + 2**2 + 3**2) + np.sqrt(4**2), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ex-header": "Loss function to train BernoulliGLM in JAX",
    "id": "xgCXpwbIEk_D"
   },
   "source": [
    "Below we use [jax.scipy.stats](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.stats.bernoulli.logpmf.html#jax.scipy.stats.bernoulli.logpmf) functions to define a *loss function* for the Bernoulli GLM model. Our loss function is the **negative** of the L2-regularised log-likelihood function.\n",
    "\n",
    "Study our implementation for a bit before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_loss(w, b, *, inputs, targets, l2weight=0.): \n",
    "    \"\"\"\n",
    "    w: weights of the linear model with shape [num_features]\n",
    "    b: bias of the linear model with shape []\n",
    "    inputs: h(x) with shape [batch_size, num_features]\n",
    "    targets: y with shape [batch_size]\n",
    "    l2weight: this is the lambda > 0 in the formula, the weight of the regulariser\n",
    "    \"\"\"\n",
    "    # SOLUTION\n",
    "    s = linear1(inputs, w=w, b=b)\n",
    "    p = make_prob(s)\n",
    "    l2 = l2_regulariser(w=w, b=b)\n",
    "    return - jax.scipy.stats.bernoulli.logpmf(targets, p=p).mean(0) + l2weight * l2     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-2'></a> **Ungraded Exercise 2 - Analysing the loss for the Bernoulli GLM**\n",
    "\n",
    "Study the implementation above, then assuming $\\lambda$ is the `l2weight`, $x$ is a single input and $y$ its target, what options are correct:\n",
    "\n",
    "1. The loss is numerically equivalent to $-\\log \\mathrm{Bernoulli}(y|\\mathrm{sigmoid}(\\mathbf w^\\top \\mathbf h(x) + b) + \\lambda \\sqrt{b^2 +\\sum_{d=1}^D w_d^2}$\n",
    "2. The first term in the loss is numerically equivalent to $-\\log \\mathrm{sigmoid}(\\mathbf w^\\top \\mathbf h(x) + b)$ if $y=1$ and $-\\log (1-\\mathrm{sigmoid}(\\mathbf w^\\top \\mathbf h(x) + b))$ if $y=0$\n",
    "3. If we had multiple inputs (and their corresponding targets), the first term of the loss would be the negative of the average of the log probability that our model assigns to the targets given the inputs\n",
    "4. For a single observations, the first term of the loss is $0$ if the model is right and $-1$ if the model is wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. Correct, the log pmf is precisely the log of the Bernoulli pmf, and the Bernoulli parameter is precisely the sigmoid of the linear predictor, besides the L2 regulariser is defined as the squared-root of the sum of squared parameters.\n",
    "2. Correct, the first term of the loss is negative of the logarithm of the mass assigned to the target, the probability mass is precisely the Bernoulli parameter value when the target is 1, when the target is 0 the mass is 1 minus the Bernoulli parameter.\n",
    "3. Correct, when we have multiple observations, the loss is based on the average of their log probabilities under the model, that's what the `.mean(0)` part of the loss does. \n",
    "4. Incorrect, the first term is a log probability of the positive (1) or negative (0) observed outcome.\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u43SmuW7FN8O"
   },
   "outputs": [],
   "source": [
    "test_case_inputs = np.array([[1., 0.], [0., 1.], [0., 0.]])\n",
    "test_case_targets = np.array([1, 0, 1])\n",
    "test_case_w = np.array([-1., 2.])\n",
    "test_case_b = np.array(1.)\n",
    "# the linear predictor is correct\n",
    "assert jnp.allclose(linear1(test_case_inputs, w=test_case_w, b=test_case_b), np.array([0., 3., 1]), 1e-3), \"Did you remember to compute dot-product with w and add the bias b?\"\n",
    "# the probability is correct\n",
    "assert jnp.allclose(make_prob(np.array([0., 3., 1.])), np.array([0.5, 0.952574, 0.73105]), 1e-3), \"Did you use the sigmoid function?\"\n",
    "# the regulariser is correct\n",
    "assert jnp.isclose(l2_regulariser(test_case_w, test_case_b), np.array(3.23606), 1e-3), \"Did you change the regulariser code?\"\n",
    "assert bernoulli_loss(test_case_w, test_case_b, inputs=test_case_inputs, targets=test_case_targets).shape == (), \"Did you remember to take the mean?\"\n",
    "# the (unregulariser) loss function is correct\n",
    "assert jnp.isclose(bernoulli_loss(test_case_w, test_case_b, inputs=test_case_inputs, targets=test_case_targets), - np.mean([np.log(0.5), np.log(1-0.952574), np.log(0.73105)]), 1e-3), \"Did you remember the minus?\"\n",
    "# the regulariseed loss function is correct\n",
    "assert jnp.isclose(bernoulli_loss(test_case_w, test_case_b, inputs=test_case_inputs, targets=test_case_targets, l2weight=1.0), - np.mean([np.log(0.5), np.log(1-0.952574), np.log(0.73105)]) + np.array(3.23606), 1e-3), \"Did you remember the minus?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXMFDMW3JDEL"
   },
   "source": [
    "Here we will pack the loss code for you in a way that can be used for automatic differentiation, and we will then compute the gradient with respect to $\\mathbf w$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPuU-6B1tyzM"
   },
   "outputs": [],
   "source": [
    "from functools import partial  # partial allows us to fix some of the arguments of a function\n",
    "\n",
    "loss_fn = partial(\n",
    "    bernoulli_loss, # we want to fix some of the arguments of bernoulli_loss\n",
    "    inputs=so_dev_h,   # namely, the part that concerns the observed data, both h(x)\n",
    "    targets=so_dev_y,  # and y\n",
    "    l2weight=0.     # we also want to fix the hyperparameters, \n",
    ") # this creates a loss function which is a function of w and b (exactly as we would like it to be)\n",
    "\n",
    "# Now we tell JAX to evaluate the loss_fn for the parameter values (w, b) that we currently have\n",
    "#  and compute partial derivatives for them\n",
    "loss, (grad_w, grad_b) = value_and_grad(loss_fn, (0, 1))(w, b)\n",
    "# the partial derivatives together form the gradient vector, which always has the same dimensionality as the parameter vector\n",
    "assert grad_w.shape == (num_features,)\n",
    "assert grad_b.shape == ()\n",
    "# the loss, of course, is a scalar \n",
    "assert loss.shape == ()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_R324aSUK1X9"
   },
   "source": [
    "This is it, you have all the ingredients needed to prescribe the model (its parameterisation in terms of $\\mathbf h(x)$ and $\\boldsymbol\\theta$, to use it (i.e., compute log probability for a given $(x, y)$ pair), and to train it (i.e., estimate its paramters using regularised MLE).\n",
    "\n",
    "We will wrap everything nicely for you into a single class, so that you can experiment with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RNZljvp63zu",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Python/JAX_GLM_Class'></a>\n",
    "# Python/JAX GLM Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-3'></a> **Ungraded Exercise 3 - GLM class**\n",
    "\n",
    "Study the class below, it implements a general purpose GLM for a conditional distribution with a 1-dimensional statistical parameter. This can be used, for example, for binary classification and for Poisson regression. This class lacks implementation for a couple of methods, but you should not attempt to implememtn it. Instead, those are implemented in classes that specialise this one. For example, study the BinaryClassifier class that we provide right after GLM1. That class, will complete the specfication of GLM1 by making it a Bernoulli conditional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDC7M3Mf6WPU"
   },
   "outputs": [],
   "source": [
    "class GLM1:\n",
    "    \"\"\"\n",
    "    This class contains all functionality that are shared across GLMs for single parameter distributions.\n",
    "    Subclasses of this class must only implement the parts that are specific to their choice of distribution.\n",
    "    For example, \n",
    "        GLM1 uses the mode for prediction, but how to compute the mode depends on which distribution we have.\n",
    "        GLM1 knows that the activation function is needed in order to constrain the linear predictor correctly, \n",
    "         but the choice of activation function will depend on the distribution we have.        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_function, seed=0):\n",
    "        self.feature_function = feature_function        \n",
    "        self.num_features = len(feature_function.get_feature_names_out())\n",
    "        self.key = random.PRNGKey(seed)\n",
    "\n",
    "    def feature_names(self):\n",
    "        return self.feature_function.get_feature_names_out()\n",
    "    \n",
    "    def random_parameters(self):\n",
    "        \"\"\"\n",
    "        Return randomly initialised weights and biases\n",
    "        \"\"\"\n",
    "        w = random.uniform(self.key, shape=(self.num_features,))\n",
    "        b = random.uniform(self.key) \n",
    "        return w, b\n",
    "\n",
    "    def l2(self, w, b):\n",
    "        \"\"\"Compute the L2 regulariser\"\"\"\n",
    "        return jax.numpy.linalg.norm(w.flatten(), 2) + jax.numpy.linalg.norm(b.flatten(), 2)\n",
    "\n",
    "    def linear(self, inputs, *, w, b):\n",
    "        \"\"\"\n",
    "        inputs: a collection of inputs [batch_size, num_features]\n",
    "        we normally program our models supporting the ability to perform the same operations\n",
    "        for multiple documents at once\n",
    "        we do so by \"batching\" documents together, so our first dimension is used \n",
    "        to iterate over different documents\n",
    "        w: parameters (vector of size num_features)\n",
    "        see that the parameters do not depend on batch size\n",
    "        that's because parameters are a property of the model and batch size isn't\n",
    "        b: parameters (single scalar)\n",
    "\n",
    "        Return x * w + b\n",
    "        \"\"\"    \n",
    "        # elementwise product\n",
    "        # [batch_size, num_features]\n",
    "        out = w * inputs  # by default this multiplication happens elementwise along the last dimension of the tensor\n",
    "        # reduce the feature dimension via sum \n",
    "        # [batch_size]\n",
    "        out = jnp.sum(out, axis=-1) \n",
    "        # add bias\n",
    "        out = out + b\n",
    "        # [batch_size]\n",
    "        return out\n",
    "\n",
    "    def activation(self, linear_predictor):\n",
    "        \"\"\"\n",
    "        Constrain the linear predictor\n",
    "\n",
    "        linear_predictor: w*h(x)+b with shape [batch_size]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me in a subclass\")    \n",
    "\n",
    "    def g(self, inputs, *, w, b):\n",
    "        \"\"\"\n",
    "        Compute the statistical parameter of the conditional model.\n",
    "\n",
    "        inputs: h(x) with shape [batch_size, num_features]\n",
    "        w: weights with shape [num_features]\n",
    "        b: bias with shape []\n",
    "\n",
    "        Return g(x) with shape [batch_size]\n",
    "        \"\"\"\n",
    "        linear_predictor = self.linear(inputs, w=w, b=b)\n",
    "        parameter = self.activation(linear_predictor)\n",
    "        return parameter\n",
    "\n",
    "    def decide(self, g):\n",
    "        \"\"\"\n",
    "        Decide what output better represents this pmf/pdf.\n",
    "\n",
    "        g: statistical parameter of the distribution with shape [batch_size]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me in a subclass!\")\n",
    "  \n",
    "\n",
    "    def log_p(self, targets, g):\n",
    "        \"\"\"\n",
    "        Return log probability mass (or density) of targets given the statistical parameter.\n",
    "\n",
    "        targets: y with shape [batch_size]\n",
    "        g: statistical parameter of the distribution with shape [batch_size]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me in a subclass!\")\n",
    "\n",
    "    def loss(self, w, b, *, inputs, targets, l2weight=0.):\n",
    "        \"\"\"\n",
    "        Compute the regularised negative log-likelihood of the model given observed (x, y) pairs.\n",
    "\n",
    "        w: weights with shape [num_features]\n",
    "        b: bias with shape []\n",
    "        inputs: h(x) with shape [batch_size, num_features]\n",
    "        targets: y with shape [batch_size]\n",
    "        l2weight: contribution of the L2 regulariser\n",
    "\n",
    "        Return - \\sum_{n=1}^N P(Y=y[n]|X=x[n]) + L2(w,b)\n",
    "        \"\"\"\n",
    "        return - self.log_p(targets, g=self.g(inputs, w=w, b=b)).mean(0) + l2weight * self.l2(w, b) \n",
    "\n",
    "    def predict(self, inputs, *, w, b):\n",
    "        \"\"\"\n",
    "        Predict y for h using the mode of the conditional distribution.\n",
    "        \n",
    "        inputs: h(x) with shape [batch_size, num_features]\n",
    "        w: weights with shape [num_features]\n",
    "        b: bias with shape []\n",
    "\n",
    "        Return \\argmax_y P(Y=y|X=x)\n",
    "        \"\"\"\n",
    "        return self.decide(self.g(inputs, w=w, b=b))\n",
    "\n",
    "    def validate(self, inputs, targets, *, w, b):\n",
    "        \"\"\"\n",
    "        Predict y for h and compare it to y_true using an appropriate metric.\n",
    "        \n",
    "        inputs: h(x) with shape [batch_size, num_features]\n",
    "        targets: true targets with shape [batch_size]\n",
    "        w: weights with shape [num_features]\n",
    "        b: bias with shape []\n",
    "\n",
    "        Return performance metric (e.g., f1 for classification, MAE for regression)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me in a subclass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vthLviltXKk_"
   },
   "source": [
    "Here we specialise this class to the case of Binary classification with a Bernoulli conditional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyeziVxg712L"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class BinaryClassifier(GLM1):\n",
    "\n",
    "    def __init__(self, feature_function, seed=0):\n",
    "        super().__init__(feature_function, seed=seed)\n",
    "        \n",
    "    def activation(self, linear_predictor):\n",
    "        \"\"\"For Binary classification we need to parameterise a Bernoulli, thus we constrain the linear predictor using sigmoid\"\"\"\n",
    "        return sigmoid(linear_predictor)\n",
    "\n",
    "    def decide(self, g):\n",
    "        \"\"\"\n",
    "        For the binary classifier, we will base decisions on the mode of the Bernoulli,\n",
    "        which is the outcome that receives most mass.\n",
    "        \"\"\"        \n",
    "        return jnp.where(g > 0.5, 1, 0)\n",
    "\n",
    "    def log_p(self, targets, g):\n",
    "        \"\"\"\n",
    "        We use JAX scipy to return the log pmf of the Bernoulli\n",
    "        \"\"\"\n",
    "        return jax.scipy.stats.bernoulli.logpmf(targets, p=g)\n",
    "\n",
    "    def validate(self, inputs, targets, *, w, b):\n",
    "        \"\"\"For binary classification we report macro F1\"\"\"\n",
    "        y_pred = self.predict(inputs, w=w, b=b)\n",
    "        metrics = classification_report(targets, y_pred, output_dict=True, zero_division=0)\n",
    "        return metrics['macro avg']['f1-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-4'></a> **Ungraded Exercise 4 - Code for parameter estimation**\n",
    "\n",
    "Here we share some code useful for training a model. Study the steps of the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3HyQUha8SOn"
   },
   "outputs": [],
   "source": [
    "# **YOU SHOULD NOT NEED TO EDIT THIS CELL**\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def get_batcher(data_size, batch_size, replace=False, rng=np.random.RandomState(1)):  \n",
    "    \"\"\"\n",
    "    Return an iterable for indices of data points organised as batches of a given size (the last batch is potentially shorter).\n",
    "    \"\"\"  \n",
    "    if rng is None:\n",
    "        permutation = np.arange(data_size)\n",
    "    else:\n",
    "        permutation = rng.permutation(data_size)\n",
    "    i = 0\n",
    "    while i < data_size:\n",
    "        yield permutation[i:i+batch_size]\n",
    "        i += batch_size        \n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, training_h_sparse, training_y, dev_h_sparse, dev_y, \n",
    "    num_epochs=5, batch_size=500, validate_freq=10, \n",
    "    l2weight=1e-6, lr0=10., step_overwrite=0, log=None, \n",
    "    w=None, b=None, \n",
    "    rng=None):\n",
    "    \"\"\"\n",
    "    model: an instance of GLM1 specialised to a type of distribution\n",
    "    training_h_sparse: a scipy sparse matrix of features for the training data\n",
    "    training_y: a numpy array of targets for the training data\n",
    "    dev_h_sparse: a scipy sparse matrix of features for the validation data\n",
    "    dev_y: a numpy array of targets for the validation data\n",
    "    num_epochs: total number of passes over the entire training data\n",
    "    batch_size: how many instances are used for a single gradient step        \n",
    "    validate_freq: how often (in number of gradient steps) we compute performance of the validation set\n",
    "    l2weight: the contribution of the L2 regulariser\n",
    "    lr0: the initial learning rate    \n",
    "    step_overwrite: when you continue training, you can overwrite the step number (which affects the schedule of the learning rate)\n",
    "    log: when you continue training, you can reuse an existing log\n",
    "    w: use this to continue training (instead of training from scratch)\n",
    "    b: use this to continue training (instead of training from scratch)\n",
    "    rng: numpy random number generator, use it for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    if w is None or b is None:  # we start with some random parameters\n",
    "        w, b = model.random_parameters()\n",
    "    \n",
    "    dev_h = dev_h_sparse.toarray()\n",
    "    data_size = training_h_sparse.shape[0]\n",
    "    if log is None:\n",
    "        log = defaultdict(list)\n",
    "    \n",
    "    # It's good to always run the model before training, \n",
    "    # just to catch any problems and to log its current performance\n",
    "    log['val_metric'].append(model.validate(dev_h, dev_y, w=w, b=b))\n",
    "    log['val_loss'].append(model.loss(w, b, inputs=dev_h, targets=dev_y, l2weight=l2weight))    \n",
    "    \n",
    "    # and we will train for a certain number of steps\n",
    "    total_steps = num_epochs * len(list(get_batcher(data_size, batch_size)))\n",
    "    step = step_overwrite  # and sometimes we have already trained for a bit  \n",
    "\n",
    "    with tqdm(range(total_steps), desc='MLE') as bar:  # tqdm is a very cool progressbar :)\n",
    "        # we pass over the entire data a number of times\n",
    "        for epoch in range(num_epochs):            \n",
    "            # but do so in random order, and process one mini batch at a time\n",
    "            for ids in get_batcher(data_size, batch_size, rng=rng):\n",
    "\n",
    "                # our learning rate decays with time, this is something needed for the theory of stochastic optimisation to check out\n",
    "                lr = lr0/np.log10(10+step)  \n",
    "\n",
    "                # we prepare our loss function\n",
    "                # essentially, we fix the data it is based on (the inputs and targets in the mini batch)\n",
    "                # and fix its hyperparameters\n",
    "                # the only things that can vary are the parameters (w, b)\n",
    "                loss_fn = partial(\n",
    "                    model.loss, \n",
    "                    inputs=training_h_sparse[ids].toarray(),  # we need dense arrays but we better not store all dense arrays (else we run out of memory)\n",
    "                    targets=training_y[ids], \n",
    "                    l2weight=l2weight\n",
    "                )\n",
    "                # here we tell JAX to evaluate the loss function at the point (w, b)\n",
    "                # and also compute its partial derivatives at that point\n",
    "                loss_value, (grad_w, grad_b) = value_and_grad(loss_fn, (0, 1))(w, b)\n",
    "                \n",
    "                # because we have a *loss* we will subtract the gradient (scaled by the learning rate)\n",
    "                # from the current parameter values\n",
    "                w -= lr * grad_w\n",
    "                b -= lr * grad_b\n",
    "\n",
    "                # Here we log some information for analysis later on\n",
    "                log['loss'].append(loss_value.item())       \n",
    "                if step % validate_freq == 0:  \n",
    "                    log['val_metric'].append(model.validate(dev_h, dev_y, w=w, b=b))\n",
    "                    log['val_loss'].append(model.loss(w, b, inputs=dev_h, targets=dev_y, l2weight=l2weight))\n",
    "                bar.set_postfix({'epoch': epoch + 1, 'step': f\"{step:4d}\", 'lr': f\"{lr:.4f}\", 'loss': f\"{loss_value:.4f}\", 'val_loss': f\"{log['val_loss'][-1]:.4f}\", 'val_metric': log['val_metric'][-1]}) \n",
    "                bar.update()\n",
    "                step += 1\n",
    "    \n",
    "    return (w, b), log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF_Hz9kvMaGw",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Binary_classification_experiment'></a>\n",
    "## Binary classification experiment\n",
    "\n",
    "From now on, we will use the following feature function. In NBC, we use counts, but counts are not a great idea for linear and generalised linear models. That's because raw counts vary with document length, which makes the magnitude of the dot product $\\mathbf w^\\top\\mathbf h(x)$ also vary a lot with document length. \n",
    "\n",
    "It's better to take frequency information into account, we wil be using a tf-idf transformation of the raw counts, sklearn can do that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TPx7ATXDAKh"
   },
   "outputs": [],
   "source": [
    "bin_ff = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(ngram_range=(1,1), min_df=5)), # we will be discarding tokens less frequent than 5\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "    ]\n",
    ")\n",
    "bin_ff.fit(so_training[:, 0])\n",
    "\n",
    "bin_training_h_sparse = bin_ff.transform(so_training[:, 0])\n",
    "bin_training_y = label_as_int(so_training[:, 1])\n",
    "\n",
    "bin_dev_h_sparse = bin_ff.transform(so_dev[:, 0])\n",
    "bin_dev_y = label_as_int(so_dev[:, 1])\n",
    "\n",
    "bin_test_h_sparse = bin_ff.transform(so_test[:, 0])\n",
    "bin_test_y = label_as_int(so_test[:, 1])\n",
    "\n",
    "bin_cls = BinaryClassifier(bin_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hpj_5WtZ46l"
   },
   "outputs": [],
   "source": [
    "# This should take a couple of minutes (the progress bar should give you a reliable estimate)\n",
    "# for a faster run reduce num_epochs or increase batch_size \n",
    "# of course, do expect changes in performance\n",
    "# you can use lr0 and l2weight to affect the optimiser and the objective function\n",
    "(bin_w, bin_b), bin_log = train_model(\n",
    "    bin_cls,\n",
    "    bin_training_h_sparse,\n",
    "    bin_training_y,\n",
    "    bin_dev_h_sparse,\n",
    "    bin_dev_y,\n",
    "    lr0=5, # vary this to change the initial learning rate\n",
    "    l2weight=1e-4, # vary this to control regularisation\n",
    "    batch_size=100, # on GPU you can use larger batches\n",
    "    num_epochs=10, # use more to train for longer\n",
    "    rng=np.random.RandomState(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BbHzrLQoBM-"
   },
   "source": [
    "We should always visualise the training loss, the validation loss, and some performance metric. \n",
    "\n",
    "The validation loss is the log-likelihood of the model given the validation data only (not the training data). If that curve starts going up, while the training loss curve is going down, we detect overfitting (a situation where the model is memorising patterns that are specific to the training data and are of no help to classify heldout data). \n",
    "\n",
    "The metric for classification helps us see how the most probable class decision rule performs for heldout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhDe2ZQw1c8K"
   },
   "outputs": [],
   "source": [
    "skip = 1 # skipping the fully first evaluation (fully untrained model)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "_ = ax[0].plot(np.arange(len(bin_log['loss'][skip:])), bin_log['loss'][skip:], '.')\n",
    "_ = ax[0].set_ylabel(\"Training loss\")\n",
    "_ = ax[0].set_xlabel(\"Steps\")\n",
    "_ = ax[1].plot(np.arange(len(bin_log['val_loss'][skip:])), bin_log['val_loss'][skip:], '.')\n",
    "_ = ax[1].set_ylabel(\"Validation loss\")\n",
    "_ = ax[1].set_xlabel(\"Validation steps\")\n",
    "_ = ax[2].plot(np.arange(len(bin_log['val_metric'][skip:])), bin_log['val_metric'][skip:], '.')\n",
    "_ = ax[2].set_ylabel(\"Validation macro F1\")\n",
    "_ = ax[2].set_xlabel(\"Validation steps\")\n",
    "fig.tight_layout(h_pad=1, w_pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRe8CwJKod19"
   },
   "source": [
    "We should always visualise the magnitude of the weights. If they get too large we typically have numerical instabilities and/or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_j6gOH4PxIT"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(np.array(bin_w).flatten(), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEkIdYV0P_GG"
   },
   "source": [
    "And we can have a look at the bias term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJtQLyO2P9Df"
   },
   "outputs": [],
   "source": [
    "bin_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5l7ahwsKr-N"
   },
   "source": [
    "We can also order the features by importance and inspect them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrxsVh6GKuUG"
   },
   "outputs": [],
   "source": [
    "print(\"20 features with large positive weight\")\n",
    "for f, w in sorted(zip(bin_cls.feature_names(), np.array(bin_w)), key=lambda pair: pair[1], reverse=True)[:20]:\n",
    "    print(f, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-1'> **Graded Exercise 1 - Subjectivity classifier** </a>\n",
    "\n",
    "Here you will experiment with various Bernoulli GLMs on the subjectivity dataset using the feature function we provided you with. Here we recommend 100 epochs, more can be better, less can be okay too. Without GPUs, more than 100 epochs may be too slow.\n",
    "\n",
    "1. Train for 100 epochs with regularisation 1e-4. Plot the log information (training/validation loss and metric curves). Comment on when the classifier converged (when the curves got roughly flat), if at all. Make a good quality plot for full points.\n",
    "\n",
    "2. Train for 100 epochs with and without regularisation (use l2weight of 0 and of 1e-4). Plot the log information (training/validation loss and metric curves). Also plot a histogram with the flattened vector of weights (no need for bias), compare the histograms for the variant with and without regularisation. Discuss whether you see any risk for overfitting with this model (it's not always the case that overfitting happens).\n",
    "\n",
    "3. For your model with regularisation, assess the model on the test set (display a classification report and the confusion matrix). For that model, inspect some features that received large positive weights and large negative weights. Discuss whether the features you see there are plausibly related to subjectivity and objectivity. \n",
    "\n",
    "\n",
    "Do not expect the performance to necessarily match the NBC. This dataset is simple enough that NBC is a *strong baseline*. Also, our feature function is not very complex, because we want to keep the notebook tutorial easy to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tCONTRIBUTE YOUR SOLUTION/DISCUSSION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-5'></a> **Ungraded Exercise 5 - Change the feature function**\n",
    "\n",
    "When you are done with the graded part of the assignment, try changing the feature function (for example, you can count word pairs), see if you can affect the result in a meaningful way (esp, part 3 of the exercise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKFYo813xEYP",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Ordinal_regression_experiment'></a>\n",
    "## Ordinal regression experiment\n",
    "\n",
    "*Semantic text similarity* is a task where humans are asked to assess the extent to which two texts $x=(a,b)$, text $a$ and text $b$, convey the same meaning. We gather human responses by asking them to assign a score, in this example, from 0 to 100 (imagine they have access to a slider from 'not at all related' 0 to 'meaning equivalent' 100). \n",
    "    \n",
    "We record data points $(x, y)$ where $x=(a,b)$ is a pair of texts and $y$ is a numerical response indicating semantic similarity. In this version we have integer scores from 0 to 100.\n",
    "    \n",
    "The dataset we provide is based on the [SICK dataset](https://marcobaroni.org/composes/sick.html).\n",
    "    \n",
    "We provide a training, a development and a test partition. We have access to 10 responses per $x$, which allows us to assess human variability when performing this task (and whether our models capture it well).\n",
    "    \n",
    "We will develop a GLM for modelling the conditional distribution of the response variable given the input pair of texts. GLMs are convenient here for at least two reasons:\n",
    "    \n",
    "1. The response variable is ordinal.\n",
    "2. Feature functions give us flexibility in how we encode 2 pieces of text together.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on model family\n",
    "\n",
    "As always, let's start by inspecting the *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_training_dicts = json.load(urllib.request.urlopen(\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/datasets/SICK_train.1.json\"))\n",
    "print(f\"We have {len(sts_training_dicts)} training data points. Example:\\n\", sts_training_dicts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the *marginal* distribution of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist([xy['original_score'] for xy in sts_training_dicts], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems 'unbalanced', with text pairs that are more similar than dissimilar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have multiple responses per text pair (in the field 'sampled_scores'), we can gain some information about the shape of the conditional distributions. Let's check the first few instances in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,5, sharex='row', figsize=(15,16))\n",
    "for i, xy in enumerate(sts_training_dicts[:20]):\n",
    "    _ = axs[i//5,i%5].set_title(\"\\n\".join(textwrap.wrap(f\"{xy['a']} ||| {xy['b']}\", 30)))\n",
    "    _ = axs[i//5,i%5].hist(xy['sampled_scores'], bins=3)\n",
    "    _ = axs[i//5,i%5].plot(xy['sampled_scores'], np.abs(np.random.normal(0, 0.2, size=len(xy['sampled_scores']))), '*', color='red')\n",
    "fig.tight_layout(w_pad=1, h_pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shapes of these distributions and the fact that the data is ordinal and discrete (integer) we need to choose a pmf that exhibit a mode that can be parameterised away from 0 (this excludes the Geometric). As the support is finite (0-100) we could consider the Binomial, but, for this exercise, we will instead use the *Poisson*.\n",
    "\n",
    "Our choice is mostly didatic, we pick the Poisson because ordinal regression (which this is an example of) is *often* performed with a Poisson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now prepare the data for JAX.\n",
    "\n",
    "For this task we will use training triples made of $x=(a,b)$ and $y$ will be the field 'original_score'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_training = []\n",
    "for xy in sts_training_dicts:\n",
    "    sts_training.append((xy['a'], xy['b'], xy['original_score']))\n",
    "    \n",
    "# This is what a data point looks like    \n",
    "print(\"We haven {len(sts_training)} training triples, example:\\n\", sts_training[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat this for the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_dev_dicts = json.load(urllib.request.urlopen(\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/datasets/SICK_test.1.json\"))\n",
    "sts_dev = []\n",
    "for xy in sts_dev_dicts:\n",
    "    for y in xy['sampled_scores']: # for dev and test we will use all the sampled scores (this is so we can see variability in human ratings)\n",
    "        sts_dev.append((xy['a'], xy['b'], y))\n",
    "print(f\"We have {len(sts_dev)} dev triples, example:\\n\", sts_dev[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's also prepare the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_test_dicts = json.load(urllib.request.urlopen(\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/datasets/SICK_test.2.json\"))\n",
    "sts_test = []\n",
    "for xy in sts_test_dicts:\n",
    "    for y in xy['sampled_scores']:\n",
    "        sts_test.append((xy['a'], xy['b'], y))        \n",
    "        \n",
    "print(f\"We have {len(sts_test)} test triples, example:\\n\", sts_test[0])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature function\n",
    "\n",
    "Our feature function will be very simple, we will compute $\\mathbf h(x) = |\\mathrm{bow}(a) - \\mathrm{bow}(b)|$. That is, the absolute difference between the bag-of-words encoding of the two sentences.\n",
    "\n",
    "For that, we start with a bag-of-words encoding. The CountVectorizer is a very efficient way to obtain those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_ff = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(ngram_range=(1,1), min_df=3)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# we train it on the concatenation of all sentences in the training data\n",
    "bow_ff.fit([xy[0] for xy in sts_training] + [xy[1] for xy in sts_training])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can compute our actual feature function as defined previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, x is a pair of sentences (a, b)\n",
    "\n",
    "# encode part a of x\n",
    "training_ha = bow_ff.transform([xy[0] for xy in sts_training])\n",
    "# encode part b of x\n",
    "training_hb = bow_ff.transform([xy[1] for xy in sts_training])\n",
    "# encode x as |bow(a)-bow(b)|\n",
    "training_h = np.abs(training_ha-training_hb)\n",
    "# grab the responses\n",
    "training_y = np.array([xy[2] for xy in sts_training])\n",
    "\n",
    "# do the same for dev set\n",
    "# encode a\n",
    "dev_ha = bow_ff.transform([xy[0] for xy in sts_dev])\n",
    "# encode b\n",
    "dev_hb = bow_ff.transform([xy[1] for xy in sts_dev])\n",
    "# compute absolute difference\n",
    "dev_h = np.abs(dev_ha-dev_hb)\n",
    "# grab responses\n",
    "dev_y = np.array([xy[2] for xy in sts_dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-2'> **Graded Exercise 2 - Poisson regression** </a>\n",
    "\n",
    "You will now design a Poisson GLM. There are striking similarities with the Bernoulli GLM, and our class structure emphasises that, but be careful to implement a **Poisson** regressor here, and not just another Bernoulli model.\n",
    "\n",
    "1. Complete the PoissonRegressor class below, it only lacks implementation for two methods, namely, `activation` and `log_p`. To help you get this right, we have coded a few test cases that your implementation should pass. \n",
    "    \n",
    "2. Once you have it, train the model for at least 40 epochs (more is also fine) with regularisation (use 1e-4). As always, plot the information logged during training. Finally, report performance on the test set (in terms of mean absolute error).\n",
    "    \n",
    "3. For your trained model, inspect the features that received higher positive weight and higher negative weight. See if you can find clear cases of word pairs that likely drag similarity up or down for their semantic content is same/different.\n",
    "\n",
    "4. Use your model to predict Poisson distributions for at least the first 16 data points in the dev set. For each data point, plot a histogram of samples from the Poisson distribution predicted by the model, also plot the observed values for $y$ that are recorded in the data set (use red crosses as we did in the beginning of the section). Remember that once you've obtained the Poisson parameter from your GLM, you can use `np.random.poisson` to obtain samples, for example. \n",
    "    \n",
    "5. Using the plotting technique above, find 5 examples where the trained model does not perform well (the observations are far from the predictions). Explain why you interepret these examples as failures. Finally, speculate about what might have caused the problem (e.g., do you think it's a problem with the feature function, or is it a problem with the choice of distribution? explain your thoughts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "class PoissonRegressor(GLM1):\n",
    "\n",
    "    def __init__(self, feature_function, seed=0):\n",
    "        super().__init__(feature_function, seed=seed)\n",
    "        \n",
    "    def activation(self, linear_predictor):\n",
    "        \"\"\"In Poisson regression we constrain the linear predictor to being strictly positive, which can be done with exp or softplus, for example\"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")        \n",
    "\n",
    "    def decide(self, g):        \n",
    "        \"\"\"For the Poisson, let's decide using the mode, which is obtained via floor\"\"\"\n",
    "        return jnp.floor(g)\n",
    "\n",
    "    def log_p(self, targets, g):\n",
    "        \"\"\"\n",
    "        We use JAX scipy to return the log pmf of the Poisson\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")        \n",
    "\n",
    "    def validate(self, inputs, targets, *, w, b):\n",
    "        y_pred = self.predict(inputs, w=w, b=b)\n",
    "        return mean_absolute_error(targets, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cases for part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrCLTR-vsaXU"
   },
   "outputs": [],
   "source": [
    "poi_reg = PoissonRegressor(bow_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OQk08WbhMCP"
   },
   "outputs": [],
   "source": [
    "assert jnp.alltrue(poi_reg.activation(np.random.uniform(size=1000)) > 0.), \"The Poisson parameter should be *strictly* positive (larger than 0)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09S8C-FxiLhz"
   },
   "outputs": [],
   "source": [
    "test_case_targets = np.array([0, 1, 2, 3, 4])\n",
    "test_case_rates = np.array([1., 1., 2., 2., 3.])\n",
    "test_case_logpmf = np.array([-1.0000005, -1.       , -1.3068521, -1.7123183, -1.7836056])\n",
    "assert jnp.allclose(poi_reg.log_p(test_case_targets, test_case_rates), test_case_logpmf, 1e-3), \"Did you implement the correct expression for Poisson's log pmf? You can use JAX scipy code for that, rather than write it from scratch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper code for part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOOmvy5isZmV"
   },
   "outputs": [],
   "source": [
    "# This should take a couple of minutes (the progress bar should give you a reliable estimate)\n",
    "# for a faster run reduce num_epochs or increase batch_size \n",
    "# of course, do expect changes in performance\n",
    "# you can use lr0 and l2weight to affect the optimiser and the objective function\n",
    "(poi_w, poi_b), poi_log = train_model(\n",
    "    poi_reg,\n",
    "    training_h,\n",
    "    training_y,\n",
    "    dev_h, \n",
    "    dev_y,\n",
    "    lr0=5.,\n",
    "    l2weight=1e-4, # larger means more regularisation\n",
    "    batch_size=100, # use more for faster training (if you are on GPU, this can be bigger)\n",
    "    num_epochs=30, # use more to find better models\n",
    "    rng=np.random.RandomState(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tCONTRIBUTE YOUR SOLUTION/DISCUSSION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-6'></a> **Ungraded Exercise 6 - Further analysis**\n",
    "\n",
    "The code below can be useful for some (optional) further analysis. We compute the proportion of red stars (observations) that fall within the blue curve (model samples) and vice versa. This gives us another notion of quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualise the results is a scatter of the lower (or upper) observation vs the lower (or upper) model sample. Ideally these quantities correlate perfectly. The more linear and diagonal the scatter looks, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(data_dicts, models):\n",
    "    \"\"\"\n",
    "    For a collection of models, plot \n",
    "        * proportion of model samples that fall within data samples\n",
    "        * proportion of data samples that fall within model sampels\n",
    "        * scatter plot of data sampels vs model samples\n",
    "        \n",
    "    data_dicts: list of data points (each a dict)\n",
    "    models: list of 4-tuples, each tuple is \n",
    "        (poisson regressor, trained weights, trained bias, a string label to display in plots)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(len(models), 3, sharey='col', sharex='col', figsize=(16, 4 * len(models)))    \n",
    "    if len(models) == 1:\n",
    "        ax = ax[None,:]       \n",
    "    \n",
    "    for j, (reg, weights, bias, name) in enumerate(models):\n",
    "        \n",
    "        pred_in_obs = []\n",
    "        obs_in_pred = []\n",
    "        lower = []\n",
    "        upper = []\n",
    "    \n",
    "        for i, xy in enumerate(data_dicts):\n",
    "            a, b, ys = xy['a'], xy['b'], xy['sampled_scores']\n",
    "            h = np.abs(reg.feature_function.transform([a]) - reg.feature_function.transform([b]))    \n",
    "            rate = reg.g(h.toarray(), w=weights, b=bias).item()    \n",
    "            # 90% central interval    \n",
    "            preds = np.random.poisson(rate, size=100)\n",
    "            central = sorted(preds)[5:95]\n",
    "            sortedys = sorted(ys)\n",
    "            pred_in_obs.append(sum(sortedys[0] <= y <= sortedys[-1] for y in preds) / len(preds))\n",
    "            obs_in_pred.append(sum(central[0] <= y <= central[-1] for y in ys) / len(ys))\n",
    "            lower.append([sortedys[0], central[0]])\n",
    "            upper.append([sortedys[-1], central[-1]])\n",
    "\n",
    "        _ = ax[j,0].hist(pred_in_obs, bins=10)\n",
    "        _ = ax[j,0].set_xlabel(\"proportion of pred y within obs\")\n",
    "        _ = ax[j,1].hist(obs_in_pred, bins=10)\n",
    "        _ = ax[j,1].set_xlabel(\"proportion of obs y within 90% central preds\")\n",
    "        _ = ax[j,0].set_ylabel(f\"{name}\")\n",
    "        \n",
    "        _ = ax[j, 2].scatter([pair[0] for pair in lower], [pair[1] for pair in lower], alpha=0.4, marker='<', label='lower')\n",
    "        _ = ax[j, 2].scatter([pair[0] for pair in upper], [pair[1] for pair in upper], alpha=0.6, marker='>', label='upper')\n",
    "        _ = ax[j, 2].plot(np.linspace(0, 100, 20), np.linspace(0, 100, 20), color='black', linestyle='--', label='ideal')\n",
    "        _ = ax[j, 2].set_xlabel(\"observations\", fontsize=14)\n",
    "        _ = ax[j, 2].set_ylabel(\"samples\", fontsize=14)\n",
    "        _ = ax[j, 2].set_xticks(ax[j, 2].get_xticks(), fontsize=14)\n",
    "        _ = ax[j, 2].set_yticks(ax[j, 2].get_yticks(), fontsize=14)\n",
    "        _ = ax[j, 2].legend(fontsize=14)\n",
    "        \n",
    "    fig.suptitle(\"Comparison: model and data overlap\")\n",
    "    plt.show()\n",
    "    \n",
    "    fig.tight_layout(h_pad=1, w_pad=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, we can train various models (using different amount of data) and compare them \n",
    "trained_models = []\n",
    "\n",
    "for pct in [0.05, 0.1, 0.2, 0.5]:\n",
    "    reg = PoissonRegressor(bow_ff)\n",
    "    # This should take a couple of minutes (the progress bar should give you a reliable estimate)\n",
    "    # for a faster run reduce num_epochs or increase batch_size \n",
    "    # of course, do expect changes in performance\n",
    "    # you can use lr0 and l2weight to affect the optimiser and the objective function\n",
    "    (reg_w, reg_b), reg_log = train_model(\n",
    "        reg,\n",
    "        training_h[:int(len(sts_training) * pct)],\n",
    "        training_y[:int(len(sts_training) * pct)],\n",
    "        dev_h, \n",
    "        dev_y,\n",
    "        lr0=5.,\n",
    "        l2weight=1e-4, # larger means more regularisation\n",
    "        batch_size=100, # use more for faster training (if you are on GPU, this can be bigger)\n",
    "        num_epochs=20, # use more to find better models\n",
    "        rng=np.random.RandomState(42),\n",
    "    )\n",
    "    trained_models.append((reg, reg_w, reg_b, f\"{pct*100}% data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(\n",
    "    sts_dev_dicts, \n",
    "    trained_models \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": false,
   "name": "T3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
