{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open this notebook on Colab](https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "jzvWfS-ELNzE",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Guide\n",
    "\n",
    "* Before working on this tutorial, you should have worked through the [introduction to PyTorch](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n",
    "* Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
    "* Note that, as always, the notebook contains a condensed version of the theory We recommend you read the theory part before the LC session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "KqR7WUDXLeME",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ILOs\n",
    "\n",
    "After completing this lab you should be able to\n",
    "\n",
    "* develop NGram LMs and autoregressive LMs in Python (and PyTorch)\n",
    "* estimate parameters of LMs via MLE\n",
    "* evaluate LMs intrinsically in terms of perplexity\n",
    "* evaluate LMs statistically in terms of properties of generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "YBR2bPwLL9gj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## General Notes\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$.\n",
    "* Use python3.\n",
    "* Use Torch\n",
    "* To have GPU support run this notebook on Google Colab (you will find more instructions later).\n",
    "\n",
    "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up.\n",
    "\n",
    "If you need a short introduction to PyTorch [check this tutorial](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "CqDZh0QJJsOu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "toc"
    ]
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Topics \n",
    "\n",
    "* [Data](#sec:Data)\n",
    "\t* [Word segmentation and tokenisation](#sec:Word_segmentation_and_tokenisation)\n",
    "\t* [Data Loader](#sec:Data_Loader)\n",
    "* [Language Models](#sec:Language_Models)\n",
    "\t* [Neural NGramLM](#sec:Neural_NGramLM)\n",
    "\t* [Autoregressive LM](#sec:Autoregressive_LM)\n",
    "* [Evaluation](#sec:Evaluation)\n",
    "* [Training](#sec:Training)\n",
    "* [Demo](#sec:Demo)\n",
    "* [Experiment](#sec:Experiment)\n",
    "\n",
    "\n",
    "### Table of ungraded exercises\n",
    "\n",
    "1. [Familiarise yourself with BPE tokenisation](#ungraded-1)\n",
    "1. [Probability of sentence](#ungraded-2)\n",
    "1. [BaseLM class](#ungraded-3)\n",
    "1. [NGramLM class](#ungraded-4)\n",
    "1. [AutoregressiveLM class](#ungraded-5)\n",
    "1. [Training loop](#ungraded-6)\n",
    "\n",
    "\n",
    "### Table of graded exercises\n",
    "\n",
    "*Important:* The grader may re-run your notebook to investigate its correctness, but you must upload your notebook with the cells already run and make sure that all your answers are visible without the need to re-run the notebook.\n",
    "\n",
    "\n",
    "Exercises have equal weights.\n",
    "\n",
    "\n",
    "1. [Comparison](#graded-1)\n",
    "1. [Analysis](#graded-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "fVnfg0kMLrsi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "QV4oRuW-XYED",
    "outputId": "edc303b0-aa88-479e-880a-7bed47f5ed4a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install sentencepiece\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "LzjCfsJDNL1B",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "B8qybX6RNDKh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Data'></a>\n",
    "# Data\n",
    "\n",
    "In this tutorial we will develop models of text generation. So our data for this tutorial will be collections of sentences, or *corpora*.\n",
    "\n",
    "To train a good language model we need _a lot_ of data. Unfortunately, we cannot learn a good language model in the time you have for a tutorial in this course. Hence, we will use a dataset that's not too large and where sentences are rather simple (so we can learn to reproduce enough of their properties with relatively little data).\n",
    "\n",
    "We will be using a portion of the stanford natural language inference (SNLI) dataset. Sentences in this dataset are usually short and have simple syntactic structure, they typically describe a simple sitatuion (e.g., a person taking their pet on a walk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "y45nRUu933x7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def download_snli(version='simple', SEP=\"[SEP]\", CLS=\"[CLS]\"):\n",
    "    \"\"\"\n",
    "    Download the dataset from nlp.stanford.ed\n",
    "    and split it into training, dev and test sets.\n",
    "    \n",
    "    :returns: a dict with three lists of sentences\n",
    "        ('training', 'dev' and 'test)\n",
    "        each sentence is a python string\n",
    "    \"\"\"\n",
    "    if not os.path.isdir('snli_1.0'):\n",
    "        url = \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\"\n",
    "        print(f\"Downloading SNLI dataset from {url}\")\n",
    "        extract_dir = \"./\"\n",
    "        zip_path, _ = urllib.request.urlretrieve(url)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "            f.extractall(extract_dir)\n",
    "\n",
    "    data = {\n",
    "        'training': [],\n",
    "        'dev': [],\n",
    "        'test': []\n",
    "    }\n",
    "    files = {\n",
    "        'training': 'snli_1.0/snli_1.0_train.jsonl',\n",
    "        'dev': 'snli_1.0/snli_1.0_dev.jsonl',\n",
    "        'test': 'snli_1.0/snli_1.0_test.jsonl',\n",
    "    }\n",
    "\n",
    "    for part, fname in files.items():\n",
    "        for line in open(fname, 'r'):\n",
    "            d = json.loads(line)\n",
    "            s1 = d['sentence1']\n",
    "            s2 = d['sentence2']\n",
    "\n",
    "            if d['gold_label'] in ['contradiction', 'neutral', 'entailment']:\n",
    "                if version == 'simple':\n",
    "                    data[part].append(f\"{s2}\")\n",
    "                elif version == 'control':\n",
    "                    data[part].append(f\"{s1} {SEP} {d['gold_label']}: {s2}\")\n",
    "                elif version == 'classify':\n",
    "                    data[part].append(f\"{s1} {SEP} {s2} {CLS} {d['gold_label']}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown option: {version}\")\n",
    "\n",
    "        data[part] = np.array(data[part])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The first time we need to download the data, this may take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "wItJEBdj40-4",
    "outputId": "e271feb3-76d4-4e01-d12b-e7d760008f4e",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "snli = download_snli('simple')  # do not change the version argument\n",
    "print(f\"Training: {snli['training'].shape}\\nDev: {snli['dev'].shape}\\nTest:{snli['test'].shape}\")\n",
    "print(\"Some examples from the training data:\\n\")\n",
    "for i in range(10):\n",
    "    print(snli['training'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "a6K-8bDy5Va3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will work with two versions of this dataset: a demo version (made of only a subset of the dataset) and the complete version (which we will use in the final experiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "UfPOkYJq5cIU",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we subset the dataset by selecting (at random)\n",
    "#  50k sentences for training\n",
    "#  500 sentences for development\n",
    "#  and 1000 sentences for test\n",
    "demo_training_subset = np.random.RandomState(42).choice(len(snli['training']), replace=False, size=50000)\n",
    "demo_dev_subset = np.random.RandomState(42).choice(len(snli['dev']), replace=False, size=500)\n",
    "demo_test_subset = np.random.RandomState(42).choice(len(snli['test']), replace=False, size=1000)\n",
    "demo = {\n",
    "    'training': snli['training'][demo_training_subset],\n",
    "    'dev': snli['dev'][demo_dev_subset],\n",
    "    'test': snli['test'][demo_test_subset],\n",
    "}\n",
    "len(demo['training']), len(demo['dev']), len(demo['test'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "x6hEpj0DOzxO",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Word_segmentation_and_tokenisation'></a>\n",
    "## Word segmentation and tokenisation\n",
    "\n",
    "Our models of text generation are probability distributions over finite-length sequences of discrete symbols. These discrete symbols are what we call *tokens*. The **vocabulary** of the model is therefore the finite set of known tokens which it can use to make sequences.\n",
    "\n",
    "We are interested in a special type of sequence, namely, sentences. Sentences are typically made of linguistic units that we call words. The linguistic notion of *word* is much too complex for our models. In practice, we use a data-driven and computationally convenient notion instead.\n",
    "\n",
    "In this tutorial we will work with tokens that are subword units obtained via a compression algorithm known as *byte pair encoding* (BPE). From the [original paper](http://www.aclweb.org/anthology/P16-1162): \n",
    "```\n",
    "Byte Pair Encoding (BPE) (Gage, 1994) is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. We adapt this algorithm for word segmentation. Instead of merging frequent pairs of bytes, we merge characters or character sequences.\n",
    "```\n",
    "You can optionally check the original paper for more detail.\n",
    "\n",
    "We will use a package called `sentencepiece` that implements an efficient BPE tokeniser (and word segmenter) for us. This tokeniser is language independent, it learns a vocabulary of subword units from a corpus of sentences (without the need for any special annotation). Because this is based on a compression algorithm, we can choose the level of compression, that is, we can choose the number of unique symbols that we want to have in the vocabulary, the BPE algorithm will find what collection of tokens  best describes the corpus at the given budget.\n",
    "\n",
    "After trained, we can use the BPE model to tokenise and detokenise sentences for us deterministically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "3q9Vx37hT_1l",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "2pEGOnBb5xRl",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This helper function trains a BPE model for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "9cMWg7AnUE1E",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_vocabulary(corpus, vocab_size, pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    \"\"\"\n",
    "    Return a BPE model as implemented by the sentencepiece package.\n",
    "\n",
    "    :param corpus: an iterable of sentences, each sentence is a python string\n",
    "    :param vocab_size: number of unique symbols\n",
    "    :param pad_id: identifier of the special PAD symbol (for batching sequences of different length)\n",
    "    :param bos_id: identifier of the special BOS symbol (which starts an input sentence)\n",
    "    :param eos_id: identifier of the special EOS symbol (which ends an output sentence)\n",
    "    :param unk_id: identifier of the special UNK symbol (which is used to process unseen words in the future)\n",
    "    \"\"\"\n",
    "    proto = io.BytesIO()\n",
    "\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        sentence_iterator=iter(corpus),\n",
    "        model_writer=proto,\n",
    "        vocab_size=vocab_size,\n",
    "        pad_id=pad_id,\n",
    "        bos_id=bos_id,\n",
    "        eos_id=eos_id,\n",
    "        unk_id=unk_id,\n",
    "    )\n",
    "\n",
    "    return spm.SentencePieceProcessor(model_proto=proto.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here we fit a BPE tokeniser (this could take a moment), for the demo we aim at a rather small vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "--9I-m4sXO6M",
    "outputId": "06e372c0-9576-4ef2-deb9-78afc4e40f0f",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'] = fit_vocabulary(demo['training'], vocab_size=1000)\n",
    "demo['tokenizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "WP54PJmc7ci6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There are some special symbols in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "mADoek7gv0mQ",
    "outputId": "0956585c-9972-4089-e01f-5dc1afb5c016",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].pad_id(), demo['tokenizer'].bos_id(), demo['tokenizer'].eos_id(), demo['tokenizer'].unk_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "fE0kDLoU7sw_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "And get to decide the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "fwDYZnQb7uzK",
    "outputId": "ad68c40f-dcce-4d8f-c910-da5978ceec56",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "aQwWTQhQ7v7C",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "And see how we can use this object to tokenize and detokenize text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "uTGnu6Ce6X_6",
    "outputId": "61ef2b4d-f941-438a-8c9c-4c0bb853a46d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_str = demo['training'][1]\n",
    "print(example_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "uLsaq2ht6bMp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The function `encode` can be used to tokenize a string into a list of tokens (each a string). To be able to read the output, you need to use the argument `out_type=str`, otherwise the tokenizer will convert the tokens to numerical codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "82Ma5982633i",
    "outputId": "a80481d3-16eb-46f7-c28a-e63d9f7212b2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].encode(example_str, out_type=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "8gMpdG7N6wAp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The `decode` method can map the tokens back to original form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "editable": true,
    "id": "VrR9lypI6wJ3",
    "outputId": "faf70614-f7f0-4c69-b655-02fd04b596c8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].decode(demo['tokenizer'].encode(example_str, out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "5mmZQwUk6pYL",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Without `out_type=str` we get a sequence of codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "PlDeuNoIV43l",
    "outputId": "120e0ed9-f354-484d-f50c-6beee8599baf",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].encode(example_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "coFQMcyY6t-_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "And, of course, `decode` can map it back to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "editable": true,
    "id": "Po5bVZndWLZ8",
    "outputId": "0c25c117-c206-4b81-de1e-87fb9e0dbc4d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].decode(demo['tokenizer'].encode(example_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "SOgw51RB7Lup",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There are two main advantages to this strategy for tokenization:\n",
    "\n",
    "1. we control the vocabulary size (which helps us control memory usage)\n",
    "2. oftentimes unseen words are made of a combination of existing subword units, so we can deal with more words than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "LitMBNpm7awk",
    "outputId": "efbbe30e-31a6-4171-f156-f00e3016aa89",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].encode(\"This is a tutorial within Natuurlijke Talmodellen en Interfaces at the UvA.\", out_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "po_HqEgsmTFl",
    "outputId": "525f405b-7b45-475e-8e09-bd1ba406e2b6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].encode(\"This is a tutorial within Natuurlijke Talmodellen en Interfaces at the UvA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "BBZfphivEmDE",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You can also preprocess whole batches of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "vw2g0klsEo5v",
    "outputId": "854ac360-a0c4-497d-d45f-89b579377992",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].encode([\"This is a sentence.\",  \"And this is another\"], out_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "NCVRuWpfEvKZ",
    "outputId": "0d476199-a96b-40ac-a8ae-80175f050bb7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['tokenizer'].decode(demo['tokenizer'].encode([\"This is a sentence.\",  \"And this is another\"], out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-1'></a> **Ungraded Exercise 1 - Familiarise yourself with BPE tokenisation**\n",
    "\n",
    "Play a bit with the tokenizer object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "BZVMkq0GZOqc",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Data_Loader'></a>\n",
    "## Data Loader\n",
    "\n",
    "As we did in the PyTorch tutorial, we will create a `Dataset` object and a `DataLoader` for batching data points. In this tutorial, we batch sentences for language modelling.\n",
    "\n",
    "The typical way to do this is to re-interpret each datapoint (a sentence `x`) as two sequences: an _input_ sequence `x_in` and an _output_ sequence `x_out`, where for any position `i` of the output sequence, the input sequence until that same position contains _past information_.\n",
    "\n",
    "Here's an example for `x=(a, person, walking, around)`:\n",
    "```\n",
    "i     : 1    2       3        4        5\n",
    "x_in  : BOS  a       person   walking  around\n",
    "x_out : a    person  walking  around   EOS\n",
    "```\n",
    "\n",
    "See how we use BOS and EOS symbols to make sure that `x_in` and `x_out` have the same length, while making it the case that if we read the sequences from left-to-right, `x_in` is always one word behind `x_out`, this is important for a generative language model, since it conditions on historical information (relative to any one position) to predict a cpd from where, by assumption, we draw the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "EPBI2HUXYkbz",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class Corpus(Dataset):\n",
    "    \"\"\"\n",
    "    A torch dataset for language modelling.\n",
    "\n",
    "    A common API for generative models is to imagine a sequence x as two sequences of equal length:\n",
    "        x_in is a tokenized version of x, where we have BOS at the beginning;\n",
    "        x_out is a tokenized version of x, where we have EOS at the end.\n",
    "    Here's an example\n",
    "        x = \"cats, dogs and rabbits\"\n",
    "    becomes\n",
    "        x_in  = [id(BOS),  id(cats),  id(,),     id(dogs),  id(and),      id(rabbits)]\n",
    "        x_out = [id(cats), id(,),     id(dogs),  id(and),   id(rabbits),  id(EOS)]\n",
    "    This way you can see that for any position i in x_out we would condition on x_in up until that same position.\n",
    "\n",
    "    (Of course, there are other ways to code a Corpus Dataset, one could even work with a single tokenized view of x.\n",
    "        We teach it this way because it's a rather common way to do it, and you will find public APIs that do the same.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus, tokenizer, max_length=None):\n",
    "        \"\"\"\n",
    "        In PyTorch we better always manipulate numerical codes, rather than text.\n",
    "        So, our Corpus object will contain a tokenizer that converts words to codes.\n",
    "\n",
    "        :param corpus: a list of sentences, each a string\n",
    "        :param tokenizer: a BPE tokenizer from sentencepiece\n",
    "        :param max_length: if specified, we discard any sentence that's longer than max_length after tokenization\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        if max_length is None:\n",
    "            self.corpus = list(corpus)\n",
    "        else:\n",
    "            self.corpus = [seq for seq in corpus if len(self.tokenizer.encode(seq)) <= max_length]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of sentences in the corpus\"\"\"\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return an (x_in, x_out) pair made of BPE tokenized version of corpus[idx]:\n",
    "            x_in is tokenized with add_bos=True\n",
    "            x_out is tokenized with add_eos=True\n",
    "        \"\"\"\n",
    "        x_in = self.tokenizer.encode(self.corpus[idx], add_bos=True)\n",
    "        x_out = self.tokenizer.encode(self.corpus[idx], add_eos=True)\n",
    "        return x_in, x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "fJ02JHwwc6Gv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "6z7xQXhHpSlX",
    "outputId": "067ff8c0-9816-4d3a-b27f-7e42701ff6a4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo['training_tok'] = Corpus(demo['training'], demo['tokenizer'], max_length=50)\n",
    "demo['dev_tok'] = Corpus(demo['dev'], demo['tokenizer'], max_length=50)\n",
    "demo['test_tok'] = Corpus(demo['test'], demo['tokenizer'], max_length=100)\n",
    "print(\"Size of tokenized corpora:\", len(demo['training_tok']), len(demo['dev_tok']), len(demo['test_tok']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FgwGUkWQZtuq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "When we manipulate sequences of variable length, we need to \"pad\" them all to the same length. That's because to batch them using tensors they need to look like as if they did have the same length.\n",
    "\n",
    "We do that with a special symbol that will get ignored later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "IpR_1AkiadQo",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_to_longest(sequences, pad_id=0):\n",
    "    \"\"\"\n",
    "    Take a list of coded sequences and returns a torch tensor where\n",
    "    every sentence has the same length (by means of using PAD tokens)\n",
    "\n",
    "    :param sequences: these are tokenized sequences of variable length\n",
    "    :param pad_id: the id of the PAD symbol\n",
    "\n",
    "    :return: batch of padded sequences\n",
    "    \"\"\"\n",
    "    longest = max(len(x) for x in sequences)\n",
    "    x = torch.tensor([x + [pad_id] * (longest - len(x)) for x in sequences])\n",
    "\n",
    "    return x\n",
    "\n",
    "def pad_to_longest_paired(paired_sequences, pad_id=0):\n",
    "    \"\"\"\n",
    "    Take a list of coded sequence pairs and returns 2 torch tensors where\n",
    "        every sentence has the same length (by means of using PAD tokens)\n",
    "\n",
    "    :param paired_sequences: these are pairs of tokenized sequences of variable length\n",
    "    :param pad_id: the id of the PAD symbol\n",
    "\n",
    "    :return: batch of padded sequences x_in, batch of padded sequences x_out\n",
    "    \"\"\"\n",
    "    seqs_in = pad_to_longest([x_in for x_in, x_out in paired_sequences])\n",
    "    seqs_out = pad_to_longest([x_out for x_in, x_out in paired_sequences])\n",
    "    return seqs_in, seqs_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Cetno7rTaKPI",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "See what this does to the first few sentences in the batch:\n",
    "- in `x_in` they will start with BOS (1) and end with as many PADs (0) as necessary\n",
    "- in `x_out` they will end with EOS (2) followed by as many PADs (0) as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "B7zeMNPgaEKZ",
    "outputId": "d5b0dd9e-46b7-4b2e-a19e-9b8823eb950a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_to_longest_paired([demo['training_tok'][0], demo['training_tok'][1], demo['training_tok'][2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "-_duaGn_aOhx",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now that we can convert batches of sentences to codes and guarantee they have the same length, we can construct a data loader to create mini batches at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "3sHa1d5WpkdE",
    "outputId": "31d4c0b0-f3a4-4659-fb15-37ba23d9f756",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's make a dataloader with a single batch of two sentences\n",
    "toy_batcher = DataLoader([demo['training_tok'][0], demo['training_tok'][1]], batch_size=2, shuffle=True, collate_fn=pad_to_longest_paired)\n",
    "len(toy_batcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "mN-9ZMef3HfY",
    "outputId": "24735a64-518c-464c-e404-52884d3a1976",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, (x_in, x_out) in enumerate(toy_batcher, 1):\n",
    "    print(f\"Input batch {k}:\")\n",
    "    print(x_in)\n",
    "    print(f\"\\nOutput batch {k}:\")\n",
    "    print(x_out)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "KVKUr3UcybC6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Language_Models'></a>\n",
    "# Language Models\n",
    "    \n",
    "A language model (LM) is a **probability distribution over text**, where text is a finite sequences of symbols (e.g., words or subword units).\n",
    "    \n",
    "LMs can be used to generate text as well as to quantify a degree of naturalness (or rather, a degree of resemblance to training data) which is useful for example to compare and rank alternative pieces of text in terms of fluency.\n",
    "    \n",
    "To design an LM, we need to talk about units of text (e.g., documents, paragraphs, sentences) as outcomes of a random experiment, and for that we need random variables.\n",
    "    \n",
    "We will generally refer to the unit of text as a *sentence*, but that's just for convenience, you could design an LM over documents and very little, if anything, would change.  \n",
    "\n",
    "A **random sentence** is a finite sequence of symbols from the vocabulary of a given language. As a running example, we will refer to this language as English. The vocabulary of English will be denoted by $\\mathcal W$, a finite collection of unique symbols, each of which we refer to as a *word* (but in practice, these symbols are any unit we care to model). We will denote a random sentence by $X$, or more explicitly, by the random sequence $X = \\langle W_1, \\ldots, W_L \\rangle$. Here $L$ indicates the sequence length. Each word in the sequence is a random variable that takes on values in $\\mathcal W$. We will adopt an important convention, every sentence is a finite sequence that ends with a special symbol, the end-of-sequence (EOS) symbol.\n",
    "\n",
    "Formally, random sentences take on values in the set $\\mathcal W^*$ of all strings made of symbols in $\\mathcal W$, which is a set that does include an infinte number of valid English sentences (possibly not all English sentences, as our vocabulary may not be complete enough, but hopefully this space is still large enough for the LM to be useful) as well as an infinite number of sequences that are not valid English sentences.       \n",
    "    \n",
    "Part of the deal with a language model is to define and estimate a probability distribution that expresses a preference for sentences that are more likely to be accepted as English sentences. In practice an LM will prefer sentences that reproduce statistics of the observations used to estimate its parameters, whether these sentences will resemble English sentences or not will depend on how expressive the LM is, that is, whether or not the LM can capture patterns as complex as those arising from well-formed English (or whatever variant/register of English was observed during training).\n",
    "    \n",
    "**Notation guide** Some textbooks or papers use $W_1^L$ instead of $W_{1:L}$ for ordered sequences, both are clear enough, but we will use the notation adopted by the textbook, that is, $W_{1:L}$. The textbook uses $W_1\\cdots W_L$ (without commas) as another notation for ordered sequences, but we prefer to explicitly mark the sequence with angle brackets to avoid ambiguities, i.e., we prefer $\\langle W_1, \\ldots, W_L \\rangle$. For assignments, we will use the lowercase version of the letter that names the random variable: $w_{1:l} = \\langle w_1, \\ldots, w_l \\rangle$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "nuyxN3qpz-nw",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The following notational shortcuts are rather convenient:\n",
    "\n",
    "* we will often use $W_{1:L}$ for a random sentence instead of the longer form $\\langle W_1, \\ldots, W_L \\rangle$, and similarly for outcomes (i.e., $w_{1:l}$ instead of $\\langle w_1, \\ldots, w_l\\rangle$), but in the long form we shall never drop the angle brackets, as otherwise it's hard to tell that we mean an ordered sequence\n",
    "* we will use $W_{<i}$ (or $w_{<i}$ for an outcome) to denote the sequence of tokens that precedes the $i$th token, this sequence is empty $W_{<i} \\triangleq \\langle \\rangle$ for $i \\le 1$, for $i>1$ the sequence is defined as $W_{<i} \\triangleq \\langle W_1, \\ldots, W_{i-1}\\rangle$\n",
    "* sometimes it will be useful to find a more compact notation for $W_{<i}$, in those cases we refer to it as a *random history* and denote it by $H$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "P4xrSiKo0bcl",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "LMs can be described by a **generative story**, that is, a stochastic procedure that explains how an outcome $w_{1:l}$ is drawn from the model distribution. Though we may find inspiration in how we believe the data were generated, the generative story is not a faithful representation of any linguistic process, it is all but an abstraction that codes our own assumptions about the problem.\n",
    "\n",
    "The most general form this generative story can take, that is, the form with the least amount of assumptions, looks as follows:\n",
    "\n",
    "1. For each position $i$ of a sequence, condition on the history $h_i$ and draw the $i$th word $w_i$ with probability $P(W=w_i|H=h_i)$.\n",
    "2. Append $w_i$ to the end of the history: $h_i \\circ \\langle w_i \\rangle$\n",
    "2. Stop generating if $w_i$ is the EOS token, else repeat from (1).\n",
    "\n",
    "We say this procedure is very general because it is essentially just chain rule spelled out in English words, though here the order of enumeration is determined by the left-to-right order of tokens in an English sentence.\n",
    "\n",
    "\n",
    "Here is an example for a sequence of length $l=3$:\n",
    "\n",
    "$P_X(\\langle w_1, w_2, w_3 \\rangle) = P_{W|H}(w_1|\\langle \\rangle) P_{W|H}(w_2|\\langle w_1 \\rangle) P_{W|H}(w_3 |\\langle w_1, w_2 \\rangle)$\n",
    "\n",
    "For our example sentence *He went to the store* this means:\n",
    "\n",
    "\\begin{align}\n",
    "P_X(\\langle \\text{He, went, to, the, store, EOS} \\rangle) &= P_{W|H}(\\text{He}|\\langle \\rangle) \\\\\n",
    "    &\\times P_{W|H}(\\text{went}|\\langle \\text{He} \\rangle) \\\\\n",
    "    &\\times P_{W|H}(\\text{to}|\\langle \\text{He}, \\text{went} \\rangle) \\\\\n",
    "    &\\times P_{W|H}(\\text{the}|\\langle \\text{He},  \\text{went}, \\text{to} \\rangle) \\\\\n",
    "    &\\times P_{W|H}(\\text{store}|\\langle \\text{He},  \\text{went}, \\text{to}, \\text{the} \\rangle) \\\\\n",
    "    &\\times P_{W|H}(\\text{EOS}|\\langle \\text{He},  \\text{went}, \\text{to}, \\text{the}, \\text{store} \\rangle)\n",
    "\\end{align}\n",
    "\n",
    "* where with some abuse of notation we use the words themselves as outcomes instead of their corresponding indices.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-2'></a> **Ungraded Exercise 2 - Probability of sentence**\n",
    "\n",
    "Write down the general rule for the probability $P_X$ of a sentence $w_{1:l}$. Don't forget to indicate the precise random variable associated with every distribution (that is, for example, $P_X(w_{1:l})$ and $P(X=w_{1:l})$ are correct while $P(w_{1:l})$ is incomplete)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "$P_X(w_{1:l}) = \\prod_{i=1}^{l}P_{W|H}(w_i|w_{<i})$\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "EysRkGqW1vDp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The LM above is just an abstraction, not a concrete implementation, think of it as a general template for building models.\n",
    "\n",
    "A concrete model design needs to specify the conditional probability distributions in the model, this is known as the **parameterisation** of the model, and an algorithm for parameter estimation.\n",
    "\n",
    "Next we work on a BaseLM class that captures the essence of an LM as well as on two specific parameterisations: an NGram LM and an autoregressive LM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "z_z8N_ZpTNMy",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_all(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "pf3iDZ-nhL9E",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as td\n",
    "import torch.optim as opt\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-3'></a> **Ungraded Exercise 3 - BaseLM class**\n",
    "\n",
    "The BaseLM class below is a generic template for building neural LMs.\n",
    "\n",
    "* It is a container for a tokenizer and an embedding layer as well as some essential LM functionality.\n",
    "* It contains the general API for assigning log probability to any batch of outcomes, for computing the model's loss on a batch of observed sentences, and for drawing samples through the generative story.\n",
    "* However, it lacks an implementation of the parameterisation of next-word cpds (this is something we will cover later).\n",
    "\n",
    "Study this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "HQsZsr9rv0mX",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Our BaseLM has access to a vocabulary (from a tokenizer) and an embedding layer.\n",
    "    It lacks a mechanism to encode histories and to parameterise next-word cpds,\n",
    "        these should be implemented in derived classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, embedding_dim: int, p_drop=0., w_drop=0.):\n",
    "        \"\"\"\n",
    "        tokenizer: an already trained BPE tokenizer\n",
    "        embedding_dim: dimensionality of word embeddings\n",
    "        p_drop: dropout rate for regular dropout\n",
    "        w_drop: dropout rate for word dropout\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pad = tokenizer.pad_id()\n",
    "        self.bos = tokenizer.bos_id()\n",
    "        self.eos = tokenizer.eos_id()\n",
    "        self.unk = tokenizer.unk_id()\n",
    "        self.p_drop = p_drop\n",
    "        self.w_drop = w_drop\n",
    "        self.embed = nn.Embedding(self.vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def device(self):\n",
    "        return self.embed.weight.device\n",
    "\n",
    "    def num_parameters(self, trainable_only=True):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the model\n",
    "\n",
    "        :param trainable_only: change to False to count all parameters (even those in frozen layers)\n",
    "        \"\"\"\n",
    "        if trainable_only:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters() if theta.requires_grad)\n",
    "        else:\n",
    "            return sum(np.prod(theta.shape) for theta in self.parameters())\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"\n",
    "        Parameterise the conditional distributions over\n",
    "            X[i] given history (i.e., given x[:i]) for i=1...I.\n",
    "\n",
    "        :param x_in: batched histories (i.e., sentences batched with add_bos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "\n",
    "        :return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement me!\")\n",
    "\n",
    "    def log_prob(self, x_in, x_out):\n",
    "        \"\"\"\n",
    "        Compute the log probability of each sentence in a batch.\n",
    "\n",
    "        :param x_in: batched histories (i.e., sentences batched with add_bos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "        :param x_out: batched output sequences (i.e., sentences batched with add_eos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "\n",
    "        :return: a batch of log probabilities with shape [batch_size]\n",
    "        \"\"\"\n",
    "        # The forward method returns the cpds for each step,\n",
    "        #  hence, we can assess log prob of the tokens in x_out\n",
    "        # [batch_size, max_length]\n",
    "        logp = self(x_in).log_prob(x_out)\n",
    "        # But the log prob of a sentence is the sum of the log probs of its tokens (assessed in context)\n",
    "        #  (and we need to leave the PAD tokens out, since they aren't part of the text)\n",
    "        # [batch_size]\n",
    "        logp = torch.where(x_out != self.pad, logp, torch.zeros_like(logp)).sum(-1)\n",
    "        return logp\n",
    "\n",
    "    def sample(self, batch_size=1, max_length=50, prompt=None):\n",
    "        \"\"\"\n",
    "        Draws a number of samples from the model, each sample is a complete sequence.\n",
    "        We impose a maximum number of steps, to avoid infinite loops.\n",
    "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
    "\n",
    "        :param batch_size: number of samples to draw\n",
    "        :param max_length: maximum number of tokens in a sample\n",
    "        :param prompt: sometimes we would like a generator to complete\n",
    "            already started generations, these already started generations are called \"prompts\".\n",
    "            For example \"When I went to\" is a prompt, which a model might complete as\n",
    "             \"When I went to the cinema last week.\"\n",
    "\n",
    "            When using prompts, the sampler obtains tokens from the prompt,\n",
    "                instead of actually sampling them, for as long as there\n",
    "                are valid tokens in the prompt.\n",
    "\n",
    "            To use prompts you should provide a tensor of size [batch_size, max_length]\n",
    "                containing the prompt sequences. Do not use BOS and do not use EOS.\n",
    "                Use PAD to pad the batch to max_length.\n",
    "\n",
    "            Here is an example that can be used to draw 2 samples, each containing at most 10 tokens,\n",
    "                the first being a completion for \"a nice little\" and the second being a completion for \"when I\":\n",
    "\n",
    "                prompt = [\n",
    "                    [id(a), id(nice), id(little)] + [id(PAD)] * 7,\n",
    "                    [id(when), id(I)] + [id(PAD)] * 8,\n",
    "                ]\n",
    "\n",
    "        \"\"\"\n",
    "        # Sampling correctly while performing batched computations is relatively tricky,\n",
    "        #  we implement a version of it here,\n",
    "        #  there are more efficient versions, but they are even trickier\n",
    "\n",
    "        # To understand the algorithm consider first study this example.\n",
    "\n",
    "        # Example: sampling with batch_size=1 and max_length=5\n",
    "\n",
    "        # each row below is a step in a for loop\n",
    "        # x_out starts with the output shape (batch_size, max_length), but it's all PAD\n",
    "        # x_in starts with BOS, then continues with tokens from x_out\n",
    "        #  but only grows one token per iteration\n",
    "        # at each iteration we sample a symbol *given* x_in as history\n",
    "        #  that symbol goes to the corresponding cell in x_out\n",
    "        # x_i is an auxiliary tensor, it has the shape of x_in\n",
    "        #  but we only care about the rightmost token (the \"next token\" to be filled in x_out),\n",
    "        #  once we have that symbol, we update x_out\n",
    "        # see the example below (in the illustration, we use ? to indicate cells\n",
    "        #  whose value we do not care to read):\n",
    "\n",
    "        # x_out                 ||| x_in                  ||| x_i\n",
    "        # ----------------------|||-----------------------|||---------------------\n",
    "        # PAD PAD  PAD PAD PAD  ||| BOS                   ||| the\n",
    "        # the PAD  PAD PAD PAD  ||| BOS the               ||| ?   cute\n",
    "        # the cute PAD PAD PAD  ||| BOS the cute          ||| ?   ?    dog\n",
    "        # the cute dog PAD PAD  ||| BOS the cute dog      ||| ?   ?    ?   EOS\n",
    "        # the cute dog EOS PAD  ||| BOS the cute dog PAD  ||| ?   ?    ?   ?   ?\n",
    "\n",
    "        if prompt is not None:\n",
    "            batch_size = prompt.shape[0]\n",
    "            if max_length > prompt.shape[1]:\n",
    "                # here we extend the prompts with a bunch of PAD tokens\n",
    "\n",
    "                # these are the missing PAD tokens\n",
    "                pads = torch.full((batch_size, max_length - prompt.shape[1]), self.pad, device=self.device())\n",
    "                # [batch_size, max_length]\n",
    "                prompt = torch.cat([prompt, pads], -1)\n",
    "            else:\n",
    "                max_length = prompt.shape[1]\n",
    "\n",
    "        # sampling discrete outcomes is not differentiable\n",
    "        # but that's not a problem because we never sample during training\n",
    "        # we only sample _after_ training\n",
    "        with torch.no_grad():\n",
    "            # Reserve memory for the samples (it's not important what symbol to use, I use PAD for clarity)\n",
    "            x_out = torch.full((batch_size, max_length), self.pad, device=self.device())\n",
    "            # We will be needing a batch of BOS symbols, so we can create an x_in tensor\n",
    "            #  for conditioning the next-word cpds\n",
    "            bos = torch.full((batch_size, 1), self.bos, device=self.device())\n",
    "\n",
    "            # Keeps track of which samples are complete (i.e., already include EOS)\n",
    "            complete = torch.zeros(batch_size, device=self.device())\n",
    "\n",
    "            for i in range(max_length):\n",
    "                # We condition on x_in and parameterise Categoricals per step\n",
    "                #  then sample tokens.\n",
    "                # This will sample all tokens (including tokens in the prefix),\n",
    "                #  but we are only interested in the 'current' one, which we use to udpate our\n",
    "                #  actual sample x_out\n",
    "\n",
    "                # [batch_size, current_length]\n",
    "                x_in = torch.cat([bos, x_out[:,:i]], 1)\n",
    "                # condition on x_in and draw the x_i\n",
    "                x_i = self(x_in).sample()[:, i]\n",
    "                if prompt is not None:  # rather than using the sample, we force the use of the tokens in the prompt\n",
    "                    x_i = torch.where(prompt[:, i] == self.pad, x_i, prompt[:, i])\n",
    "\n",
    "                # Here we update the current token to something freshly sampled\n",
    "                #  and also replace the token by 0 (pad) in case the sentence is already complete\n",
    "                x_out[:, i] = x_i * (1 - complete)\n",
    "\n",
    "                # Here we update the state of the sentence (i.e., complete or not).\n",
    "                complete = (complete.bool() + (x_i == self.eos)).float()\n",
    "\n",
    "            return x_out\n",
    "\n",
    "    def loss(self, x_in, x_out):\n",
    "        \"\"\"\n",
    "        Compute a scalar loss from a batch of sentences.\n",
    "        The loss is the negative log likelihood of the model estimated on a single batch:\n",
    "            - 1/batch_size * \\sum_{s} log P(x[s]|theta)\n",
    "\n",
    "        :param x_in: batched histories (i.e., sentences batched with add_bos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "        :param x_out: batched output sequences (i.e., sentences batched with add_eos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "\n",
    "        :returns: the average loss with shape []\n",
    "        \"\"\"\n",
    "        # We assess the (negative) log prob of each and every batched sentence\n",
    "        # [batch_size]\n",
    "        loss = - self.log_prob(x_in, x_out)\n",
    "        # and then return the average\n",
    "        return loss.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "ex-header": "NeuralNGramLM",
    "id": "Lx0KNq86diKB",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Later, we will need code to regularise our LMs, as we did in T4, we will use dropout and word dropout. While dropout is already supported by torch (`nn.Dropout`), word dropout needs to be implemented. Here we adapt the word dropout technique you saw in T4 slightly to avoid dropping the BOS token when it's there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "0ul4PiMmv0mW",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_dropout(x, pad, unk, rate=0., bos=True):  # this is a helper function we will use to specify a good encoder\n",
    "    \"\"\"\n",
    "    Neural networks have so many parameters that they tend to overfit.\n",
    "    The strategy we saw to counter overfitting in GLMs (i.e., L2 regularisation) isn't sufficient.\n",
    "    A strategy that's more effective for recurrent nets is something called \"word dropout\",\n",
    "        whereby we omit some words from the input at random. This tends to force the RNN\n",
    "        to learn generalisable features.\n",
    "\n",
    "    :param x: input sequence (batched sequences of token ids)\n",
    "    :param pad: pad idx\n",
    "    :param unk: unk idx\n",
    "    :param rate: rate at which we omit words\n",
    "        we omit a word by replacing its token id by that of the UNK token\n",
    "    :param bos: if True, position 0 of each sequence is treated as a BOS symbol,\n",
    "        hence, never perturbed\n",
    "    :return: a perturbed version of the input x, with same shape.\n",
    "    \"\"\"\n",
    "    if rate <= 0.:\n",
    "        return x\n",
    "    # 1 if valid\n",
    "    vmask = (x != pad).float()\n",
    "    # 1 if dropped\n",
    "    rmask = (torch.rand(x.shape, device=vmask.device) < rate).float()\n",
    "    # if a position is valid and should be dropped, we replace it by unk\n",
    "    # else, we leave it unchanged\n",
    "    x_ = torch.where(vmask + rmask == 2, torch.full_like(x, unk), x)\n",
    "    if bos:\n",
    "        x_[...,0] = x[...,0]\n",
    "    return x_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "JlqKGKLn1FSH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Neural_NGramLM'></a>\n",
    "## Neural NGramLM\n",
    "\n",
    "Our first stop is a historical stop. We meet the NGram LM, in particular,  [a neural NGram LM](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf).\n",
    "\n",
    "Just like the classic $n$-gram LM, a neural $n$-gram LM makes a conditional independence assumption to simplify the factors in the chain rule. Rather than storying the probabilities values of observed $(h, w)$ pairs, the neural model stores the parameters necessary to predict those probabilities by transformation of the concatenation of the embeddings ofof the words in the history. The pmf of the neural $n$-gram LM is defined as follows:\n",
    "\n",
    "\\begin{align}\n",
    "f(w_{1:l};\\theta) &= \\prod_{i=1}^l \\mathrm{Cat}(w_i|\\mathbf g(w_{i-n+1:i-1}; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf g$ is a neural network with parameters $\\theta$, it maps a tuple of words $w_{i-n+1:i-1}$, from a vocabulary of $V$ known words, to a $V$-dimensional probability vector.\n",
    "\n",
    "This neural network is typically implemented as follows:\n",
    "\n",
    "* embed each word in the $i$th history into a $D$-dimensional space: $\\mathbf e_j = \\mathrm{embed}_D(w_j; \\theta_{\\text{in}})$ for $i-n + 1 \\le j <i$;\n",
    "* concatenate the word embeddings for the words in the $i$th history: $\\mathbf u_i = \\mathrm{concat}(\\mathbf e_{i-n+1}, \\ldots, \\mathbf e_{i-1})$;\n",
    "* use a single-layer feed-forward NN to transform the history encoding $\\mathbf u_i$ into a vector of $V$ logits: $\\mathbf s_i = \\mathrm{ffnn}_V(\\mathbf u_i; \\theta_{\\text{out}})$;\n",
    "* use softmax to obtain probabilities for the possible words at the $i$th position: $\\mathbf g(w_{i-n+1:i-1}; \\theta) = \\mathrm{softmax}(\\mathbf s_i)$;\n",
    "* the parameters are $\\theta = \\theta_{\\text{in}} \\cup \\theta_{\\text{out}}$ where\n",
    "    * $\\theta_{\\text{in}}$ is an embedding matrix $\\mathbf E \\in \\mathbb R^{V\\times D}$;\n",
    "    * $\\theta_{\\text{out}}$ are the parameters of the FFNN (we use 2 hidden layers): $\\mathbf W^{[1]} \\in \\mathbb R^{H\\times (n-1)D}$ and $\\mathbf b^{[1]} \\in \\mathbb R^H$ are the parameters of the input-to-hidden layer, $\\mathbf W^{[2]} \\in \\mathbb R^{H\\times H}$ and $\\mathbf b^{[2]} \\in \\mathbb R^H$ are the parameters of the hidden-to-hidden layer, and $\\mathbf W^{[3]} \\in \\mathbb R^{V\\times H}$ and $\\mathbf b^{[3]} \\in \\mathbb R^V$ are the parameters of the hidden-to-output layer.\n",
    "\n",
    "\n",
    "Next we implement this in PyTorch using `torch.nn.Embedding` for the embedding layer and `torch.nn.Linear` for the affine transformations inside the FFNN. As non-linearity for hidden layers we will use ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-4'></a> **Ungraded Exercise 4 - NGramLM class**\n",
    "\n",
    "Study the NGramLM class below and complete its constructor (you need to specify the FFNN that predicts logits) and the forward method (you need to parameterise the Categorical cpds for each step). Use the tests available to check your implementation. When you are happy with it consult our solution. For exercises, use our own solution to avoid cascading errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class NGramLM(BaseLM):\n",
    "\n",
    "    def __init__(self, tokenizer, embedding_dim: int, hidden_size: int, ngram_size: int, p_drop=0., w_drop=0.):\n",
    "        \"\"\"\n",
    "        tokenizer: an already trained BPE tokenizer\n",
    "        embedding_dim: dimensionality of word embeddings\n",
    "        hidden_size: dimensionalitty of hidden layer in FFNN\n",
    "        ngram_size: size of NGram\n",
    "        p_drop: dropout rate for regular dropout\n",
    "        w_drop: dropout rate for word dropout\n",
    "        \"\"\"\n",
    "        super().__init__(tokenizer=tokenizer, embedding_dim=embedding_dim, p_drop=p_drop, w_drop=w_drop)\n",
    "        assert ngram_size > 1, \"This class expects at least ngram_size 2\"\n",
    "        self.ngram_size = ngram_size\n",
    "\n",
    "        # **EXERCISE** Implement the FFNN\n",
    "        \n",
    "        # This FFNN should map a concatenation of the embeddings\n",
    "        #  of the (ngram_size-1) tokens in the history\n",
    "        #  to a vector of self.vocab_size logits/scores.\n",
    "        # Use nn.Sequential(...) to specify this FFNN\n",
    "        self.logits_predictor = None\n",
    "\n",
    "    def make_ngrams(self, x_in):\n",
    "        \"\"\"\n",
    "        Return a batch of ngram histories for conditioning the next-word cpds.\n",
    "\n",
    "        Example with ngram_size=3:\n",
    "\n",
    "        x_in: \n",
    "        [\n",
    "            [BOS, what, a, nice, day, !  ],\n",
    "            [BOS, what, a, day,  !,   PAD],\n",
    "        ]\n",
    "\n",
    "        # here the ngrams associated with x_in\n",
    "        # it's also useful to remember what the corresponding\n",
    "        #  x_out would be (we indicate it after '#')\n",
    "        ngrams:\n",
    "        [\n",
    "            [\n",
    "                [BOS, BOS],   # what\n",
    "                [BOS, what],  # a\n",
    "                [what, a],    # nice\n",
    "                [a, nice],    # day\n",
    "                [nice, day],  # !\n",
    "                [day, !]      # EOS\n",
    "            ],\n",
    "            [\n",
    "                [BOS, BOS],   # what\n",
    "                [BOS, what],  # a\n",
    "                [what, a],    # day\n",
    "                [a, day],     # !\n",
    "                [day, !],     # EOS\n",
    "                [!, PAD]      # PAD (this position isn't real)\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        :param x_in: batched histories (i.e., sentences batched with add_bos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "        :return: batched ngram histories\n",
    "            with shape [batch_size, max_length, ngram_size - 1]\n",
    "        \"\"\"\n",
    "        x_shape = x_in.shape\n",
    "        # We might need to add more BOS symbols to x_in\n",
    "        # (remember, x_in already has one)\n",
    "        if self.ngram_size > 2:\n",
    "            # [batch_size, ngram_size - 2]\n",
    "            bos = torch.full((x_in.shape[0], self.ngram_size - 2), self.bos, device=x_in.device)\n",
    "            # [batch_size, max_length + ngram_size - 2]\n",
    "            _x = torch.cat([bos, x_in], 1)\n",
    "        else:\n",
    "            _x = x_in\n",
    "\n",
    "        # For each output step, we will have ngram_size - 1 inputs, so we collect those from x\n",
    "        # [batch_size, max_length, ngram_size - 1]\n",
    "        return torch.cat([_x.unsqueeze(-1)[:,i:i+self.ngram_size-1].reshape(x_shape[0], 1, -1) for i in range(x_shape[1])], 1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"\n",
    "        Parameterise the conditional distributions over\n",
    "            X[i] given history (i.e., given x[:i]) for i=1...I.\n",
    "\n",
    "        :param x_in: batched histories (i.e., sentences batched with add_bos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "\n",
    "        :return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.training:\n",
    "            x_in = word_dropout(x_in, self.pad, self.unk, self.w_drop, bos=True)\n",
    "\n",
    "        # The inputs to the FFNN are the ngram_size-1 tokens that precede each position\n",
    "        #  this helper code gathers all histories in a single tensor.\n",
    "        # Read the document of self.make_ngrams to understand it.\n",
    "        # [batch_size, max_length, ngram_size - 1]\n",
    "        inputs = self.make_ngrams(x_in)\n",
    "\n",
    "        # *EXERCISE*\n",
    "        # Now you should \n",
    "        # 1. Embed the input histories into a tensor of shape        \n",
    "        # [batch_size, max_length, ngram_size - 1, D]\n",
    "        # 2. Concatenate the D-dimensional vectors in the history \n",
    "        # [batch_size, max_length, (ngram_size - 1) * D]\n",
    "        # hint: check torch `reshape`\n",
    "        # 3. Map the concatenated encodings to vocab_size-dimensional logits\n",
    "        # [batch_size, max_length, V]\n",
    "        # 4. Return a batch of td.Categorical objects parameterised\n",
    "        # with the logits computed in step (3).\n",
    "        raise NotImplementedError(\"Complete this forward pass\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# *SOLUTION*\n",
    "class NGramLM(BaseLM):\n",
    "\n",
    "    def __init__(self, tokenizer, embedding_dim: int, hidden_size: int, ngram_size: int, p_drop=0., w_drop=0.):\n",
    "        \"\"\"\n",
    "        tokenizer: an already trained BPE tokenizer\n",
    "        embedding_dim: dimensionality of word embeddings\n",
    "        hidden_size: dimensionalitty of hidden layer in FFNN\n",
    "        ngram_size: size of NGram\n",
    "        p_drop: dropout rate for regular dropout\n",
    "        w_drop: dropout rate for word dropout\n",
    "        \"\"\"\n",
    "        super().__init__(tokenizer=tokenizer, embedding_dim=embedding_dim, p_drop=p_drop, w_drop=w_drop)\n",
    "        assert ngram_size > 1, \"This class expects at least ngram_size 2\"\n",
    "        self.ngram_size = ngram_size\n",
    "\n",
    "        # **SOLUTION**\n",
    "        self.logits_predictor = nn.Sequential(\n",
    "            nn.Dropout(p_drop), # regularisation via dropout before each Linear layer\n",
    "            nn.Linear((ngram_size - 1) * embedding_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop), # regularisation via dropout before each Linear layer\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop), # regularisation via dropout before each Linear layer\n",
    "            nn.Linear(hidden_size, self.vocab_size),\n",
    "        )\n",
    "\n",
    "    def make_ngrams(self, x_in):\n",
    "        \"\"\"\n",
    "        Return a batch of ngram histories for conditioning the next-word cpds.\n",
    "\n",
    "        Example with ngram_size=3:\n",
    "\n",
    "        x_in: \n",
    "        [\n",
    "            [BOS, what, a, nice, day, !  ],\n",
    "            [BOS, what, a, day,  !,   PAD],\n",
    "        ]\n",
    "\n",
    "        # here the ngrams associated with x_in\n",
    "        # it's also useful to remember what the corresponding\n",
    "        #  x_out would be (we indicate it after '#')\n",
    "        ngrams:\n",
    "        [\n",
    "            [\n",
    "                [BOS, BOS],   # what\n",
    "                [BOS, what],  # a\n",
    "                [what, a],    # nice\n",
    "                [a, nice],    # day\n",
    "                [nice, day],  # !\n",
    "                [day, !]      # EOS\n",
    "            ],\n",
    "            [\n",
    "                [BOS, BOS],   # what\n",
    "                [BOS, what],  # a\n",
    "                [what, a],    # day\n",
    "                [a, day],     # !\n",
    "                [day, !],     # EOS\n",
    "                [!, PAD]      # PAD (this position isn't real)\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        :param x_in: batched histories (i.e., sentences batched with add_bos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "        :return: batched ngram histories\n",
    "            with shape [batch_size, max_length, ngram_size - 1]\n",
    "        \"\"\"\n",
    "        x_shape = x_in.shape\n",
    "        # We might need to add more BOS symbols to x_in\n",
    "        # (remember, x_in already has one)\n",
    "        if self.ngram_size > 2:\n",
    "            # [batch_size, ngram_size - 2]\n",
    "            bos = torch.full((x_in.shape[0], self.ngram_size - 2), self.bos, device=x_in.device)\n",
    "            # [batch_size, max_length + ngram_size - 2]\n",
    "            _x = torch.cat([bos, x_in], 1)\n",
    "        else:\n",
    "            _x = x_in\n",
    "\n",
    "        # For each output step, we will have ngram_size - 1 inputs, so we collect those from x\n",
    "        # [batch_size, max_length, ngram_size - 1]\n",
    "        return torch.cat([_x.unsqueeze(-1)[:,i:i+self.ngram_size-1].reshape(x_shape[0], 1, -1) for i in range(x_shape[1])], 1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"\n",
    "        Parameterise the conditional distributions over\n",
    "            X[i] given history (i.e., given x[:i]) for i=1...I.\n",
    "\n",
    "        :param x_in: batched histories (i.e., sentences batched with add_bos=True)\n",
    "            with shape [batch_size, max_length]\n",
    "\n",
    "        :return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
    "        \"\"\"\n",
    "        # The inputs to the FFNN are the ngram_size-1 previous words:\n",
    "        if self.training:\n",
    "            x_in = word_dropout(x_in, self.pad, self.unk, self.w_drop, bos=True)\n",
    "\n",
    "        # [batch_size, max_length, ngram_size - 1]\n",
    "        inputs = self.make_ngrams(x_in)\n",
    "\n",
    "        # *SOLUTION*\n",
    "        # Embed the input histories\n",
    "        # [batch_size, max_length, ngram_size - 1, D]\n",
    "        e = self.embed(inputs)\n",
    "        # [batch_size, max_length, (ngram_size - 1) * D]\n",
    "        e = e.reshape(x_in.shape + (-1,))\n",
    "\n",
    "        # Compute the V-dimensional scores (logits)\n",
    "        # [batch_size, max_length, V]\n",
    "        s = self.logits_predictor(e)\n",
    "\n",
    "        # For numerical stability, we prefer to parameterise the Categorical using logits, rather than probs.\n",
    "        # It would be equivalent (up to numerical precision) to use: Categorical(probs=F.softmax(s, -1))\n",
    "        return td.Categorical(logits=s)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's construct a toy model for testing the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "syu-Z_-PkNwP",
    "outputId": "7e309269-18a2-4d87-9b35-7b3b6a5e7bf2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_all()\n",
    "toy_ngram_lm = NGramLM(demo['tokenizer'], ngram_size=3, embedding_dim=12, hidden_size=7)\n",
    "toy_ngram_lm.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "QNZGBisHCzgY",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here is a toy observation for some tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "mrI1FXd9CyvX",
    "outputId": "4ede79a9-5206-486d-cde3-921fbeda563b",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remember the special symbols\n",
    "# PAD=0\n",
    "# BOS=1\n",
    "# EOS=2\n",
    "# UNK=3\n",
    "# These sentences aren't real, they just illustrate what a batch looks like\n",
    "obs_in = torch.tensor(\n",
    "    [[1, 5, 7, 6, 3, 0],\n",
    "     [1, 4, 5, 7, 4, 6]]\n",
    ")\n",
    "obs_out = torch.tensor(\n",
    "    [[5, 7, 6, 3, 2, 0],\n",
    "     [4, 5, 7, 4, 6, 2]]\n",
    ")\n",
    "obs_in, obs_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "See what `make_ngrams` returns, this might help you understand the _forward_ method of the NGramLM class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "fWs97OVsv0mZ",
    "outputId": "e56a0010-e263-40e6-8644-34eecbfb30d9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this gives us one ngram history for each step of obs_in\n",
    "toy_ngram_lm.make_ngrams(obs_in) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "qXiUmqwmv0mZ",
    "outputId": "a5eaee23-cf68-47f9-b8b1-88caf8c6f20e",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for each history in obs_in, the forward pass gives us \n",
    "# one Categorical cpd over vocab_size symbols \n",
    "\n",
    "# as we have a batch of 2 sentences, \n",
    "# each sentence has 6 tokens (including PADs)\n",
    "# we have a batch of [2, 6] Categorical cpds, each cpd is 1000-dimensional\n",
    "# hence logits will have shape [2, 6, 1000]\n",
    "toy_ngram_lm(obs_in)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "EamB0ca0mC_t",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Check that the forward pass is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "9RQIYa-plZ7K",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert type(toy_ngram_lm(obs_in)) is td.Categorical, \"Did you change the return type?\"\n",
    "assert torch.allclose(torch.sum(toy_ngram_lm(obs_in).probs, -1), torch.ones_like(obs_in).float(), 1e-3), \"Your probabilities do not sum to 1\"\n",
    "assert toy_ngram_lm(obs_in).probs.shape == obs_in.shape + (demo['tokenizer'].vocab_size(),), \"The shape should be [2, 6, vocab_size]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "K_Dnm6SW9VM5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can estimate the loss using the 2 observations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "dcJ_Dw5ykWmq",
    "outputId": "795a9982-1239-4b87-e40d-b09207f9c0f7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ngram_lm.loss(obs_in, obs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "CYWYSFLOYIq4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert type(toy_ngram_lm.loss(obs_in, obs_out)) is torch.Tensor, \"Your loss should be a torch tensor\"\n",
    "assert toy_ngram_lm.loss(obs_in, obs_out).requires_grad, \"Your loss should be differentiable\"\n",
    "assert toy_ngram_lm.loss(obs_in, obs_out).shape == tuple(), \"Your loss should be a scalar tensor\"\n",
    "assert np.isclose(toy_ngram_lm.loss(obs_in, obs_out).item(), 37, 1), \"Without training, with seed 42, and the obs we gave you, your loss should be close to 37. If this is not correct, check with your TA.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "a_tNbvLK9cb0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can also obtain samples from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "En3VysIstjW-",
    "outputId": "f4e4d06c-2764-41df-cfb8-3153b994cdf3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ngram_lm.sample(5, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "m_-XPRoJDHzj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As our model hasn't been trained to do anything in particular, the samples do not show any structure (they don't resemble the observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "G0jFuCE7C-Jv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can also sample from the model to continue some existing histories, these existing histories are called \"prompts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "dpoEJ9BXC-vN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs_prompt = torch.tensor(\n",
    "    [[5, 7, 0, 0],\n",
    "     [4, 5, 7, 4]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "bLkTojYYDQpl",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Without training, our prompts have no effect, and the continuations exhibit no structure whatsoever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "pSiNmJFK8-Gv",
    "outputId": "578f7d20-77f9-4f19-9069-21f83dc418a8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ngram_lm.sample(max_length=10, prompt=obs_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "4psB0bjUDXa3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "So, let's write a training procedure (MLE).\n",
    "For this toy demostration, we will overfit the model to the two observations in `obs_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "vArH3GzAv0ma",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def toy_training(toy_model: BaseLM, x_in, x_out, lr=0.01, steps=1000):\n",
    "    \"\"\"\n",
    "    Fit the model on a single batch.\n",
    "    :param toy_model: an instance of BaseLM\n",
    "    :param x_in: a batch of input sequences [batch_size, max_length]\n",
    "        these are tokenized with add_bos=True\n",
    "    :param x_out: a batch of output sequences [batch_size, max_length]\n",
    "        these are tokenized with add_eos=True\n",
    "    :param lr: learning rate for Adam optimizer\n",
    "    :param steps: number of optimisation steps\n",
    "    \"\"\"\n",
    "    optimiser = opt.Adam(toy_model.parameters(), lr=lr)\n",
    "    with tqdm(range(steps)) as bar:\n",
    "        for _ in bar:\n",
    "            toy_model.train() # we need to tell torch that we are training (so dropout is enabled)\n",
    "            optimiser.zero_grad() # we need to clear gradient information\n",
    "\n",
    "            loss = toy_model.loss(x_in, x_out) # we obtain a loss\n",
    "            bar.set_postfix({'loss': f\"{loss:.2f}\" } )\n",
    "\n",
    "            loss.backward() # compute gradients of that loss wrt trainable parameters\n",
    "            optimiser.step() # and take an optimisation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "QISEAHulD6wf",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "740d7d0564ff4fa8b2e3103e8a77b3f5",
      "93ab3f027d3f4df1b574f2cf6caace4d",
      "97e098e4fef849cc81afe41ab2bcf9b1",
      "53868461d0da482194eb481ac7def665",
      "91001efb667c4762b0f9a7be63bc7151",
      "f3f01d7b80a546d18e39cd7edc4cff0c",
      "595c86f24ad540d5b390b80dc66162ab",
      "3df669b3bce246fa977d7c5ddc5798d3",
      "8b8d6a71fae544f2b3e0ddb5064eec8f",
      "4e0b0cda076c47d2a4910a84af1e25ec",
      "9c0ca3187990448486ed4c9b3ef5a24f"
     ]
    },
    "editable": true,
    "id": "9WjxNbRtv0ma",
    "outputId": "93848ca3-17f2-4ff6-facb-6fb9db6de287",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_training(toy_ngram_lm, obs_in, obs_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FLzguuudD9cj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "See how sampling will now reproduce the observed sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "90mWDKWVv0mb",
    "outputId": "35894251-681d-4c7b-e5e9-5de4dffd4026",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ngram_lm.sample(5, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "lu5wyah3EBnK",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Same is true for sampling conditionally, given a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "PHzMmLdjv0mb",
    "outputId": "3bf8b55a-0fd6-4da2-e0c1-54b0ab03111a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ngram_lm.sample(max_length=10, prompt=obs_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "qtxqGY8_v0mb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Autoregressive_LM'></a>\n",
    "## Autoregressive LM\n",
    "\n",
    "An autoregressive LM is a parameterisation of the chain rule of probabilities, without introducing conditional independence assumptions. We make the assumption that our tokens come from a finite vocabulary and that the sequence is generated from left to right, but nothing else. This is possible because we know NN architectures that allow us to represent variable-length sequences, such as the history preceeding any one token, one such architecture is an LSTM.\n",
    "\n",
    "\\begin{align}\n",
    "f(w_{1:l};\\theta) &= \\prod_{i=1}^l \\mathrm{Cat}(w_i|\\mathbf g(w_{<i}; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf g$ is a neural network with parameters $\\theta$, it maps a prefix sequence $w_{<i}$ to a $V$-dimensional probability vector.\n",
    "\n",
    "This neural network is typically implemented as follows:\n",
    "\n",
    "* embed each word in the $i$th history into a $D$-dimensional space: $\\mathbf e_j = \\mathrm{embed}_D(w_j; \\theta_{\\text{in}})$ for $j<i$;\n",
    "* for any step $i$, encode the prefix history using an LSTM generator/decoder (remember, the difference between the LSTM encoder and the LSTM decoder is that, relative to any given step, the LSTM decoder can only \"see\" the past): $\\mathbf u_i = \\mathrm{rnndec}(\\mathbf u_{i-1}; \\mathbf e_{i-1}; \\theta_{\\text{dec}})$\n",
    "* predict logits for the next-token distribution: $\\mathbf s_i = \\mathrm{linear}_V(\\mathbf u_i; \\theta_{\\text{out}})$;\n",
    "* use softmax to obtain probabilities for the possible words at the $i$th position: $\\mathbf g(w_{<i}; \\theta) = \\mathrm{softmax}(\\mathbf s_i)$\n",
    "* the parameters are $\\theta = \\theta_{\\text{in}} \\cup \\theta_{\\text{dec}} \\cup \\theta_{\\text{out}}$ where\n",
    "    * $\\theta_{\\text{in}}$ is an embedding matrix $\\mathbf E \\in \\mathbb R^{V\\times D}$\n",
    "    * $\\theta_{\\text{dec}}$ are the LSTM parameters (proportional to $D\\times H + H\\times H$ for a hidden cell of size $H$ and inputs of size $D$)\n",
    "    * $\\theta_{\\text{out}}$ are the parameters of the final output layer ($H \\times V + V$)\n",
    "\n",
    "Our architecture is loosely inspired by the [original paper](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf).\n",
    "\n",
    "\n",
    "\n",
    "Next we implement this in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-5'></a> **Ungraded Exercise 5 - AutoregressiveLM class**\n",
    "\n",
    "Study the AutoregressiveLM class below and complete its constructor (you need to specify the LSTM that encodes the history and the FFNN that predicts logits) and the forward method (you need to parameterise the Categorical cpds for each step). Use the tests available to check your implementation. When you are happy with it consult our solution. For exercises, use our own solution to avoid cascading errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class AutoregressiveLM(BaseLM):\n",
    "\n",
    "    def __init__(self, tokenizer, embedding_dim: int, hidden_size: int, cell_size: int, num_layers=1, p_drop=0., w_drop=0.):\n",
    "        \"\"\"\n",
    "        :param tokenizer: an already trained BPE tokenizer\n",
    "        :param embedding_dim: dimensionality of word embeddings\n",
    "        :param hidden_size: dimensionality of the hidden layer in FFNN\n",
    "        :param cell_size: dimensionality of the LSTM cell\n",
    "        :param num_layers: number of LSTM layers\n",
    "        :param p_drop: dropout rate for regular dropout\n",
    "        :param w_drop: dropout rate for word dropout\n",
    "        \"\"\"\n",
    "        super().__init__(tokenizer=tokenizer, embedding_dim=embedding_dim, p_drop=p_drop, w_drop=w_drop)\n",
    "\n",
    "        # **EXERCISE** Construct an nn.LSTM for self.decoder\n",
    "        # and a FFNN for self.logits_predictor.\n",
    "\n",
    "        self.decoder = None\n",
    "        self.logits_predictor = None\n",
    "        \n",
    "        # You want the decoder to accept a word embedding\n",
    "        # as input at each step\n",
    "        # and you want it to have `cell_size` as hidden size\n",
    "        # remember to have it with bidirectional=False\n",
    "        # (because decoders cannot see the 'future')\n",
    "        # and remember to have it batch_first=True\n",
    "        \n",
    "        # You want the FFNN to map from the output dimensionality\n",
    "        # of your LSTM to vocab_size scores\n",
    "        # while using at least one hidden layer of size hidden_size\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"\n",
    "        Parameterise the conditional distributions over X[i] given history x[:i] for i=1...I.\n",
    "\n",
    "        This procedure takes care that the ith output distribution conditions only on the n-1 observations before x[i].\n",
    "        It also takes care of padding to the left with BOS symbols.\n",
    "\n",
    "        x: [batch_size, max_length]\n",
    "\n",
    "        Return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        # here we pad the tag sequence with BOS on the left\n",
    "        # this is how we make sure that the current position of the tag sequence\n",
    "        # is not available for conditioning (only the past is)\n",
    "        # batch_size, max_len = x.shape\n",
    "        # bos = torch.full((batch_size, 1), self.bos, device=x.device)\n",
    "        if self.training:\n",
    "            x_in = word_dropout(x_in, self.pad, self.unk, self.w_drop, bos=True)\n",
    "\n",
    "\n",
    "        # **EXERCISE**\n",
    "        # You should \n",
    "        # 1. embed the input tokens in x_in into a tensor of shape\n",
    "        # [batch_size, max_len, emb_dim]\n",
    "        # 2. encode the embedded x_in using the LSTM to obtain a tensor with shape\n",
    "        # [batch_size, max_len, cell_size]\n",
    "        # 3. use the FFNN to map the LSTM states to vocab_size logits \n",
    "        # [batch_size, max_length, V]\n",
    "        # 4. return a batched td.Categorical object\n",
    "        raise NotImplementedError(\"Implement me!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# *SOLUTION*\n",
    "\n",
    "class AutoregressiveLM(BaseLM):\n",
    "\n",
    "    def __init__(self, tokenizer, embedding_dim: int, hidden_size: int, cell_size: int, num_layers=1, p_drop=0., w_drop=0.):\n",
    "        \"\"\"\n",
    "        :param tokenizer: an already trained BPE tokenizer\n",
    "        :param embedding_dim: dimensionality of word embeddings\n",
    "        :param hidden_size: dimensionality of the hidden layer in FFNN\n",
    "        :param cell_size: dimensionality of the LSTM cell\n",
    "        :param num_layers: number of LSTM layers\n",
    "        :param p_drop: dropout rate for regular dropout\n",
    "        :param w_drop: dropout rate for word dropout\n",
    "        \"\"\"\n",
    "        super().__init__(tokenizer=tokenizer, embedding_dim=embedding_dim, p_drop=p_drop, w_drop=w_drop)\n",
    "\n",
    "        # **SOLUTION**\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=cell_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=0. if num_layers == 1 else p_drop,\n",
    "        )\n",
    "        # **SOLUTION**\n",
    "        self.logits_predictor = nn.Sequential(\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(cell_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_size, self.vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"\n",
    "        Parameterise the conditional distributions over X[i] given history x[:i] for i=1...I.\n",
    "\n",
    "        This procedure takes care that the ith output distribution conditions only on the n-1 observations before x[i].\n",
    "        It also takes care of padding to the left with BOS symbols.\n",
    "\n",
    "        x: [batch_size, max_length]\n",
    "\n",
    "        Return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        # in training mode, we apply word dropout\n",
    "        if self.training:\n",
    "            x_in = word_dropout(x_in, self.pad, self.unk, self.w_drop, bos=True)\n",
    "\n",
    "        # **SOLUTION**\n",
    "        \n",
    "        # 1. we embed the tokens in x_in\n",
    "        # [batch_size, max_len, emb_dim]\n",
    "        e = self.embed(x_in)\n",
    "        \n",
    "        # 2. this runs the LSTM all the way to the end (even for sequences that end before the max length),\n",
    "        # in the computation of the loss, however, we are careful to ignore positions that happen after EOS\n",
    "        # [batch_size, max_len, hidden_size]\n",
    "        v, _ = self.decoder(e)\n",
    "\n",
    "        # 3. Compute the V-dimensional scores (logits)\n",
    "        # [batch_size, max_length, V]\n",
    "        s = self.logits_predictor(v)\n",
    "\n",
    "        # 4. For numerical stability, we prefer to parameterise the Categorical using logits, rather than probs.\n",
    "        # It would be equivalent (up to numerical precision) to use: Categorical(probs=F.softmax(s, -1))\n",
    "        return td.Categorical(logits=s)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's construct a toy model for testing the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "rENFiZl2v0mc",
    "outputId": "81fc827d-b3e3-47b1-d850-0c03c538a399",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_all()\n",
    "toy_ar_lm = AutoregressiveLM(demo['tokenizer'], embedding_dim=12, hidden_size=7, cell_size=20)\n",
    "toy_ar_lm.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "See the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "BIe_fwMTv0mc",
    "outputId": "8070dcfd-f072-407b-fb34-27d9db269f1e",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ar_lm(obs_in) # a batch of [2, 6] Categorical cpds over 1000 symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "xrq8CzTPv0mc",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Check that the forward pass is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "p8a5oMDOv0mc",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert type(toy_ar_lm(obs_in)) is td.Categorical, \"Did you change the return type?\"\n",
    "assert torch.allclose(torch.sum(toy_ar_lm(obs_in).probs, -1), torch.ones_like(obs_in).float(), 1e-3), \"Your probabilities do not sum to 1\"\n",
    "assert toy_ar_lm(obs_in).probs.shape == obs_in.shape + (demo['tokenizer'].vocab_size(),), \"The shape should be [2, 6, vocab_size]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "UGxYtN8tv0md",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can estimate the loss using the 2 observations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "h2AtYpK2v0md",
    "outputId": "c4e0e4a5-a0fc-426f-fa97-5e215ba6b15a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ar_lm.loss(obs_in, obs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "UWIzBR5Gv0md",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert type(toy_ar_lm.loss(obs_in, obs_out)) is torch.Tensor, \"Your loss should be a torch tensor\"\n",
    "assert toy_ar_lm.loss(obs_in, obs_out).requires_grad, \"Your loss should be differentiable\"\n",
    "assert toy_ar_lm.loss(obs_in, obs_out).shape == tuple(), \"Your loss should be a scalar tensor\"\n",
    "assert np.isclose(toy_ar_lm.loss(obs_in, obs_out).item(), 37, 1), \"Without training, with seed 42, and the obs we gave you, your loss should be close to 37. If this is not correct, check with your TA.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "vusUFIhav0md",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can also obtain samples from the model, since it's untrained the samples do not resemble the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "bxkpgBqzv0md",
    "outputId": "ce39f757-fed0-468f-a778-d3bda4d1b405",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ar_lm.sample(5, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "And sampling conditionally (given some prompts) shows that the prompts have no meaningful effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "dCM61wb5v0me",
    "outputId": "c19904e9-b36b-4372-af91-7aa75cbe9ef7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ar_lm.sample(max_length=10, prompt=obs_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "So, let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bd2d3696f66444ada1681bb6cb79e5a3",
      "8e0011a1af3e4e0fb6ade05e989411b4",
      "8bc507f51b3d4309b9339c7599b4dc6d",
      "3653e0cc98f44989805a80528b33283b",
      "6520b4a6ecbe44be822a25cac356fac6",
      "cd83957cc0d343cca00ce4a7f7b5701d",
      "17d8e4c032d84a0f8d9cf77ef33d9d51",
      "bd5e1f19f69e4fd59160a6b6b9a7e3b4",
      "9fb1e9d26d184130be602bd5d0bff0e7",
      "8b6aa85400bd4767b739acd6ddd6aa2c",
      "e109f33829564ad28bf73cb4aad3b374"
     ]
    },
    "editable": true,
    "id": "14StYgrbv0me",
    "outputId": "322a9887-9416-494d-f3ae-eafbd99110b1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_training(toy_ar_lm, obs_in, obs_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "cqgDpsgaHs-p",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "See that sampling now returns the observed structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "CvdT5FiUv0me",
    "outputId": "964358bd-62be-44c1-b01f-d831c67d0d11",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ar_lm.sample(5, max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "ITeV2n9Qv0me",
    "outputId": "95a3b6e1-b99c-4dda-a4d3-d346234e5546",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_ar_lm.sample(max_length=10, prompt=obs_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "nYcaZEYgJzJz",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Evaluation'></a>\n",
    "# Evaluation\n",
    "\n",
    "There are a few ways to evaluate the performance of an LM.\n",
    "\n",
    "Sometimes you can plug it into an application (e.g., an auto-complete system or a system that ranks sentence for fluency), in those cases we can test whether that downstream application improves as we modify the language model. This is called *extrinsic* evaluation.\n",
    "\n",
    "To evaluate an LM independently from an application we need to evaluate its statistical properties in an attempt to determine how well the model fits the data, namely, how well it reproduces statistics of observed data. This is called *intrinsinc* evaluation. In this course, we are going to focus on intrinsic evaluation of the LM.\n",
    "\n",
    "We generally have access to 3 datasets:\n",
    "\n",
    "* Training is used for estimating $\\theta$.\n",
    "* Develpment is used to make choices during the design phase (choose hyperparameters such as smoothing technique, order of model, etc).\n",
    "* Test is used for measuring the accuracy of the final model.\n",
    "   \n",
    "One indication of the model's fitness to the data is the value of the model likelihood given novel sentences (e.g., sentence held-out from training).\n",
    "We assume this dataset $\\mathcal T$ of novel sentences consits of $K$ independent sentences each denoted $w_{1:l_k}^{(k)}$, then the model likelihood given $\\mathcal T$ is the probability mass that the model assigns to $\\mathcal T$:\n",
    "\n",
    "$\\prod_{k=1}^K f_X(w_{1:l_k}^{(k)}; \\theta)$\n",
    "\n",
    "or in form of the log-probability:\n",
    "\n",
    "$\\sum_{k=1}^K \\log f_{X}(w_{1:l_k}^{(k)}; \\theta)$\n",
    "\n",
    "Then define the log-likelihood as follows:\n",
    "\n",
    "$\\mathcal L_{\\mathcal T}(\\theta) = \\sum_{k=1}^K \\log f_X(w_{1:l_k}^{(k)}; \\theta)$\n",
    "\n",
    "\n",
    "Then the model that assings the higher $\\mathcal L$ given the test set is the one that's better predictive of future data, presumably that's the case because it found a better fit of the training data (a better compromise between memorisation and generalisation).\n",
    "\n",
    "In other words, given two probabilistic models, the one that assigns a higher probability to the test data is taken as intrinsically better. One detail we need to abstract away from is differences in factorisation of the models which may cause their likelihoods not to be comparable, but for that we will define *perplexity* below.\n",
    "\n",
    "The log likelihood is used because the probability of a particular sentence according to the LM can be a very small number, and the product of these small numbers can become even smaller, and it will cause numerical\n",
    "precision problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "KRvVRrEpIWq3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Perplexity\n",
    "\n",
    "The _perplexity_ of a language model on a test set is the inverse probability of the test set, normalized\n",
    "by the number of tokens. Perplexity is a notion of average branching factor, thus an LM with low perplexity can be thought of as a *less confused* LM. That is, each time it introduces a word given some history it picks from a reduced subset of the entire vocabulary (in other words, it is more certain of how to continue from the history).\n",
    "\n",
    "If a dataset contains $t$ tokens where $t = \\sum_{k=1}^K l_k$, then the perplexity of the model given the test set is\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{PP}_{\\mathcal T}(\\theta) = \\exp\\left( -\\frac{1}{t} \\mathcal L_{\\mathcal T}(\\theta) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "The lower the perplexity, the better the model is. Comparisons in terms of perplexity are only fair if the models have the same vocabulary.\n",
    "\n",
    "We will need a batched PyTorch version of perplexity, to make sure you can run experiments with the correct code, we implement it here for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "NyxEV40SI1q_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perplexity(model: BaseLM, dl: DataLoader, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    model: an instance of NeuralNGramLM\n",
    "    dl: a data loader for the heldout data\n",
    "    device: the PyTorch device where the model is stored\n",
    "    \"\"\"\n",
    "    model.eval() # we tell torch that we are in evaluation mode (so, for example, dropout is disabled)\n",
    "    total_tokens = 0 # needed for perplexity\n",
    "    total_log_prob = 0.\n",
    "    with torch.no_grad():  # we don't need gradients in evaluation\n",
    "        for x_in, x_out in dl:\n",
    "            total_tokens += (x_out > 0).float().sum()\n",
    "            total_log_prob = total_log_prob + model.log_prob(x_in=x_in.to(device), x_out=x_out.to(device)).sum()\n",
    "    return torch.exp(-total_log_prob / total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "d4VkhjcIJL6F",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Training'></a>\n",
    "# Training\n",
    "\n",
    "We now implement a complete training loop for our demo. \n",
    "This time, rather than overfitting to 2 made up observations, we want to use a DataLoader that iterates over our tokenized datasets.\n",
    "\n",
    "The _demo_ runs on CPU (each model will take about 5 minutes), but the demo will not lead to a model that produces reasonable sentences. The actual experiment using the complete SNLI dataset cannot be done on CPU, so we suggest you use Google Colab. For developing your code, you can use the demo (and you can make it even smaller if you like), but for the final experiment, please use the complete SNLI dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "pKFUw21bYCUm",
    "outputId": "a7718d24-7057-4c4c-d8af-925655d5ddf5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda:0')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "    print(\"You may continue with CPU, but when you get to the final experiment a CPU will be much too slow.\")\n",
    "my_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-6'></a> **Ungraded Exercise 6 - Training loop**\n",
    "\n",
    "Study the training loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "ZQR5TGAqd-HC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_neural_model(exp_name: str, model: BaseLM, optimizer, training_corpus: Corpus, dev_corpus: Corpus, batch_size=200, num_epochs=10, check_every=100, device=torch.device('cuda:0'), ckptdir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    :param exp_name: filename to save the model\n",
    "    :param model: an instance of BaseLM\n",
    "    :param optimizer: a torch optimizer, we suggest Adam\n",
    "    :param training_corpus: tokenized training data (an instance of Corpus)\n",
    "    :param dev_corpus: tokenized dev/validation data (an instance of Corpus)\n",
    "    :param batch_size: number of sentences in a single batch\n",
    "    :param num_epochs: number of passes over the entire training corpus\n",
    "    :param check_every: number of steps between validation checks\n",
    "    :param device: torch device where the model is\n",
    "    :param ckptdir: directory to save models\n",
    "    :returns: training log as a dictionary of measurements such as\n",
    "        - training loss\n",
    "        - dev perplexity\n",
    "    \"\"\"\n",
    "    if not os.path.exists(ckptdir):  # we will be storing models in this folder\n",
    "        os.makedirs(ckptdir)\n",
    "    # we use the training data in random order for parameter estimation\n",
    "    batcher = DataLoader(training_corpus, batch_size=batch_size, shuffle=True, collate_fn=pad_to_longest_paired)\n",
    "    # we use the dev data for evaluation during training (no need for randomisation here)\n",
    "    dev_batcher = DataLoader(dev_corpus, batch_size=batch_size, shuffle=False, collate_fn=pad_to_longest_paired)\n",
    "\n",
    "    total_steps = num_epochs * len(batcher)\n",
    "    log = defaultdict(list)\n",
    "    ppl = perplexity(model, dev_batcher, device=device).item()\n",
    "    log['ppl'].append(ppl)\n",
    "    best_ppl = ppl\n",
    "    torch.save(model.state_dict(), f\"{ckptdir}/{exp_name}.pt\")\n",
    "    step = 0\n",
    "\n",
    "    with tqdm(range(total_steps)) as bar:\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            for x_in, x_out in batcher:\n",
    "                model.train()  # this is how pytorch knows that we will be updating parameters\n",
    "                optimizer.zero_grad()  # this is needed in order to reset gradient information from previous iterations\n",
    "\n",
    "                loss = model.loss(x_in.to(device), x_out.to(device))  # compute the loss for this batch\n",
    "\n",
    "                loss.backward()  # compute the gradient with respect to the parameters using backpropagation\n",
    "                optimizer.step()  # take a step towards minimising the loss\n",
    "\n",
    "                # udpate logging info\n",
    "                bar.set_postfix({'loss': f\"{loss.item():.2f}\", 'ppl': f\"{ppl:.2f}\"})\n",
    "                bar.update()\n",
    "                log['loss'].append(loss.item())\n",
    "\n",
    "                if step % check_every == 0:  # every so often, check performance on validation set\n",
    "                    ppl = perplexity(model, dev_batcher, device=device).item()\n",
    "                    log['ppl'].append(ppl)\n",
    "                    if ppl <= best_ppl: # save a better checkpoint\n",
    "                        best_ppl = ppl\n",
    "                        torch.save(model.state_dict(), f\"{ckptdir}/{exp_name}.pt\")\n",
    "\n",
    "                step += 1\n",
    "\n",
    "    ppl = perplexity(model, dev_batcher, device=device).item()\n",
    "    if ppl <= best_ppl: # save a better checkpoint\n",
    "        best_ppl = ppl\n",
    "        torch.save(model.state_dict(), f\"{ckptdir}/{exp_name}.pt\")\n",
    "    log['ppl'].append(ppl)\n",
    "    return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "RwXhhAUChsKT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Demo'></a>\n",
    "# Demo\n",
    "\n",
    "Here we demonstrate how to train and evaluate a model. \n",
    "First, some helper code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss_and_ppl(log, skip=3):\n",
    "    \"\"\"\n",
    "    :param log: dict with loss measurements and ppl measurements\n",
    "    :param skip: skip the first few ppl measurements\n",
    "        the ppl of the untrained model is so high\n",
    "        that it's more insightful not to plot it\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    _ = axs[0].plot(np.arange(len(log['loss'])), log['loss'])\n",
    "    _ = axs[0].set_xlabel('steps')\n",
    "    _ = axs[0].set_ylabel('training loss')\n",
    "    _ = axs[1].plot(np.arange(len(log['ppl'][skip:])), log['ppl'][skip:])\n",
    "    _ = axs[1].set_xlabel('steps (in 100s)')\n",
    "    _ = axs[1].set_ylabel('model ppl given dev')\n",
    "    _ = fig.tight_layout(h_pad=2, w_pad=2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_few_samples(model, num_samples, max_length=50):\n",
    "    \"\"\"\n",
    "    :param model: an instance of BaseLM\n",
    "    :param num_samples: number of samples (these will be drawn in a single batch)\n",
    "    :param max_length: longest sample\n",
    "    \"\"\"\n",
    "    for i, x in enumerate(model.tokenizer.decode(model.sample(num_samples, max_length).tolist(), out_type=str), 1):\n",
    "        print(f\"{i}\\t{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_few_prompted_samples(model, prompt_strings=['A person', 'A person walking', 'A person selling their'], num_samples=5, max_length=50):\n",
    "    \"\"\"\n",
    "    :param model: an instance of BaseLM\n",
    "    :param prompt_strings: a list of prompts (each a string)\n",
    "    :param num_samples: how many times we will complete the prompts by sampling from the model\n",
    "    :param max_length: longest sample\n",
    "    \"\"\"\n",
    "    prompt_batch = pad_to_longest(\n",
    "        model.tokenizer.encode(prompt_strings),\n",
    "        model.tokenizer.pad_id()\n",
    "    ).to(model.device())\n",
    "\n",
    "    for k in range(num_samples):\n",
    "        print(f\"\\nExperiment {k+1}\\n\")\n",
    "        for i, x in enumerate(model.tokenizer.decode(model.sample(max_length=max_length, prompt=prompt_batch).tolist(), out_type=str)):\n",
    "            print(f\"{prompt_strings[i]} ||| {x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**NGramLM**\n",
    "\n",
    "On CPU this should take about 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seed_all() # reset random number generators before creating your model and training it\n",
    "\n",
    "demo['model.ngram'] = NGramLM(\n",
    "    demo['tokenizer'],\n",
    "    embedding_dim=32,\n",
    "    hidden_size=64,\n",
    "    ngram_size=3,\n",
    "    p_drop=0.1\n",
    ").to(my_device)\n",
    "\n",
    "print(\"Model\")\n",
    "print(demo['model.ngram'])\n",
    "# report number of parameters\n",
    "print(\"NGramLM model size:\", demo['model.ngram'].num_parameters())\n",
    "\n",
    "demo['log.ngram'] = train_neural_model(\n",
    "    \"3gram\",\n",
    "    demo['model.ngram'],\n",
    "    opt.Adam(demo['model.ngram'].parameters(), lr=1e-2),\n",
    "    demo['training_tok'], \n",
    "    demo['dev_tok'],\n",
    "    batch_size=100, \n",
    "    num_epochs=5, \n",
    "    check_every=100,\n",
    "    device=my_device\n",
    ")\n",
    "\n",
    "plot_loss_and_ppl(demo['log.ngram'])\n",
    "\n",
    "test_ppl = perplexity(\n",
    "    demo['model.ngram'],\n",
    "    DataLoader(\n",
    "        demo['test_tok'], \n",
    "        batch_size=100, \n",
    "        shuffle=False, \n",
    "        collate_fn=pad_to_longest_paired\n",
    "    ),\n",
    "    device=my_device\n",
    ")\n",
    "print(\"\\n\\n Model perplexity given test set\", test_ppl.item())\n",
    "\n",
    "print(\"\\n# Samples\\n\\n\")\n",
    "print_few_samples(demo['model.ngram'], 10)\n",
    "\n",
    "print(\"\\n# Prompted samples\\n\\n\")\n",
    "print_few_prompted_samples(demo['model.ngram'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**AutoregressiveLM**\n",
    "\n",
    "On CPU this should take about 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878,
     "referenced_widgets": [
      "e9f3c731ab744c8c9f2a870701bb0b2d",
      "6becedfdc1d0469eb233de5f31841382",
      "3cba0dcf676d468a9e37096d141a0a97",
      "f787c6b07f3a4d4eac326e2d8d0314fe",
      "2a556f42b1c94d598e0fe3f875450985",
      "3e68607379a444dbbd900b6f9187601c",
      "1effd3ad34a24afb9ef319e139add069",
      "41a0393f426942e586f21c45a2d12913",
      "64112fa0dd944ab0879b0d0699befdf6",
      "4df004151f0a48eb88135181d61cff54",
      "48013a0029f6465494bc8fc68bd4c27b"
     ]
    },
    "editable": true,
    "id": "6EORnlrW3vAH",
    "outputId": "d0568b9a-3edc-42a2-f8db-4b71013a856b",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seed_all() # reset random number generators before creating your model and training it\n",
    "\n",
    "demo['model.lstm'] = AutoregressiveLM(\n",
    "    demo['tokenizer'],\n",
    "    embedding_dim=32,\n",
    "    hidden_size=64,\n",
    "    cell_size=64,\n",
    "    num_layers=1,\n",
    "    w_drop=0.1,\n",
    "    p_drop=0.1\n",
    ").to(my_device)\n",
    "\n",
    "print(\"Model\")\n",
    "print(demo['model.lstm'])\n",
    "# report number of parameters\n",
    "print(\"Autoregressive model size:\", demo['model.lstm'].num_parameters())\n",
    "\n",
    "demo['log.lstm'] = train_neural_model(\n",
    "    \"lstm\",\n",
    "    demo['model.lstm'],\n",
    "    opt.Adam(demo['model.lstm'].parameters(), lr=1e-2),\n",
    "    demo['training_tok'], \n",
    "    demo['dev_tok'],\n",
    "    batch_size=100, \n",
    "    num_epochs=5, \n",
    "    check_every=100,\n",
    "    device=my_device\n",
    ")\n",
    "\n",
    "plot_loss_and_ppl(demo['log.lstm'])\n",
    "\n",
    "test_ppl = perplexity(\n",
    "    demo['model.lstm'],\n",
    "    DataLoader(\n",
    "        demo['test_tok'], \n",
    "        batch_size=100, \n",
    "        shuffle=False, \n",
    "        collate_fn=pad_to_longest_paired\n",
    "    ),\n",
    "    device=my_device\n",
    ")\n",
    "print(\"\\n\\n Model perplexity given test set\", test_ppl.item())\n",
    "\n",
    "print(\"\\n# Samples\\n\\n\")\n",
    "print_few_samples(demo['model.lstm'], 10)\n",
    "\n",
    "print(\"\\n# Prompted samples\\n\\n\")\n",
    "print_few_prompted_samples(demo['model.lstm'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Experiment'></a>\n",
    "# Experiment\n",
    "\n",
    "The next two experiments will use a larger version of the dataset and larger models. Because of that, they are too slow on CPU, hence we recommend you use Colab. \n",
    "\n",
    "If for some reason you are unable to use Colab, it's okay to perform the two experiments using the dataset, tokenizer and models you trained in the `demo` above, though the results will be less interesting to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():        \n",
    "    raise ValueError(\"The next experiment is probably too slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is the dataset and tokenizer for the following two experiments/exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "snli['tokenizer'] = fit_vocabulary(snli['training'], vocab_size=10000)\n",
    "snli['training_tok'] = Corpus(snli['training'], snli['tokenizer'], max_length=50)\n",
    "snli_dev_subset = np.random.RandomState(42).choice(len(snli['dev']), replace=False, size=2000)\n",
    "snli['dev_tok'] = Corpus(snli['dev'][snli_dev_subset], snli['tokenizer'], max_length=50)\n",
    "snli['test_tok'] = Corpus(snli['test'], snli['tokenizer'])\n",
    "len(snli['training_tok']), len(snli['dev_tok']), len(snli['test_tok'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-1'> **Graded Exercise 1 - Comparison** </a>\n",
    "\n",
    "On GPU, train the following two models using the specified hyper-parameters.\n",
    "\n",
    "```python\n",
    "snli['model.ngram'] = NGramLM(\n",
    "    snli['tokenizer'],\n",
    "    embedding_dim=100,\n",
    "    hidden_size=128,\n",
    "    ngram_size=3,  \n",
    "    p_drop=0.1\n",
    ").to(my_device)\n",
    "\n",
    "# and\n",
    "\n",
    "snli['model.lstm'] = AutoregressiveLM(\n",
    "    snli['tokenizer'],\n",
    "    embedding_dim=100,\n",
    "    hidden_size=128,\n",
    "    cell_size=128,\n",
    "    num_layers=1,\n",
    "    w_drop=0.1,\n",
    "    p_drop=0.1\n",
    ").to(my_device)\n",
    "```\n",
    "\n",
    "Use \n",
    "* `lr=1e-2`\n",
    "* `batch_size=500`\n",
    "* `num_epochs=2` \n",
    "* `check_every=100`\n",
    "\n",
    "\n",
    "\n",
    "Then, \n",
    "\n",
    "1. Plot training loss and validation perplexity. Do you observe overfitting?\n",
    "2. Report perplexity given the `test` set. Which model performs best on this criterion?\n",
    "3. Obtain 20 samples from each model, decode them back to strings and print them. Analyse the samples manually and make remarks about their quality (or lack of quality), try to ground observations you make to technical aspects of your model.\n",
    "4. Prompt the model with at least the provided prompts (feel free to test more prompts if you like). Obtain 5 samples for each prompt (you can basically prompt the model 5 times with the same batch of prompts). Do you observe meaningful differences between the two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tCONTRIBUTE YOUR SOLUTION/DISCUSSION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='graded-2'> **Graded Exercise 2 - Analysis** </a>\n",
    "\n",
    "Last, conduct a statistical evaluation of the two models. \n",
    "For this, for each model, you should:\n",
    "1. Draw as many samples as you have sentences in the dev set. Your samples should be obtained without prompts. Be careful, if you try to draw all samples in a single batch, you will run out of memory (instead, you should draw many batches of some reasonable size).\n",
    "2. Compare the distribution of the length of the observed sentences in the dev set to the distribution of the length of the sentences generated in step (1). For this comparison, compute length in number of BPEs (and make sure to not count PAD). Use an appropriate data visualisation tool for this. Make remarks about what you see.\n",
    "3. Now, tokenize the sentences you generated in (1) using NLTK's `word_tokenize` (as this will give you symbols that are more similar to linguistic words than BPE tokenization does). Similarly, tokenize the sentences in the **training** data using `word_tokenize`. Investigate what percentage of generated words did not exist in the training data. Finally, inspect a subset of these novel symbols (concentrate on the 30 most frequent such symbols). Comment on what you observe (e.g., are they English words, are they gibberish?).\n",
    "\n",
    "\n",
    "Last, but not least, compare your findings for step (2) and (3) across models. What do you notice? Do you notice any effect of conditional independences (or lack thereof) in your findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRIBUTE YOUR SOLUTION/CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tCONTRIBUTE YOUR SOLUTION/DISCUSSION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "ujDRDpYcv0ml",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": false,
   "name": "T5.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "17d8e4c032d84a0f8d9cf77ef33d9d51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1effd3ad34a24afb9ef319e139add069": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a556f42b1c94d598e0fe3f875450985": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3653e0cc98f44989805a80528b33283b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b6aa85400bd4767b739acd6ddd6aa2c",
      "placeholder": "",
      "style": "IPY_MODEL_e109f33829564ad28bf73cb4aad3b374",
      "value": "1000/1000[00:07&lt;00:00,152.23it/s,loss=0.70]"
     }
    },
    "3cba0dcf676d468a9e37096d141a0a97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41a0393f426942e586f21c45a2d12913",
      "max": 2500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_64112fa0dd944ab0879b0d0699befdf6",
      "value": 2500
     }
    },
    "3df669b3bce246fa977d7c5ddc5798d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e68607379a444dbbd900b6f9187601c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41a0393f426942e586f21c45a2d12913": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48013a0029f6465494bc8fc68bd4c27b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4df004151f0a48eb88135181d61cff54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e0b0cda076c47d2a4910a84af1e25ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53868461d0da482194eb481ac7def665": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e0b0cda076c47d2a4910a84af1e25ec",
      "placeholder": "",
      "style": "IPY_MODEL_9c0ca3187990448486ed4c9b3ef5a24f",
      "value": "1000/1000[00:04&lt;00:00,256.11it/s,loss=1.39]"
     }
    },
    "595c86f24ad540d5b390b80dc66162ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64112fa0dd944ab0879b0d0699befdf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6520b4a6ecbe44be822a25cac356fac6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6becedfdc1d0469eb233de5f31841382": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e68607379a444dbbd900b6f9187601c",
      "placeholder": "",
      "style": "IPY_MODEL_1effd3ad34a24afb9ef319e139add069",
      "value": "100%"
     }
    },
    "740d7d0564ff4fa8b2e3103e8a77b3f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93ab3f027d3f4df1b574f2cf6caace4d",
       "IPY_MODEL_97e098e4fef849cc81afe41ab2bcf9b1",
       "IPY_MODEL_53868461d0da482194eb481ac7def665"
      ],
      "layout": "IPY_MODEL_91001efb667c4762b0f9a7be63bc7151"
     }
    },
    "8b6aa85400bd4767b739acd6ddd6aa2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b8d6a71fae544f2b3e0ddb5064eec8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8bc507f51b3d4309b9339c7599b4dc6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd5e1f19f69e4fd59160a6b6b9a7e3b4",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9fb1e9d26d184130be602bd5d0bff0e7",
      "value": 1000
     }
    },
    "8e0011a1af3e4e0fb6ade05e989411b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd83957cc0d343cca00ce4a7f7b5701d",
      "placeholder": "",
      "style": "IPY_MODEL_17d8e4c032d84a0f8d9cf77ef33d9d51",
      "value": "100%"
     }
    },
    "91001efb667c4762b0f9a7be63bc7151": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93ab3f027d3f4df1b574f2cf6caace4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3f01d7b80a546d18e39cd7edc4cff0c",
      "placeholder": "",
      "style": "IPY_MODEL_595c86f24ad540d5b390b80dc66162ab",
      "value": "100%"
     }
    },
    "97e098e4fef849cc81afe41ab2bcf9b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3df669b3bce246fa977d7c5ddc5798d3",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b8d6a71fae544f2b3e0ddb5064eec8f",
      "value": 1000
     }
    },
    "9c0ca3187990448486ed4c9b3ef5a24f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fb1e9d26d184130be602bd5d0bff0e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bd2d3696f66444ada1681bb6cb79e5a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e0011a1af3e4e0fb6ade05e989411b4",
       "IPY_MODEL_8bc507f51b3d4309b9339c7599b4dc6d",
       "IPY_MODEL_3653e0cc98f44989805a80528b33283b"
      ],
      "layout": "IPY_MODEL_6520b4a6ecbe44be822a25cac356fac6"
     }
    },
    "bd5e1f19f69e4fd59160a6b6b9a7e3b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd83957cc0d343cca00ce4a7f7b5701d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e109f33829564ad28bf73cb4aad3b374": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9f3c731ab744c8c9f2a870701bb0b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6becedfdc1d0469eb233de5f31841382",
       "IPY_MODEL_3cba0dcf676d468a9e37096d141a0a97",
       "IPY_MODEL_f787c6b07f3a4d4eac326e2d8d0314fe"
      ],
      "layout": "IPY_MODEL_2a556f42b1c94d598e0fe3f875450985"
     }
    },
    "f3f01d7b80a546d18e39cd7edc4cff0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f787c6b07f3a4d4eac326e2d8d0314fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4df004151f0a48eb88135181d61cff54",
      "placeholder": "",
      "style": "IPY_MODEL_48013a0029f6465494bc8fc68bd4c27b",
      "value": "2500/2500[05:56&lt;00:00,5.77it/s,loss=48.16,ppl=33.31]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
