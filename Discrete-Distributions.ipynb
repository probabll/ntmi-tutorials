{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/Discrete-Distributions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTIbLPRpFj6b"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Theory for Discrete Outcomes\n",
    "\n",
    "This is a very brief review of terminology, notation and results from basic probability theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample space\n",
    "\n",
    "When studying random experiments, the *sample space* is the set of outcomes potentially observable. We normally denote it by some capital Greek letter. For example, $\\Omega$. In a coin flip, we may observe it land heads up or tails up, so the sample space of a coin flip is $\\Omega = \\{\\text{H}, \\text{T} \\}$ (using H to mean heads and T to mean tails). The sample space of the experiment 'flip two coins' is $\\Omega = \\{ \\text{HH}, \\text{HT}, \\text{TH}, \\text{TT}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event\n",
    "\n",
    "An event groups outcomes of a sample space, which makes an event itself a set. We say the event 'occurred' if any of its outcomes is observed. This is a convenient way to talk about properties of outcomes of random experiments. For example, in a certain game of chance, when I flip two coins, perhaps all I really care about is the number of 'heads'. Then we have events like\n",
    "\n",
    "1. drawing no heads\n",
    "2. drawing 1 heads\n",
    "3. drawing 2 heads\n",
    "\n",
    "These can be described as sets of outcomes as follows:\n",
    "\n",
    "1. $\\{TT\\}$\n",
    "2. $\\{\\text{HT}, \\text{TH}\\}$\n",
    "3. $\\{\\text{HH}\\}$\n",
    "\n",
    "The space of all possible events is called the **event space**. There are different possibilities for event spaces, but normally we just assume all possible subsets of the sample space are possible events. All possible sets of a countable set is what we call the *powerset* of the set. The event space which is the powerset of the sample space is denoted $\\mathbb P(\\Omega)$ or also $2^\\Omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability measure\n",
    "\n",
    "A probability measure maps each event in the event space to a real number which we call a \\textbf{probability}.  Probability measures are often denoted by $\\mathbb P$ or $\\Pr$.\n",
    "\n",
    "For a probability measure, it must hold:\n",
    "\n",
    "1. $\\mathbb P(A) \\ge 0$ for all $A \\in \\mathcal A$: the smallest probability value attainable by any event is 0;\n",
    "2. $\\mathbb P \\left( \\bigcup_{i=1}^n A_i \\right) = \\sum_{i=1}^n \\mathbb P(A_i)$ for a countable collection $\\{A_1, \\ldots, A_n\\} \\subseteq \\mathcal A$ of pairwise disjoint events: the total probability assigned to $n$ pairwise disjoint events is the sum of the probability values assigned to each of the $n$ events;\n",
    "3. $\\mathbb P(\\Omega) = 1$: the event which is the set of all possible outcomes is assigned a total probability value of 1. \n",
    "\n",
    "\n",
    "These properties also implie that\n",
    "\n",
    "4. $\\mathbb P(\\emptyset) = 0$: the empty event has probability 0, that is, if we observed a random experiment something must have happened;\n",
    "5. $\\mathbb P(A \\cup B) = \\mathbb P(A) + \\mathbb P(B) - \\mathbb P(A \\cap B)$ for events $A, B \\in \\mathcal A$: is a generalisation of property (2) which does not require disjoint events;\n",
    "6. $\\mathbb P(\\Omega \\setminus A) = 1 - \\mathbb P(A)$ for an event $A \\in \\mathcal A$: the probability of the complement of an event $A$ in $\\Omega$ (denoted $\\Omega \\setminus A$) is the 1 minus the probability of $A$; this result is known as the *complement rule*.\n",
    "\n",
    "From now on, there will be many layers of abstractions and a lot of notation. Make an effort to keep track. And recall, the argument that the probability measure accepts is always a set of outcomes (i.e., an event).\n",
    "\n",
    "There are many ways to specify probability measures, this is not really a concern of probability theory. Probability theory only tells us the *formal properties* of probabilities measures. We have the freedom to design them as we like as long as the result complies with these formal properties. Statistics will help us specify some probability measures with the help of observed data. Machine learning techniques will also help us specify some probability measures, in particular, for very complex event spaces.\n",
    "\n",
    "For the two-coins example, we might specify the following probability measure:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pr(A) = \\begin{cases}\n",
    "0.2 & \\text{if }A=\\{TT\\} \\\\\n",
    "0.2 & \\text{if }A=\\{HH\\}\\\\\n",
    "0.6 & \\text{if }A=\\{\\text{HT}, \\text{TH}\\}\\\\\n",
    "0   & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "You can verify that this function complies with all formal properties of probability measures and is, therefore, a probability measure.\n",
    "\n",
    "Another way to specify a valid probability measure for this is to use another measure known as the *cardinality measure*. This can be a good model of coin flips under certain idealisations. For example, assume the coins are fair, independent, and the experimenter does not interfere with the results. Then the function \n",
    "\n",
    "\\begin{equation}\n",
    "\\Pr(A) = \\frac{|A|}{|\\Omega|}\n",
    "\\end{equation}\n",
    "\n",
    "where $|A|$ is the number of elements in the set (the 'size' or 'cardinality' of the set) prescribes a valid probability measure. In this case, it would be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pr(A) = \\begin{cases}\n",
    "0.25 & \\text{if }A=\\{TT\\} \\\\\n",
    "0.25 & \\text{if }A=\\{HH\\}\\\\\n",
    "0.5 & \\text{if }A=\\{\\text{HT}, \\text{TH}\\}\\\\\n",
    "0   & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "You can verify it for yourself that this is a valid probability measure.\n",
    "\n",
    "**Remark:** There's nothing objectively right or wrong about any of these measures, the first could be a decent model of the experiment depending on the circumstances. Similarly, the second could be a very bad approximation of reality (for example, if the coins aren't fair, experimenter isn't honest, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variable\n",
    "\n",
    "A random variable is an abstraction that allows us to give convenient mathematical treatment to the most diverse types of random experiments (involving the most diverse types of samples spaces). \n",
    "Formally, a *real random variable* is a function that maps elements from a sample space $\\Omega$ to (a subset of) the real line $\\mathbb R$, known as the random variable's *range*.\n",
    "\n",
    "For example, $X: \\Omega \\to \\mathcal X$, with $\\mathcal X \\subseteq \\mathbb R$ is a random variable whose domain (or sample space) is $\\Omega$ and whose range is $\\mathcal X$. By mapping outcomes to the real line we give outcomes a numerical treatment, which will be *very* convenient when designing tools to specify probability measures efficiently and in a compact manner.\n",
    "\n",
    "For the 'heads in two coin flips', we could have a random variable:\n",
    "\n",
    "\\begin{equation}\n",
    "X(\\omega) = \\begin{cases}\n",
    "0 & \\text{if }\\omega=\\text{TT} \\\\\n",
    "1 & \\text{if }\\omega=\\text{HT} \\\\\n",
    "1 & \\text{if }\\omega=\\text{TH}\\\\\n",
    "2 & \\text{if }\\omega=\\text{HH}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Here the range of $X$ is $\\{0, 1, 2\\} \\subset \\mathbb R$.\n",
    "\n",
    "If we ever need to work out the specific event (in the event space) associated with an outcome $x$ of a random variable, we can use a shortcut notation: $X=x$, which is pronounced \"$X$ takes on the value $x$\". This notation evaluates to $\\{\\omega \\in \\Omega: X(\\omega) = x\\}$. That is, the set of all outcomes in the sample space $\\Omega$ which the random variable $X$ maps to the outcome $x$ in its range.  For example, using the random variable above, $X=1$ evaluates to the event $\\{\\text{HT}, \\text{TH}\\}$. \n",
    "\n",
    "**Remark:** it might look like the notation $X=x$ is a waste of symbols, and that we might as well simply replace it by $x$. That is not true though. Written alone, some $x$ from $\\mathcal X$ is just a real value. Written alone, $X$ is a random variable (no specific value). Written together, $X=x$ we recover a set of outcomes in the sample space, or, more precisely, the specific event that corresponds to $X$ taking on the outcome $x$ in its range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distribution\n",
    "\n",
    "A probability distribution $P_X$ is essentially a probability measure, but used specifically with random variables. That is why we use the subscript $_X$ in $P_X$.\n",
    "\n",
    "If $X$ is a random variable with range $\\mathcal X$ and distribution $P_X$, then $P_X(X=x)$ is the probability that $X$ takes on the value $x \\in \\mathcal X$. \n",
    "\n",
    "Remarks:\n",
    "* $X$ is not any one particular value of the random outcome, $X$ is at best described by its probability distribution; \n",
    "* $x \\in \\mathcal X$ is an outcome in the range of the random variable $X$, this means that $x$ is a value that $X$ may take on (if you were to observe a realisation of the random phenomenon or experiment that $X$ is meant to model); \n",
    "* $X=x$ is the notation we use to say that of all possible events that can happen, we concentrate only on the event that captures the outcomes of a random experiment that are consistent with $x$. \n",
    "* it's common to drop the subscript $P_X(X=x)$, as in $P(X=x)$, while this is reasonably clear with a single variable, it gets less clear when we have multiple random variables (thus we recommend not to drop it);\n",
    "* it's common to find notation such as $P(x)$, this is a big abuse of notation as the probability distribution *does not take real values as arguments*, rather, it needs events, that's why we write $P(X=x)$; sometimes you find $P_X(x)$, this is a bit better in that we can see the rv $X$ in the subscript and we may then imagine that $P_X(X=x)$ was meant, while $P(x)$ does not even tell you which rv to use;\n",
    "* finally, later we will learn about another important device, the probability mass function (pmf), which is a function of outcomes in the *range* of rv, that device is often denoted $p(x)$ or $f(x)$, or various other names (as we shall see), and that's why we better use notation for distributions carefully.\n",
    "\n",
    "\n",
    "The two cells below illustrate the difference between a standard variable and a random variable. A standard variable gets assigned a value and never changes, unless we reassign that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 1\n",
    "print([v for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random variable gets assigned a distribution of values. Each time we observe a realisation of the random variable, we in effect observe the result of a random phenomenon/experiment, which can be any one of the values that are in the range of our random variable (in the example below this can be 0 or 1). The type of distribution we choose and its parameters (we will learn more about that in a moment) determine the frequency with which we expect to see the possible outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_X = lambda : np.random.binomial(2, 0.5)\n",
    "print([observe_X() for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any probability distribution, $\\forall x \\in \\mathcal X$ it holds that  $0 \\le P_X(X=x) \\le 1$ and $\\sum_{x \\in \\mathcal X} P_X(X=x) = 1$. \n",
    "\n",
    "The notation $\\forall x \\in \\mathcal X$ is pronounced \"for all $x$ in the range of the random variable $X$\".\n",
    "\n",
    "The set $\\mathrm{supp}(P_X)$ is the *support* of the distribution, that is, the subset of $\\mathcal X$ for which $P_X(X=x)>0$.\n",
    "\n",
    "An example we just used above is the Binomial distribution (you will learn more about it later), which captures the number of times we obtain in a number of random repetitions of an experiment with binary outcomes $0$ or $1$. This distribution is controlled by two parameters, the number of repetitions, and the probability of obtaining $1$ whenever we draw an outcome (this probability is kept fixed throughout all repetitions). The support of the random variable is the set of integers from $0$ to the total number of repetitions. So, if we flip a coin 3 times, the support of the Binomial distribution is $\\{0, 1, 2, 3\\}$, any other value is assigned probability 0. Later you will see the complete Binomial law, including the probability mass function that specifies the probabilities values of the different outcomes in the support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQQEbCG5HXDl"
   },
   "source": [
    "## Cumulative distribution function\n",
    "\n",
    "For a random variable $X$ with distribution $P_X$, the cumulative distribution function (cdf) is the function $F_X(a) = \\sum_{x \\le a} P_X(X=x)$. \n",
    "Because probability values are never negative, the cdf is increasing and monotone. \n",
    "\n",
    "The cdf plays an important role in sampling algorithms. In particular, if we draw $X$ from $P_X$ and evaluate the cdf, the distribution of the result is uniform over the interval $[0, 1]$. That is, $F_X(X) \\sim \\mathcal U(0, 1)$. Conversely, if $U$ is uniformly distributed over $[0,1]$, denoted $U\\sim \\mathcal U(0, 1)$, transforming $U$ through the inverse cdf $F^{-1}_X$, also known as *quantile function*, gives us a random variable $X$ with distribution $P_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability mass function\n",
    "\n",
    "We often specify a probability distribution via a parametric function that relates the mass of an outcome to the outcome itself and a set of numerical parameters. The probability mass function (pmf) is a useful device to specify probability distributions through general mathematical laws (functions), rather than one probability value at a time. We normally name the pmf for convenience. \n",
    "\n",
    "For example, $f_\\theta(x) = \\theta^x(1-\\theta)^{1-x}$ is the pmf of a binary random variable that takes the value 1 with probability $\\theta$, and the value 0 with probability $1-\\theta$. The probability distribution $P_X$ such that $P_X(X=x)=f_\\theta(x)$ also goes by the name of Bernoulli distribution. \n",
    "\n",
    "Standard probability distributions are usually \"named\", which helps us remember the pmfs that prescribe them. We will see some examples later.\n",
    "\n",
    "**Expected value**\n",
    "\n",
    "The expected value (or mean) of a random variable is denoted $\\mathbb E[X]$ and defined as $\\mathbb E[X] = \\sum_{x \\in \\mathrm{supp}(p_X)} x P_X(X=x)$. \n",
    "\n",
    "**Variance**\n",
    "\n",
    "The variance of a random variable is denoted $\\mathrm{Var}(X)$ and defined as $\\mathbb E[(X - \\mathbb E[X])^2] = \\mathbb E[X^2] - \\mathbb E[X]^2$.\n",
    "\n",
    "**Mode**\n",
    "\n",
    "The modes of the distribution $P_X$ of the random variable $X$ are the values $x \\in \\mathcal X$ for which $P_X(X=x)$ is maximum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM8gX1QyGAOA"
   },
   "source": [
    "## Bernoulli\n",
    "\n",
    "The [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) is the discrete probability distribution of a random variable which takes the value 1 with probability $p$ and the value 0 with probability $1-p$.\n",
    "\n",
    "**Notation** If $X \\sim \\mathrm{Bernoulli}(p)$, then $P_X(X=x)$ is given by the Bernoulli pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Bernoulli}(x|p) = \\begin{cases}\n",
    "  p & x=1\\\\\n",
    "  1-p & x=0\\\\\n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    "\n",
    "**Properties**\n",
    "\n",
    "* Support: $\\{0, 1\\}$\n",
    "* Mean: $\\mathbb E[X] = p$\n",
    "* Variance: $\\mathrm{var}(X)= \\mathbb E[X]^2 - \\mathbb E[X^2]=p(1-p)$\n",
    "* Mode(s): $1$ if $p > 0.5$, $0$ if $p < 0.5$, $\\{0, 1\\}$ if $p=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viqE9UcoNqp_"
   },
   "source": [
    "Bernoulli pmf and cdf using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2S1fhVQGf5p"
   },
   "outputs": [],
   "source": [
    "bern = st.bernoulli(0.3)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot([0, 1], bern.pmf([0, 1]), 'o')\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot([0, 1], bern.cdf([0, 1]), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4mQgHS9Ntsq"
   },
   "source": [
    "Drawing samples from Bernoulli using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s83F71dBNlbk"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(bern.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Bern$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5oe9DBsNxsF"
   },
   "source": [
    "## Categorical\n",
    "\n",
    "The [Categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) is the discrete probability distribution of a random variable that can take on one of $K$ possible categories, with the probability of each category separately specified. \n",
    "\n",
    "**Notation** If $X \\sim \\mathrm{Categorical}(\\pi_1, \\ldots, \\pi_K)$, then $P_X(X=x)$ is given by the Categorical pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Categorical}(x|\\pi_{1}, \\ldots, \\pi_K) = \\begin{cases}\n",
    "  \\pi_x & x \\in \\{1, \\ldots, K\\} \\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    "where $\\pi_k \\ge 0$ and $\\sum_{k=1}^K \\pi_k = 1$.\n",
    "Other common notations: $X \\sim \\mathrm{Categorical}(\\pi_{1:K})$, $X \\sim \\mathrm{Categorical}(\\pi_1^K)$, and $X \\sim \\mathrm{Categorical}(\\boldsymbol\\pi)$ with $\\boldsymbol\\pi \\in \\Delta_{K-1}$. \n",
    "\n",
    "The set $\\Delta_{K-1} \\subset \\mathbb R^K$ is called the *probability simplex*, it is the set of $K$-dimensional vectors whose coordinates are positive and sum to 1.\n",
    "\n",
    "**Properties**\n",
    "\n",
    "* Support: $\\{1, \\ldots, K\\}$\n",
    "* Mean: undefined (Categorical outcomes are normally not interpreted as ordinal values)\n",
    "* Variance: undefined (Categorical outcomes are normally not interpreted as ordinal values)\n",
    "* Mode(s): $\\{k: \\pi_k = \\max(\\pi_1, \\ldots, \\pi_K)\\} \\subseteq \\{1, \\ldots, K\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lVozjSPTYXt"
   },
   "source": [
    "We will make our own Categorical object imitating the scipy API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pt_yTAuPlpZ"
   },
   "outputs": [],
   "source": [
    "class Categorical:\n",
    "    \n",
    "    def __init__(self, probs):\n",
    "        self._probs = np.array(probs)\n",
    "        assert self._probs.size > 1, \"We need 2 or more classes\"\n",
    "        assert self._probs.shape == (self._probs.size,), \"probs must be a vector\"\n",
    "        assert np.all(self._probs >= 0), \"The coordinates of the Categorical parameter must be positve\"\n",
    "        assert self._probs.sum() == 1, \"The coordinates of the Categorical parameter must add to 1\"\n",
    "        \n",
    "    def rvs(self, size=None):\n",
    "        \"\"\"Use this to draw 1 or more samples from the distribution\"\"\"\n",
    "        # we shift by one because random.choice returns 0-based outcomes\n",
    "        return np.random.choice(len(self._probs), p=self._probs, size=size) + 1\n",
    "\n",
    "    def pmf(self, x):        \n",
    "        \"\"\"Use this to assess the probability mass of the elements of a data vector\"\"\"\n",
    "        x = np.array(x, dtype=int) - 1 # convert to 0-based\n",
    "        return self._probs[x]\n",
    "    \n",
    "    def logpmf(self, x):\n",
    "        \"\"\"Use this to assess the logarithm of the probability mass of the elements of a data vector\"\"\"\n",
    "        x = np.array(x, dtype=int) - 1 # covert to 0-based\n",
    "        return np.log(self._probs[x])\n",
    "\n",
    "    def cdf(self, x):\n",
    "        if type(x) is int:\n",
    "            return self._probs[:x].sum()\n",
    "        else:\n",
    "            x = np.array(x, dtype=int) - 1. # convert to 0-based      \n",
    "            # flags\n",
    "            b = x[:, None] >= np.arange(self._probs.size)[None, :]\n",
    "        # cdf\n",
    "        return (b * self._probs).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKVe2erTNyql"
   },
   "outputs": [],
   "source": [
    "cat = Categorical([0.2, 0.1, 0.3, 0.4])\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot([1, 2, 3, 4], cat.pmf([1, 2, 3, 4]), 'o')\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot([1, 2, 3, 4], cat.cdf([1, 2, 3, 4]), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMjnuOqqP07A"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(cat.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$X \\sim Cat$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1nW8HC4Tede"
   },
   "source": [
    "## Binomial\n",
    "\n",
    "The [Binomial distribution](https://en.m.wikipedia.org/wiki/Binomial_distribution) with parameters $n$ and $p$ is the discrete probability distribution of the number of successes in a sequence of $n$ independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability $p$) or failure (with probability $1 − p$). \n",
    "\n",
    "**Notation** If $X \\sim \\mathrm{Binomial}(n, p)$, then $P_X(X=x)$ is given by the Binomial pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Binomial}(x|n, p) = \\begin{cases}\n",
    "  \\binom{n}{x} p^x(1-p)^{n-x}& x \\in \\{0, \\ldots, n\\} \\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    " where $n > 0$, $0 \\le p \\le 1$, and $\\binom{n}{x} = \\frac{n!}{x!(n-x)!}$, pronounced $n$-choose-$k$, is the binomial coefficient.\n",
    "\n",
    "**Properties**\n",
    "\n",
    "* Support: $\\{0, \\ldots, n\\}$\n",
    "* Mean: $\\mathbb E[X] = np$\n",
    "* Variance: $\\mathrm{var}(X)= \\mathbb E[X]^2 - \\mathbb E[X^2]=np(1-p)$\n",
    "* Mode(s): $\\lfloor (n+1)p \\rfloor$ or $\\lceil (n+1)p\\rceil -1 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81JcCcaNTgVw"
   },
   "outputs": [],
   "source": [
    "binom = st.binom(10, 0.3)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(11), binom.pmf(np.arange(11)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 11))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(11), binom.cdf(np.arange(11)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkqC3OxgVtPP"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(binom.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Binomial$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wuM8m5_Tgwk"
   },
   "source": [
    "## Geometric\n",
    "\n",
    "The [Geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) is either one of two discrete probability distributions:\n",
    "\n",
    "* The probability distribution of the number $X$ of Bernoulli trials needed to get one success, supported on the set $\\{1, 2, \\ldots\\}$. In this case $P_X(X=x)$ is given by the pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Geometric}_1(x|p) = \\begin{cases}\n",
    "  p(1-p)^{x-1} & x \\in \\{1,2, \\ldots\\} \\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    "with $p > 0$.\n",
    "\n",
    "  * Support: $\\{1, 2, \\ldots \\}$\n",
    "  * Mean: $\\frac{1}{p}$\n",
    "  * Variance: $\\frac{1-p}{p^2}$\n",
    "  * Mode: 1\n",
    "\n",
    "* The probability distribution of the number $Y = X - 1$ of failures before the first success, supported on the set $\\{0, 1, \\ldots\\}$. In this case $P_Y(Y=y)$ is given by the pmf\n",
    "\\begin{equation}\n",
    "  \\mathrm{Geometric}_0(y|p = \\begin{cases}\n",
    "  p(1-p)^{y} & x \\in \\{0, 1, \\ldots\\} \\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation} \n",
    "with $p > 0$.\n",
    "\n",
    "  * Support: $\\{0, 1, \\ldots \\}$\n",
    "  * Mean: $\\frac{1-p}{p}$\n",
    "  * Variance: $\\frac{1-p}{p^2}$\n",
    "  * Mode: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzpxtMSig-q0"
   },
   "source": [
    "Scipy's default Geometric is Geometric1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLIs3UNHThWu"
   },
   "outputs": [],
   "source": [
    "geom1 = st.geom(0.6)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(20), geom1.pmf(np.arange(20)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 20))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(20), geom1.cdf(np.arange(20)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnWYrJ8YhK_p"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(geom1.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Geometric1$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGr6a_6VhB_2"
   },
   "source": [
    "But we can shift it using a so-called location parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ketvKrS0gwx2"
   },
   "outputs": [],
   "source": [
    "geom0 = st.geom(0.6, loc=-1)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(20), geom0.pmf(np.arange(20)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 20))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(20), geom0.cdf(np.arange(20)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkCHx1GQhOso"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(geom0.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Geometric0$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm5Dz5S9Thr_"
   },
   "source": [
    "## Poisson\n",
    "\n",
    "The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event.\n",
    "\n",
    "**Notation** If $X \\sim \\mathrm{Poisson}(\\lambda)$, then $P_X(X=x)$ is given by the Poisson pmf:\n",
    "\\begin{equation}\n",
    "  \\mathrm{Poisson}(x|\\lambda) = \\begin{cases}\n",
    "  \\frac{\\lambda^xe^{-\\lambda}}{x!} & x \\in \\mathbb N_0\\\\  \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "\\end{equation}  \n",
    "where $\\lambda > 0$ is the *rate* parameter.\n",
    "\n",
    "**Properties**\n",
    "\n",
    "* Support: $\\mathbb N_0$\n",
    "* Mean: $\\mathbb E[X] = \\lambda$\n",
    "* Variance: $\\mathrm{var}(X)= \\mathbb E[X]^2 - \\mathbb E[X^2]= \\lambda$\n",
    "* Mode(s): $\\{ \\lceil \\lambda \\rceil - 1, \\lfloor \\lambda \\rfloor \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZBTC0R9Tics"
   },
   "outputs": [],
   "source": [
    "poi = st.poisson(9)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(20), poi.pmf(np.arange(20)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 20))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(20), poi.cdf(np.arange(20)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsdcXAiLiZLQ"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(poi.rvs(size=1000), bins='auto')\n",
    "_ = plt.xlabel(r'$x \\sim Poisson$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR-UQmcITjFf"
   },
   "source": [
    "## Zipf and Zeta\n",
    "\n",
    "The [Zipf distribution](https://en.wikipedia.org/wiki/Zipf's_law) and the [Zeta distribution](https://en.wikipedia.org/wiki/Zeta_distribution)\n",
    "are closely-related distributions that relate the frequency of outcomes and their rank.\n",
    "\n",
    "\n",
    "Scipy's `zipf` distribution is in fact the Zeta distribution, and that's because the two names are often used interchangeably. Even though they are similar power laws they are not identical, the Zeta generalises the Zipf removing the need to specify the total population size.\n",
    "\n",
    "The **Zipf distribution** predicts the probability of the element with rank $x$ in a population of size $N$. If $X \\sim \\mathrm{Zipf}(N, s)$ with $N>1$ and power $s>1$, $P_X(X=x)$ is given by the Zipf pmf:\n",
    "\\begin{equation}\n",
    "\\mathrm{Zipf}(x|N, s) = \\frac{1}{k^s H_{N,s}}\n",
    "\\end{equation}\n",
    "where $H_{N,s}=\\sum_{n=1}^N \\frac{1}{n^s}$.\n",
    "\n",
    "* Support: $\\mathbb N_1$\n",
    "* Mean: $\\frac{H_{N,s-1}}{H_{N,s}}$\n",
    "* Variance: see Wikipedia\n",
    "* Mode: 1\n",
    "\n",
    "The **Zeta distribution** predicts the probability of an element of rank $x$. If $X \\sim \\mathrm{Zeta}(s)$ with $s>1$, $P_X(X=x)$ is given by the Zeta pmf:\n",
    "\\begin{equation}\n",
    "\\mathrm{Zeta}(x|s) = \\frac{1}{k^s \\zeta(s)}\n",
    "\\end{equation}\n",
    "where $\\zeta(s) = \\sum_{n=1}^\\infty \\frac{1}{n^s}$ is the [Riemann Zeta function](https://en.wikipedia.org/wiki/Riemann_zeta_function).\n",
    "\n",
    "* Support: $\\mathbb N_1$\n",
    "* Mean: $\\frac{\\zeta(s-1)}{\\zeta(s)}$\n",
    "* Variance: see Wikipedia\n",
    "* Mode: 1\n",
    "\n",
    "Power law distributions are [heavy-tailed distributions](https://en.wikipedia.org/wiki/Heavy-tailed_distribution) and because of that they usually produce very large outcomes that deviate drammatically from their modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3jR-0wkmOgl"
   },
   "source": [
    "The scipy `zipf` is in fact a Zeta distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BN8ox4G3TjqB"
   },
   "outputs": [],
   "source": [
    "zeta = st.zipf(1.1)\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "_ = ax[0].plot(np.arange(50), zeta.pmf(np.arange(50)), 'o')\n",
    "_ = ax[0].set_xlim((-1, 20))\n",
    "_ = ax[0].set_xlabel(r'$x$')\n",
    "_ = ax[0].set_ylabel(r'$P_X(X=x)$')\n",
    "_ = ax[1].plot(np.arange(50), zeta.cdf(np.arange(50)), 'x')\n",
    "_ = ax[1].set_xlabel(r'$x$')\n",
    "_ = ax[1].set_ylabel(r'$F_X(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TedFKqcUm19p"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(zeta.rvs(size=1000), bins=100)\n",
    "_ = plt.xlabel(r'$X \\sim Zeta$')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqlCLXrSm7NP"
   },
   "source": [
    "The easiest way to recognise a Zipf/Zeta distribution (or a power law in general) is to plot a log-log plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHu-Sx9pmkZY"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(np.log(zeta.rvs(size=1000)), bins=100, log=True)\n",
    "_ = plt.xlabel(r'$\\log x$ for $X \\sim Zeta$')\n",
    "_ = plt.ylabel('Log count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-P99v1aTlBB"
   },
   "source": [
    "## Multinomial\n",
    "\n",
    "The [Multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) is a generalization of the binomial distribution. It models the probability of counts for each side of a $K$-sided die rolled $n$ times. For $n$ independent trials each of which leads to a success for exactly one of $K$ categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.\n",
    "\n",
    "A multinomial random variable is a *vector-valued* random variable with $K$ dimensions, we denote it $X \\sim \\mathrm{Multinomial}(n, \\pi_{1:K})$ for $\\pi_{1:K} \\in \\Delta_{K-1}$. The probability $P_X(X=x)$ is given by \n",
    "\\begin{equation}\n",
    "\\mathrm{Multinomial}(x|n, \\pi_{1:K}) = \\frac{n!}{\\prod_{k=1}^K x_k!} \\prod_{k=1}^K \\pi_k^{x_k}\n",
    "\\end{equation}\n",
    "for $\\{x \\in \\mathbb N^K: \\sum_{k=1}^K x_k = N\\}$.\n",
    "\n",
    "* Support: $\\{x \\in \\mathbb N^K: \\sum_{k=1}^K x_k = n\\}$, that is, the subset of $K$ dimensional positive count vectors that add to $n$.\n",
    "* Mean: per coordinate $\\mathbb E[X_k] = \\pi_k$\n",
    "* Variance: per coordinate $n\\pi_k(1-\\pi_k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dba-d5JoAaG"
   },
   "source": [
    "As this is a vector-valued rv, we cannot easily plot its pmf. But we can easily use scipy to obtain draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz5iEwiGTmD6"
   },
   "outputs": [],
   "source": [
    "multi = st.multinomial(10, [0.1, 0.2, 0.7])\n",
    "x = multi.rvs(size=5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My1t3f_YoN_m"
   },
   "source": [
    "To assess the pmf for a given sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHfQQR_iXjkq"
   },
   "outputs": [],
   "source": [
    "multi.pmf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9AHNek3XqNr"
   },
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "We are given a data set of $N$ observations $x_1, \\ldots, x_N$, which we assume were drawn independently from a given distribution $P_X$ whose pmf has parameter $\\theta$, i.e., $P_X(X=x)=f_\\theta(x)$. \n",
    "\n",
    "The **likelihood function** assigns the value\n",
    "\\begin{equation}\n",
    "L_{\\mathcal D}(\\theta) =  \\prod_{n=1}^N f_\\theta(x_n) ~.\n",
    "\\end{equation}\n",
    "\n",
    "Frequentist point estimation tells us to search for the parameter value that maximises the likelihood function given a fixed dataset, or, equivalently, maximises the logarithm of the likelihood function:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{(\\text{MLE})} &= \\arg\\max_\\theta ~ \\mathcal L_{\\mathcal D}(\\theta) \\\\\n",
    "&= \\arg\\max_\\theta ~ \\log L_{\\mathcal D}(\\theta) \\\\\n",
    "&= \\arg\\max_\\theta ~ \\sum_{n=1}^N \\log f_\\theta(x_n)\n",
    "\\end{align}\n",
    "\n",
    "In some cases, the MLE solution can be obtained in closed-form by solving $\\nabla_\\theta \\mathcal L_{\\mathcal D}(\\theta) = \\mathbf 0$. In other cases we have to design numerical algorithms to approximate it.  \n",
    "\n",
    "Here we state (without proof) the MLE solutions for a few classic distributions.\n",
    "\n",
    "\n",
    "**Bernoulli.** For $X \\sim \\mathrm{Bernoulli}(p)$\n",
    "\n",
    "\\begin{equation}\n",
    "p = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "\\end{equation}\n",
    "\n",
    "**Categorical.**  For $X \\sim \\mathrm{Categorical}(\\pi_{1:K})$ the MLE per coordinate is\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_k = \\frac{\\sum_{n=1}^N [x_n = k]}{N}\n",
    "\\end{equation}\n",
    "\n",
    "**Binomial.** For $X \\sim \\mathrm{Binomial}(N, p)$\n",
    "\n",
    "\\begin{equation}\n",
    "p = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "\\end{equation}\n",
    "\n",
    "**Geometric.** For $X \\sim \\mathrm{Geometric}_1(p)$\n",
    "\n",
    "\\begin{equation}\n",
    "p = \\frac{N}{\\sum_{n=1}^N x_n} \n",
    "\\end{equation}\n",
    "\n",
    "**Poisson.** For $X \\sim \\mathrm{Poisson}(\\lambda)$\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "\\end{equation}\n",
    "\n",
    "**Zipf and Zeta.** No closed-form expression, but in T1 you develop a numerical approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dp0hmVcXkC3"
   },
   "source": [
    "# Recognise\n",
    "\n",
    "Consider the following very simplistic model of words in English:\n",
    "\n",
    "* An English word has 3 slots, namely, the prefix, the root, and the suffix. Example: unlikely (un-, -like-, -ly).\n",
    "* Every English word has a root.\n",
    "* With probability $p$ the prefix is non-empty.\n",
    "* With probability $q$ the suffix is non-empty.\n",
    "* In this simplistic model, we decide on whether or not to fill in the prefix slot independently of what root we have and independently of whether or not the suffix is slot is non-empty. Similarly, the suffix slot is filled in independently of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu5WjIHebFUE"
   },
   "source": [
    "**Exercise with solution** Describe the probability distribution of the number $N$ of number parts (measured as the number of non-empty slots) of English words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNod7EoFYXIE"
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "To be discussed in class.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXhd5oOTbOGt"
   },
   "source": [
    "**Exercise with solution** Assume $p=q$, prescribe the distribution of the number of parts $N$ using a standard distribution from the options in section 1. Make sure to state its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s61Td10HbePB"
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "To be discussed in class.\n",
    "\n",
    "---\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV29Lc3_Xnbz"
   },
   "source": [
    "# Estimate\n",
    "\n",
    "Assume we obtain a dataset containing $M$ English words, each segmented into a triple (prefix, root, suffix) where we use \"\" to denote the empty string.\n",
    "\n",
    "Dataset: $\\mathcal D = \\{(A=a_m, B=b_m, C=c_m)_{m=1}^M\\}$, where $A$ is the prefix, $B$ is the root and $C$ is the suffix.\n",
    "\n",
    "Using $\\mathcal D$ and the simplistic model of English words (the general version where $p$ and $q$ are not necessarily the same), give an expression for the MLE solutions for $p$ and $q$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90BDE1nvdajJ"
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "To be discussed in class.\n",
    "\n",
    "---\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvkBV-sMXn-k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBLba60VH5BQhkA+fqQtEZ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2022/HC1b-prep",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
