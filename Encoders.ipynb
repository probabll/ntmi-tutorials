{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open this notebook on Colab](https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/Encoders.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzvWfS-ELNzE"
   },
   "source": [
    "# Guide\n",
    "\n",
    "* Check the entire notebook before you get started, this gives you an idea of what lies ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqR7WUDXLeME"
   },
   "source": [
    "## ILOs\n",
    "\n",
    "After completing this lab you should be able to\n",
    "\n",
    "* specify neural text encoders in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBR2bPwLL9gj"
   },
   "source": [
    "## General Notes\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3.\n",
    "* Use Torch. \n",
    "* This tutorial runs on CPU with no problem.\n",
    "\n",
    "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up.\n",
    "\n",
    "If you need a short introduction to PyTorch [check this tutorial](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqDZh0QJJsOu",
    "tags": [
     "toc"
    ]
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    "### Topics \n",
    "\n",
    "* [Text Encoders](#sec:Text_Encoders)\n",
    "* [Vocabulary](#sec:Vocabulary)\n",
    "* [From Tokens to Vectors](#sec:From_Tokens_to_Vectors)\n",
    "\t* [One-Hot Encoding](#sec:One-Hot_Encoding)\n",
    "\t* [Word embeddings](#sec:Word_embeddings)\n",
    "* [Pooling from multiple vectors](#sec:Pooling_from_multiple_vectors)\n",
    "\t* [Sum pooling](#sec:Sum_pooling)\n",
    "\t* [Average pooling](#sec:Average_pooling)\n",
    "* [Mapping from one real coordinate space to another](#sec:Mapping_from_one_real_coordinate_space_to_another)\n",
    "\t* [Linear transformation](#sec:Linear_transformation)\n",
    "\t* [Nonlinear activation functions](#sec:Nonlinear_activation_functions)\n",
    "* [Composing multiple vectors](#sec:Composing_multiple_vectors)\n",
    "\t* [Concatenation](#sec:Concatenation)\n",
    "\t* [Feed forward network](#sec:Feed_forward_network)\n",
    "\t* [Recurrent neural network encoder](#sec:Recurrent_neural_network_encoder)\n",
    "\t \t* [Bidirectional RNN encoder](#sec:Bidirectional_RNN_encoder)\n",
    "\n",
    "\n",
    "### Table of ungraded exercises\n",
    "\n",
    "1. [One-hot encoding](#ungraded-1)\n",
    "1. [Token embedding](#ungraded-2)\n",
    "1. [Sum pooling](#ungraded-3)\n",
    "1. [Average pooling](#ungraded-4)\n",
    "1. [Limitations of pooling](#ungraded-5)\n",
    "1. [Linear transformation](#ungraded-6)\n",
    "1. [Activation functions](#ungraded-7)\n",
    "1. [Concatenation](#ungraded-8)\n",
    "1. [FFNN](#ungraded-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVnfg0kMLrsi"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzjCfsJDNL1B"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV4oRuW-XYED"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjldtg5dJW7a",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Text_Encoders'></a>\n",
    "# Text Encoders\n",
    "\n",
    "In NLP applications, we often have to *encode* a piece of text, that is, map it to one (or more) vector(s) in some real coordinate space. For example, that is the case in text classification.\n",
    "\n",
    "In this section we will discuss standard NN building blocks and how to use them to encode a document (i.e., turn a document into features) and then map that encoding to the parameters of our choice of probability mass function (pmf). \n",
    "Whenever a neural network has parameters of its own, these are initialised in some standard way (typically at random). At initialisation, these parameters are uniformative. That is, we can use the NN, but the outputs are not optimised for any specific purpose. We will implement a training procedure later, for now, it is sufficient to focus on how to specify the NN functions and making sure we understand their inputs, their outputs and how what operations they perform. \n",
    "\n",
    "Throughout, we will focus on building blocks useful in the design of $C$-way text classifiers. We assume a *document* is a sequence $x=\\langle w_1, \\ldots, w_l \\rangle$ of $l$ tokens, each token comes from a vocabulary $\\mathcal V$ of $V$ tokens. The label space $\\mathcal C$ of our text classifier is made of $C$ classes. Hence, our goal is to map from any given $x$ to a $C$-dimensional probability vector $\\boldsymbol \\pi^{(x)} \\in \\Delta_{C-1}$.\n",
    "\n",
    "\n",
    "The rough idea is as follows:\n",
    "* we convert the tokens in a document to fixed-dimensional vectors ;\n",
    "* then, we map these vectors to a single vector representing the entire document (depending on how we design this operation, it may or may not discard information such as the order in which the tokens ocurred);\n",
    "* finally, we map this document encoding to a vector of $C$ logits (and softmax gives us $C$ probabilities), which then is used to parameterise the Categorical pmf.\n",
    "\n",
    "In order to maximise the benefit of working with computation graphs, we will design our NNs to process batches of documents, where the documents in a batch may differ in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Vocabulary'></a>\n",
    "# Vocabulary\n",
    "\n",
    "The first thing we do when working with text is to map our tokens to unique 0-based integer identifiers. It does not matter which token gets which id, so long as the correspondence between tokens and ids is fixed and unique.\n",
    "\n",
    "A vocabulary often contains some special symbols, which help us design good text encoders. \n",
    "\n",
    "For example, \n",
    "\n",
    "* After training we may encounter symbols we've never seen before, to better deal with those it's convenient to have a dedicated `unk` symbol. For example, if \"otter\" was never seen in training, but occurs at one point in a document test \"a cute otter\", we change that document to \"a cute -UNK-\".\n",
    "* When working with batches of documents, we will use tensors of fixed dimensionality to store documents of variable length. The general ideal is to have the tensor be as long as needed to accommodate the longest document, as for the shorter documents, we extend them to the longest length using a special symbol `pad` symbol (later, whenever we encounter this symbol in a certain position, we know that that position is not really part of the document). For example, if we need to batch together \"the nice cat\" and \"the cat\", we instead batch together \"the nice cat\" and \"the cat -PAD-\".\n",
    "\n",
    "There are various implementations of vocabulary, here's a very basic one (for most projects we need to adapt it to our needs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Constructs a 1-to-1 map between symbols (strings) and 0-based identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reserve: list):\n",
    "        \"\"\"\n",
    "        reserve: reserve this symbols in order\n",
    "        \"\"\"\n",
    "        if len(reserve) != len(set(reserve)):\n",
    "            raise ValueError(\"Every reserved symbol must be unique\")\n",
    "        self._reserve = tuple(reserve)\n",
    "        self._sym2id = dict()\n",
    "        self._symbols = []\n",
    "        for sym in reserve:\n",
    "            self.add(sym)\n",
    "        \n",
    "    def add(self, sym: str):\n",
    "        \"\"\"Add a symbol (if it is unique) and return its index\"\"\"\n",
    "        idx = self._sym2id.get(sym, None)\n",
    "        if idx is None:\n",
    "            idx = len(self._symbols)\n",
    "            self._symbols.append(sym)\n",
    "            self._sym2id[sym] = idx\n",
    "        return idx\n",
    "    \n",
    "    def idx(self, sym):\n",
    "        \"\"\"Return the index of an existing symbol\"\"\"\n",
    "        idx = self._sym2id.get(sym, None)\n",
    "        if idx is None:\n",
    "            raise KeyError(f\"Unknown symbol {sym}\")\n",
    "        return idx\n",
    "    \n",
    "    def symbol(self, idx):\n",
    "        \"\"\"Return the symbol associated with an index\"\"\"\n",
    "        return self._symbols[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Vocabulary size\"\"\"\n",
    "        return len(self._symbols)\n",
    "    \n",
    "    def items(self):\n",
    "        \"\"\"Items in the dictionary of symbols\"\"\"\n",
    "        return self._sym2id.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary(['-PAD-', '-UNK-', '-BOS-', '-EOS-'])\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, s in vocabulary.items():\n",
    "    print(k, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary.add(\"a\"))\n",
    "print(vocabulary.add(\"the\"))\n",
    "print(vocabulary.add(\"a\")) # duplicates aren't added twice\n",
    "print(vocabulary.add(\"nice\"))\n",
    "print(vocabulary.add(\"cat\"))\n",
    "print(vocabulary.add(\"dog\"))\n",
    "print(vocabulary.add(\"rabbit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary.idx(\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary.symbol(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, s in vocabulary.items():\n",
    "    print(k, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, if we talk about a \"token\" we mean a _token id_ (i.e., the 0-based integer that identifies that symbol uniquely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:From_Tokens_to_Vectors'></a>\n",
    "# From Tokens to Vectors\n",
    "\n",
    "Neural networks are functions that take real-valued vectors as inputs and produce real-valued vectors as outputs. They are quite flexible, but not flexible enough to process _symbolic_ data without special treatment. \n",
    "\n",
    "In this section we discuss 2 techniques that we can use to map from a symbolic space, such as the set of known words (i.e., the vocabulary) to a real coordinate space (a real-valued vector space of fixed dimensionality). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:One-Hot_Encoding'></a>\n",
    "## One-Hot Encoding\n",
    "\n",
    "If we know a _finite_ set of symbols (such as our vocabulary $\\mathcal V$), and the total number of unique symbols is some number $V$, then the simplest technique to map symbols to vectors is to map each symbol $t \\in \\mathcal V$ to a vector $\\mathbf v \\in \\mathbf R^V$ such that $v_t=1$ and $v_{d\\neq t}=0$. \n",
    "\n",
    "This technique is called 'one-hot encoding' because it returns a vector whose coordinates are 0, except for one dimension (the one corresponding to the symbol of interest), which gets 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we illustrate how to obtain one-hot encodings using `F.one_hot` from torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the 10-dimensional encoding of dog (the fourth symbol in the vocabulary)\n",
    "F.one_hot(\n",
    "    torch.tensor(vocabulary.idx(\"dog\")).long(), \n",
    "    len(vocabulary) # output dimensionality\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the output dimensionality is fixed, this means we can only encode up to $V$ elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    F.one_hot(\n",
    "        torch.tensor(-1).long(), \n",
    "        len(vocabulary) # output dimensionality\n",
    "    )\n",
    "except RuntimeError:\n",
    "    print(f\"Torch is 0-based, hence for a {len(vocabulary)}-dimensional space, we can encode from 0 to {len(vocabulary)-1}\")\n",
    "    \n",
    "try: \n",
    "    F.one_hot(\n",
    "        torch.tensor(len(vocabulary)).long(), \n",
    "        len(vocabulary) # output dimensionality\n",
    "    )\n",
    "except RuntimeError:\n",
    "    print(f\"Torch is 0-based, hence for a {len(vocabulary)}-dimensional space, we can encode from 0 to {len(vocabulary)-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always work with batches of symbols, for example, a batch containing 2 documents of different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [vocabulary.idx(\"the\"), vocabulary.idx(\"nice\"), vocabulary.idx(\"dog\")], # first document \n",
    "            [vocabulary.idx(\"a\"), vocabulary.idx(\"cat\"), vocabulary.idx(\"-PAD-\")] # second document\n",
    "        ]\n",
    "    ).long(), # two example documents (already expressed as sequences of token ids)\n",
    "    len(vocabulary) # vocabulary size (pretend we only know 10 words)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-1'></a> **Ungraded Exercise 1 - One-hot encoding**\n",
    "\n",
    "1. The output of `F.one_hot` above should have dimensionality [2, 3, 10]. Can you tell why?\n",
    "2. In general, if the input batch has shape `[B, L]` and our vocabulary has `V` tokens, what's the expected output size of `F.one_hot(input_batch, V)`?\n",
    "3. How many trainable parameters are needed to specify a one-hot encoding function for a vocabulary of size `V`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. Every token id in the input should have been converted to a 10-dimensional one-hot vector.\n",
    "The input has shape [2, 3], that is, we have two sequences of length 3.\n",
    "Hence, the output should be [2, 3, 10].\n",
    "2. In general, for [B, L] inputs we expect [B, L, V] outputs.\n",
    "3. None. The one-hot encoding function does not require trainable parameters, the output is \"hard-coded\" to be exactly the one-hot representation of a symbol, for each and every symbol in the vocabulary. \n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT8JUS4nKSpt",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Word_embeddings'></a>\n",
    "## Word embeddings\n",
    "\n",
    "Our next operation is a bit more interesting. It allows us to associate with each token a vector of trainable parameters. So, instead of encoding a symbol $t \\in \\mathcal V$ into a $V$-dimensional one-hot vector, we encode it into a $D$-dimensional vector of real-numbers. We normally refer to this operation as *embedding* the token into a $D$-dimensional space. Suppose we have a table of parameters $\\mathbf E \\in \\mathbb R^{V \\times D}$, with one $D$-dimensional row for each of the known symbols in the vocabulary $\\mathcal V$. Then, for some symbol $t \\in \\mathcal V$, the embedding operation returns the vector $\\mathbf e \\in \\mathbb R^D$ that corresponds to it.\n",
    "\n",
    "Sometimes, we need to describe this operation in written form (for example, to sketch our model architecture). Here is a compact notation for it:\n",
    "\\begin{align}\n",
    "    \\mathbf e &= \\mathrm{embed}_D(t; \\mathbf E)\n",
    "\\end{align}\n",
    "\n",
    "The subscript $_D$ indicates the output dimensionality. After `;` we have the trainable parameters of the operation.\n",
    "\n",
    "\n",
    "If we had a sequence of symbols, for example, a document $w_{1:l}$, we could apply the embedding operation to each symbol in the sequence, and denote it this way:\n",
    "\\begin{align}\n",
    "    \\mathbf e_i &= \\mathrm{embed}_D(w_i; \\mathbf E) \\quad \\text{for }i=1,\\ldots, l\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use torch to specify an embedding layer. We will design a toy embedding layer, for a toy vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PU-SYeT-bim"
   },
   "outputs": [],
   "source": [
    "# this creates the layer with untrained parameters\n",
    "toy_emb_dim = 2\n",
    "toy_vocab_size = len(vocabulary)\n",
    "toy_emb = nn.Embedding(\n",
    "    num_embeddings=toy_vocab_size, \n",
    "    embedding_dim=toy_emb_dim, \n",
    ")\n",
    "toy_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that pytorch will intialise all 10 vectors for us, each 2-dimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toy_emb.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 2-dimensional embedding layer, we can plot the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if toy_emb.embedding_dim == 2:\n",
    "    _ = plt.scatter(toy_emb.weight[:,0].detach().numpy(), toy_emb.weight[:,1].detach().numpy())\n",
    "    for t, i in vocabulary.items(): # pretending our vocabulary is this toy example\n",
    "        _ = plt.annotate(t, toy_emb.weight[i].detach().numpy())\n",
    "    _ = plt.xlabel(\"Dimension 1\")\n",
    "    _ = plt.ylabel(\"Dimension 2\")    \n",
    "    _ = plt.title(\"Embeddings at initialisation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method of the embedding module will embed every token in a batch of token ids.\n",
    "Let's test test it on a toy batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qj47hI4aRiA2"
   },
   "outputs": [],
   "source": [
    "toy_batch = torch.tensor(\n",
    "    [\n",
    "        [vocabulary.idx(s) for s in \"nice cat nice dog\".split()],\n",
    "        [vocabulary.idx(s) for s in \"a dog -PAD- -PAD-\".split()],\n",
    "        [vocabulary.idx(s) for s in \"nice dog nice cat\".split()],\n",
    "        [vocabulary.idx(s) for s in \"a nice dog -PAD-\".split()]\n",
    "    ]\n",
    ")\n",
    "toy_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O93Nc_pG-qKT"
   },
   "outputs": [],
   "source": [
    "# this embeds the tokens in the sequences in the batch\n",
    "e = toy_emb(toy_batch)\n",
    "print(e.shape)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to count the number of parameters in a layer, here is some helper code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHJOVtd-NX9y"
   },
   "outputs": [],
   "source": [
    "def num_parameters(torch_module):\n",
    "    \"\"\"A helper to count the number of parameters in a torch module\"\"\"\n",
    "    return sum(np.prod(theta.shape) for theta in torch_module.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-2'></a> **Ungraded Exercise 2 - Token embedding**\n",
    "\n",
    "Assume the input batch has shape `[B, L]`, our vocabulary has size `V` and our embedding vectors are `D`-dimensional. \n",
    "\n",
    "1. Once we pass the input batch through the embedding layer, what's the output shape?\n",
    "2. How many trainable parameters do we need in order to specify an embedding layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. The output shape is `[B, L, D]` because each one of the token ids is mapped to a `D`-dimensional vector.\n",
    "2. We need $V \\times D$ trainable parameters, that is, $D$ parameters for each of the $V$ symbols in the vocabulary.\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# We can use this to verify that the output shape is correct\n",
    "\n",
    "assert toy_emb(toy_batch).shape == toy_batch.shape + (toy_emb_dim,)\n",
    "\n",
    "# We can use this to verify the size of the embedding layer\n",
    "\n",
    "assert num_parameters(toy_emb) == toy_vocab_size * toy_emb_dim, \"Embedding layers are built upon [V, D] matrices\"\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Pooling_from_multiple_vectors'></a>\n",
    "# Pooling from multiple vectors\n",
    "\n",
    "Sometimes we need to combine a variable number of $D$-dimensional vectors (they all have the same dimensionality) into a single $D$-dimensional vector, this is normally referred to as *pooling*.\n",
    "\n",
    "There are different pooling operations, some with and some without trainable parameters. In this tutorial we will only cover two simple and rather common ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Sum_pooling'></a>\n",
    "## Sum pooling \n",
    "\n",
    "The output is the elementwise sum of the inputs.\n",
    "\n",
    "* Input: a collection of $l > 0$ vectors $\\mathbf e_1, \\ldots, \\mathbf e_l$, all of the same dimensionality $D$.\n",
    "* Output $\\mathbf u \\in \\mathbb R^D$ defined as\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf u &= \\sum_{i=1}^l \\mathbf e_i\n",
    "\\end{align}\n",
    "\n",
    "That is, for each dimension $d \\in [D]$, we have\n",
    "\\begin{align}\n",
    "    u_d &=  \\sum_{i=1}^l e_{i,d}\n",
    "\\end{align}\n",
    "\n",
    "A *batched* implementation of this operation must give special treatment to PAD positions (as those should be ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_pooling(input_sequences, sequence_mask):\n",
    "    \"\"\"\n",
    "    Returns the sum of the vectors along the sequence dimension.\n",
    "    \n",
    "    input_sequences: [batch_size, max_length, D] a batch of sequences of D-dimensional vectors\n",
    "    sequence_mask: [batch_size, max_length] indicates which positions are valid (i.e., not PAD)\n",
    "        we use 1 for valid (not PAD) and 0 for PAD\n",
    "    \n",
    "    Output shape is [batch_size, D]\n",
    "    \"\"\"\n",
    "    \n",
    "    # here we replace padding positions by D-dimensional vectors of 0s, \n",
    "    #  this way those options won't contribute to the sum\n",
    "    # [batch_size, max_length, D]    \n",
    "    masked = torch.where(\n",
    "        # we create an extra axis at the end of the tensor\n",
    "        sequence_mask.unsqueeze(-1),  # this has shape [batch_size, max_length, 1]\n",
    "        input_sequences,  # this has shape [batch_size, max_length, D]\n",
    "        torch.zeros_like(input_sequences)  # this has shape [batch_size, max_length, D]\n",
    "    )\n",
    "    \n",
    "    # we sum, along the sequence dimension (second last),\n",
    "    #  the valid vectors (those that are not PAD)\n",
    "    # [batch_size, D]\n",
    "    return torch.sum(masked, dim=-2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this using our toy batch. To obtain a sequence mask, we compare the tokens in the batch to the PAD id (we are using `0` for that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toy_batch, toy_batch != vocabulary.idx(\"-PAD-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pooling(F.one_hot(toy_batch, toy_vocab_size), toy_batch != vocabulary.idx(\"-PAD-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-3'></a> **Ungraded Exercise 3 - Sum pooling**\n",
    "\n",
    "Suppose we have a batch of documents, with shape `[B, L]`, we turn the tokens into one-hot vectors of dimensionality `V`. Then, then we apply sum pooling to this input batch. \n",
    "\n",
    "1. What's the shape of the output?\n",
    "2. Can we interpret the result as a bag-of-words encoding of the documents in the batch?\n",
    "3. How many trainable parameters are needed to specify the sum pooling operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. The input batch has shape `[B, L]`, after one-hot encoding, we get `[B, L, V]`, since each token becomes a V-dimensional one-hot vector. Finally, sum pooling works along the sequence dimension, hence we have output shape `[B, V]`.\n",
    "2. Yes, that's precisely how we can interpret the output. you can see it in the toy example above. \n",
    "3. None, the sum pooling operation is an elementwise transformation of existing vectors and it does not require any trainable parameters. \n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "h = sum_pooling(F.one_hot(toy_batch, toy_vocab_size), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "# Note how the 'step' or 'time' dimension is gone\n",
    "assert h.shape == (toy_batch.shape[0], toy_vocab_size)\n",
    "# In the first document h[0], \"nice\" occurs twice:\n",
    "assert h[0, vocabulary.idx(\"nice\")] == 2\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TCR1DkBLOBM",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Average_pooling'></a>\n",
    "## Average pooling \n",
    "\n",
    "When the output is the elementwise average of the inputs, this is known as *average pooling*. \n",
    "\n",
    "* Input: a collection of $l > 0$ vectors $\\mathbf e_1, \\ldots, \\mathbf e_l$, all of the same dimensionality $D$.\n",
    "* Output $\\mathbf u \\in \\mathbb R^D$ defined as\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf u &= \\frac{1}{l} \\sum_{i=1}^l \\mathbf e_i\n",
    "\\end{align}\n",
    "\n",
    "That is, for each dimension $d \\in [D]$, we have\n",
    "\\begin{align}\n",
    "    u_d &= \\frac{1}{l} \\sum_{i=1}^l e_{i,d}\n",
    "\\end{align}\n",
    "\n",
    "A with sum pooling, a *batched* implementation of this operation must give special treatment to PAD positions (as those should be ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnEclurjA0u5"
   },
   "outputs": [],
   "source": [
    "def average_pooling(input_sequences, sequence_mask):\n",
    "    \"\"\"\n",
    "    Returns the average encoding of each sequence.\n",
    "    \n",
    "    input_sequences: [batch_size, max_length, D] a batch of sequences of D-dimensional vectors\n",
    "    sequence_mask: [batch_size, max_length] indicates which positions are valid (i.e., not PAD)\n",
    "        we use 1 for valid (not PAD) and 0 for PAD\n",
    "    \n",
    "    Output shape is [batch_size, D]\n",
    "    \"\"\"\n",
    "    \n",
    "    # here we replace padding positions by D-dimensional vectors of 0s, \n",
    "    #  this way those options won't contribute to the sum\n",
    "    # [batch_size, max_length, D]    \n",
    "    masked = torch.where(\n",
    "        # we create an extra axis at the end of the tensor\n",
    "        sequence_mask.unsqueeze(-1),  # this has shape [batch_size, max_length, 1]\n",
    "        input_sequences,  # this has shape [batch_size, max_length, D]\n",
    "        torch.zeros_like(input_sequences)  # this has shape [batch_size, max_length, D]\n",
    "    )\n",
    "    \n",
    "    # we sum, along the sequence dimension (second last),\n",
    "    #  the valid vectors (those that are not PAD)\n",
    "    # we also divide by sequence length\n",
    "    # [batch_size, D]\n",
    "    avg = torch.sum(masked, dim=-2) / torch.sum(sequence_mask.float(), dim=-1, keepdims=True)\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-4'></a> **Ungraded Exercise 4 - Average pooling**\n",
    "\n",
    "For an input batch with shape `[B, L, D]`, where `D` is the embedding dimension and `L` is max sequence length.\n",
    "\n",
    "1. What's the output shape if we do average pooling over the sequence dimension?\n",
    "2. Does average pooling require trainable parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. `[B, D]` since average pooling will eliminate the sequence dimension, by elementwise average of the vectors in the sequence\n",
    "2. No, it simply operates over the input vectors.\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# This checks the shape of the output\n",
    "h = average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "assert h.shape == (toy_batch.shape[0], toy_emb_dim)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-5'></a> **Ungraded Exercise 5 - Limitations of pooling**\n",
    "\n",
    "Can you already recognise a big limitation of `sum_pooling` or `average_pooling` (in combination with one-hot or embeddings) as a means to represent a document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "Sum or average pooling are not sensitive to the order in which the input vectors are presented. So, if the inputs to pooling do not preserve information about word order, then neither will the output. You can see that in our `toy_batch` the first and the third document are permutations of one another, hence they have the same BoW encoding and the same average embedding encoding.\n",
    "\n",
    "Later, in this tutorial, we will address this limitation.\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "bow = sum_pooling(F.one_hot(toy_batch, toy_vocab_size), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "avgemb = average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "assert torch.all(bow[0] == bow[2])\n",
    "assert torch.allclose(avgemb[0], avgemb[2])  # when comparing floating numbers, we use `allclose` rather than `all`\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Mapping_from_one_real_coordinate_space_to_another'></a>\n",
    "# Mapping from one real coordinate space to another\n",
    "\n",
    "\n",
    "Sometimes we are working with vectors of a certain dimensionality $I$ and we need to convert them to vectors of another dimensionality $O$. This is very common, for example, when mapping a document encoding (e.g., average embedding, or BoW encoding) to $C$ logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Linear_transformation'></a>\n",
    "## Linear transformation \n",
    "\n",
    "A *linear transformation* is the standard way to get this done. \n",
    "\n",
    "If $\\mathbf u \\in \\mathbb R^I$, then with $\\mathbf W \\in \\mathbb R^{O\\times I}$ and $\\mathbf b \\in \\mathbb R^O$ we can use $\\mathbf v = \\mathbf W \\mathbf u + \\mathbf b$ to obtain $\\mathbf v \\in \\mathbb R^O$.\n",
    "\n",
    "In some situations, we prefer to use this transformation without a bias vector (i.e., setting $\\mathbf b$ to a vector of $0$s); sometimes, this is also called a _projection_. \n",
    "\n",
    "Sometimes, we need to describe this operation in written form (for example, to sketch our model architecture). Here is a compact notation for it:\n",
    "\\begin{align}\n",
    "    \\mathbf v &= \\mathrm{linear}_O(\\mathbf u; \\mathbf W, \\mathbf b)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we can construct a linear transformation with its trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_linear = nn.Linear(\n",
    "    toy_emb_dim, # number of inputs\n",
    "    3 # number of outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a toy example, you can inspect the initialised parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_linear.weight, toy_linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, linear layers work on batched inputs too, so let's use the toy batch, convert its tokens to D-dimensional embeddings, combine all vectors in each sequence using average pooling, and then, finally, project each D-dimensional document encoding to 3 logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_linear(average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-6'></a> **Ungraded Exercise 6 - Linear transformation**\n",
    "\n",
    "If we have a batch of inputs with shape `[B, L]`, use a `D`-dimensional embedding layer to encode tokens,  average pooling to encode documents, and then a linear transformation to `K`-dimensional outputs:\n",
    "\n",
    "1. What's the shape of the output?\n",
    "2. How many trainable parameters are necessary to specify the linear layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. After the embedding layer we have `[B, L, D]`, after the pooling operation we have `[B, D]` and after the linear transformation we have `[B, K]`.\n",
    "2. We need to store a matrix of shape `[K, D]` and a vector of `K` biases to linearly transform `D`-dimensional vectors into `K`-dimensional vectors.\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Nonlinear_activation_functions'></a>\n",
    "## Nonlinear activation functions\n",
    "\n",
    "Sometimes we need to work on a _subspace_ of the real coordinate space, for example, where numbers are constrained to being positive, or strictly positive, or strictly positive and sum up to 1, etc.\n",
    "\n",
    "We can achieve this by working with activation functions. An activation function will not change the dimensionality of its input, and it does not require any trainable parameter, typically, an activation function is a formula that transforms a vector elementwise. \n",
    "\n",
    "For example, if $\\mathbf u \\in \\mathbb R^D$\n",
    "* $\\exp(\\mathbf u)$ will apply $\\exp(u_d)$ to each coordinate $u_d$ of $\\mathbf u$, hence returning a vector of $D$ strictly positive numbers;\n",
    "* $\\mathrm{relu}(\\mathbf u)$ will apply $\\max(0, u_d)$ to each coordinate $u_d$ of $\\mathbf u$, hence returning a vector of $D$ positive (and possibly 0) numbers;\n",
    "* $\\tanh(\\mathbf u)$ applies $\\tanh(u_d)$ to each coordinate $u_d$ of $\\mathbf u$, hence return a vector $D$ numbers in the space $(-1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(torch.tensor([-2.5, -1., 0., 1, 2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.relu(torch.tensor([-2.5, -1., 0., 1, 2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tanh(torch.tensor([-2.5, -1., 0., 1, 2.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these operations also work with batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-7'></a> **Ungraded Exercise 7 - Activation functions**\n",
    "\n",
    "Play a bit with the following activations and explain what they do:\n",
    "1. `torch.sigmoid`\n",
    "2. `F.softplus`\n",
    "3. `F.softmax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# sigmoid maps from R to (0, 1), it's appropriate to predict probability values\n",
    "torch.sigmoid(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]))\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# softplus maps from R to R+, it's appropriate to predict statistical parameters that must be strictly positive\n",
    "# like rate, scale, concentration, etc.\n",
    "F.softplus(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]))\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# softmax maps from R^K to Simplex K-1, it's appropriate to predict probability vectors (that are normalised as to sum to 1)\n",
    "print(F.softmax(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]), dim=-1))\n",
    "assert torch.allclose(torch.sum(F.softmax(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]), dim=-1), -1), torch.ones(2))\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Composing_multiple_vectors'></a>\n",
    "# Composing multiple vectors\n",
    "\n",
    "We now look into *composing* multiple vectors of fixed dimensionality into one (or more) vectors, possibly changing the dimensionality of the output (with respect to the dimensionality of the input(s)), possibly preserving information about their relative order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMgtKxTPKxBR",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Concatenation'></a>\n",
    "## Concatenation\n",
    "\n",
    "The simplest thing we can do is to concatenate vectors, in their given order. This operation does not require any trainable parameters, but it has a downside: the size of the output increases as a function of the number of inputs.\n",
    "\n",
    "For example, we can concatenate an $A$-dimensional vector $\\mathbf u$ with a $B$-dimensional vector $\\mathbf v$ obtaining a $A+B$-dimensional vector:\n",
    "\\begin{align}\n",
    "    \\mathbf h &= \\mathrm{concat}(\\mathbf u, \\mathbf v) = (u_1, \\ldots, u_{A}, v_1, \\ldots, v_{B})^\\top\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-8'></a> **Ungraded Exercise 8 - Concatenation**\n",
    "\n",
    "Concatenate the BoW encoding of the documents in the toy batch with their average embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "bow = sum_pooling(F.one_hot(toy_batch, toy_vocab_size), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "avgemb = average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "bow_avg = torch.cat([bow, avgemb], -1)\n",
    "assert bow_avg.shape[-1] == toy_vocab_size + toy_emb_dim\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Feed_forward_network'></a>\n",
    "## Feed forward network\n",
    "\n",
    "\n",
    "A basic feed-forward network (FFNN) combines two linear transformations with a non-linear activation function in between. It is also possible to have deep FFNN, by stacking additional linear transformations (and nonlinearities).\n",
    "\n",
    "This is the basic one:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf h &= a(\\mathrm{linear}_H(\\mathbf u; \\theta_{\\text{hid}})) \\\\\n",
    "    \\mathbf o &= \\mathrm{linear}_O(\\mathbf h; \\theta_{\\text{out}}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $a(\\cdot)$ is an elementwise nonlinearity (typically, $\\tanh$ or $\\mathrm{relu}$), $\\theta_{\\text{hid}}$ are the parameters of the first input-to-hidden layer (a weight matrix and a bias vector) and  $\\theta_{\\text{out}}$ are the parameters of the second hidden-to-output layer (another weight matrix and another bias vector). See that instead of writing down the parameters explicitly, we named them (using $\\theta$ and a suggestive subscript); this is a convenient notation shortcut.\n",
    "\n",
    "A nice way to prescribe FFNNs in torch is to use the so-called `Sequential` API, which stacks transformations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we concatenate two views of each document: the bag-of-words encoding and the average token embedding.\n",
    "\n",
    "We then map each such vector to 7 ReLU hidden units and then to 3 output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 7\n",
    "output_size = 3\n",
    "toy_ffnn = nn.Sequential(\n",
    "    nn.Linear(toy_vocab_size + toy_emb_dim, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_size)\n",
    ")\n",
    "assert num_parameters(toy_ffnn) == ((toy_vocab_size + toy_emb_dim)*hidden_size + hidden_size + hidden_size*output_size + output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a FFNN accepts batched inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = sum_pooling(F.one_hot(toy_batch, toy_vocab_size), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "avgemb = average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "bow_avg = torch.cat([bow, avgemb], -1)\n",
    "assert toy_ffnn(bow_avg).shape == (bow_avg.shape[0], output_size)\n",
    "toy_ffnn(bow_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-9'></a> **Ungraded Exercise 9 - FFNN**\n",
    "\n",
    "Use the Sequential API to specify a FFNN similar to the example above, but with 2 hidden layers: the first with 7 units and the second with 13 units. Use ReLU for the first hidden layer and Tanh for the second hidden layer. The output of your FFNN should have 3 units, like in the example above. Inspect the number of parameters of the layer. Then, test it on the concatenation of bow and avg encodings, as we did above. Explain why the output shape is the same as in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "hidden_sizes = [7, 13]\n",
    "output_size = 3\n",
    "toy_ffnn2 = nn.Sequential(\n",
    "    nn.Linear(toy_vocab_size + toy_emb_dim, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_sizes[1], output_size)\n",
    ")\n",
    "assert num_parameters(toy_ffnn2) == ((toy_vocab_size + toy_emb_dim)*hidden_sizes[0] + hidden_sizes[0] + hidden_sizes[0]*hidden_sizes[1] + hidden_sizes[1] + hidden_sizes[1]*output_size + output_size)\n",
    "\n",
    "assert toy_ffnn2(bow_avg).shape == (bow_avg.shape[0], output_size)\n",
    "\n",
    "assert toy_ffnn2(bow_avg).shape == toy_ffnn(bow_avg).shape\n",
    "\n",
    "# The output shape is the same as before \n",
    "# because the shape is determined by the ouptut_size \n",
    "# and not by the number of hidden layers\n",
    "toy_ffnn2(bow_avg)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chHImOvsCrnA",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Recurrent_neural_network_encoder'></a>\n",
    "## Recurrent neural network encoder\n",
    "\n",
    "\n",
    "The **RNN encoder** uses one or more feed-forward nets inside of a for loop to iterate over the steps of a sequence, at each step the RNN encoder combines a _recurrent_ state and the input at that step using a FFNN. Rather than one different FFNN per step, the RNN encoder reuses the same FFNN (that's where the _recurrent_ in its name comes from). Suppose we have a sequence of vectors $\\mathbf e_{1:l}$, for example, those are the the token embeddings for a document $w_{1:l}$.\n",
    "\n",
    "The RNN encoder has a initial state $\\mathbf u_0$, this could be a vector of 0s, for example. Then, at each step $i \\in [l]$ of the sequence, the RNN encoder combines the recurrent state $u_{i-1}$ (as obtained in the step before) with the current input $\\mathbf e_i$, this combination produces a new value $\\mathbf u_i$ for the recurrent state, which will be used in the next step. \n",
    "\n",
    "See the figure for an illustration:\n",
    "<img src=\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/img/rnn-encoder.jpeg\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "We can describe the computation as follows:\n",
    "\\begin{align}\n",
    "\\mathbf u_i &= \\mathrm{rnnstep}_K(\\mathbf u_{i-1}, \\underline{\\mathbf e_{i}}; \\theta_{\\text{enc}}) \\quad \\text{for }i \\in [l].\n",
    "\\end{align}\n",
    "\n",
    "The simplest implementation of the block `rnnstep` looks like this:\n",
    "\\begin{align}\n",
    "\\mathrm{rnnstep}_K(\\mathbf u_{i-1}, \\underline{\\mathbf e_{i}}; \\theta_{\\text{enc}}) &= \\tanh(\\mathbf U \\mathbf u_{i-1} + \\mathbf W \\mathbf e_{i-1}) \n",
    "\\end{align}\n",
    "where the trainable parameters $\\theta_{\\text{enc}} = \\{\\mathbf U \\in \\mathbb R^{K\\times K}, \\mathbf W \\in \\mathbb R^{K\\times D}, \\}$ are two matrices that project the recurrent state and the current input to size $K$.\n",
    "\n",
    "\n",
    "If you pay close attention to the illustration, or to the formulae, you will see that the recurrent state at any one position $i$ can potentially store information from any of the inputs $\\mathbf e_1, \\ldots, \\mathbf e_{i-1}$. Besides, due to the nonlinearity of the rnnstep function, the state $\\mathbf u_i$ is sensitive to the order in which the inputs are presented (that is, if we presented inputs in a different order, the numerical values of the coordinates of $\\mathbf u_i$ might differ). This is an important aspet of a feature function for natural language processing, given that natural languages encode a lot of information in word order.\n",
    "\n",
    "In written form, the RNN encoder can be denoted even more compactly as \n",
    "\\begin{align}\n",
    "\\mathbf u_{1:l} &= \\mathrm{rnnenc}_K(\\mathbf e_{1:l}; \\theta_{\\text{enc}})\n",
    "\\end{align}\n",
    "it takes a sequence of vectors as input and returns a sequence of $K$-dimensional vectors as output. The last vector  $\\mathbf u_l$ in the output sequence has information about the entire document $x$ in order.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgTTQ_uWN0qV"
   },
   "source": [
    "### LSTM\n",
    "\n",
    "In this tutorial we will use a modern architecture for the recurrent cell called [*Long Short-Term Memory*](https://arxiv.org/pdf/1503.04069.pdf) (LSTM for short). [It's already implemented for us in torch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n",
    "\n",
    "You do not need to study the LSTM paper, what you will need to know will be explained in this tutorial. \n",
    "\n",
    "Here we briefly explain the **internal design of the LSTM**. The choice of letters we use in this part are internal to the LSTM and are not to be confused for letters used in other contexts.\n",
    "\n",
    "For a step $t$, let $\\mathbf x_t$ be an $I$-dimensional input to an LSTM (at this point it's not important whether this corresponds to a step in the token sequence of in the tag sequence, or any sequence really, it just matters that at some point in time $t$ we want to use this input to udpate the LSTM memory).\n",
    "\n",
    "At this point, the memory of an LSTM is made of two $K$-dimensional vectors called the *cell vector* $\\mathbf c_{t-1}$ and the *hidden state* $\\mathbf h_{t-1}$, each of which is $K$-dimensional. When we process the input $\\mathbf x_t$ with an LSTM, these two vectors are updated step by step as shown below:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf i_t &=\\mathrm{sigmoid}(\\mathrm{affine}_K(\\mathbf x_t; \\theta_1) + \\mathrm{affine}_K(\\mathbf h_{t-1}; \\theta_2))\\\\\n",
    "    \\mathbf f_t &=\\mathrm{sigmoid}(\\mathrm{affine}_K(\\mathbf x_t; \\theta_3) + \\mathrm{affine}_K(\\mathbf h_{t-1}; \\theta_4))\\\\\n",
    "    \\mathbf g_t &=\\tanh(\\mathrm{affine}_K(\\mathbf x_t; \\theta_5) + \\mathrm{affine}_K(\\mathbf h_{t-1}; \\theta_6))\\\\\n",
    "    \\mathbf o_t&=\\mathrm{sigmoid}(\\mathrm{affine}_K(\\mathbf x_t; \\theta_7) + \\mathrm{affine}_K(\\mathbf h_{t-1}; \\theta_8))\\\\\n",
    "    \\mathbf c_t &= \\mathbf f_t \\odot \\mathbf c_{t-1} + \\mathbf i_t \\odot \\mathbf g_t \\\\\n",
    "    \\mathbf h_t &= \\mathbf o_t \\odot \\tanh(\\mathbf c_t)\n",
    "\\end{align}\n",
    "\n",
    "The first four steps compute the following using the input and the hidden state: the *input gate* $\\mathbf i_t$, then the *forget gate* $\\mathbf f_t$, the *draft cell* $\\mathbf g_t$, and the *output gate* $\\mathbf o_t$. \n",
    "These are all $K$-dimensional, and the affine transformations all have their own parameters (there 8 such affine transformations in total, they map either from $I$ dimensions to $K$ dimensions, or from $K$ dimensions to $K$ dimensions, and they have biases vectors in them). The last couple of steps finally uppdate the cell and the hidden state by combining the intermediate gates and draft cell. The symbol $\\odot$ denotes elementwise multiplication. After the update the LSTM memory is made of two states $\\mathbf c_t$ and $\\mathbf h_t$, which are $K$-dimensional. Typically, the cell is internal to the LSTM and we rarely need to use it for anything outside of it. It is the hidden state after each update that we normally want to use in applications.  The torch implementation gives us access to both of them, and we will see later how to use it.\n",
    "\n",
    "Compared to our illustration of the RNN encoder, we will have token embeddings as input, and we will interpret hidden states as outputs of the LSTM. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcyP1TxcC6Q5"
   },
   "outputs": [],
   "source": [
    "toy_hidden_size = 6\n",
    "toy_lstm = nn.LSTM(\n",
    "    input_size=toy_emb_dim, # size of the vectors in the input sequence\n",
    "    hidden_size=toy_hidden_size, # size of the recurrent cell\n",
    "    num_layers=1,\n",
    "    batch_first=True,  # this is important, it's telling the nn.LSTM class\n",
    "    bidirectional=False,  # we will explain this argument in the next example\n",
    ")\n",
    "toy_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9F1CDM5UNwcp"
   },
   "outputs": [],
   "source": [
    "num_parameters(toy_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dG1fCFVnDLCO"
   },
   "outputs": [],
   "source": [
    "# [batch_size, max_len, emb_dim]\n",
    "e = toy_emb(toy_batch)\n",
    "# [batch_size, max_len, hidden_dim]\n",
    "# internally, the LSTM maintains two vectors in the memory\n",
    "# the forward method of the LSTM class will return \n",
    "# a tensor which has the sequence of so called hidden states (this is usually what you want to use in a text encoder)\n",
    "# and tuple of tensors that can be used in case you need access to the internal \n",
    "# mechanism of the LSTM cell\n",
    "\n",
    "# For convenient, torch provides two auxiliary functions that help us deal with \n",
    "# batches of sequences that may differ in length\n",
    "# First, we pack the padded sequences in a special way using `pack_padded_sequence`\n",
    "#  for this to work correctly, torch needs to know the length of the sequences (discounting padding)\n",
    "# [batch_size]\n",
    "lengths = (toy_batch != vocabulary.idx(\"-PAD-\")).long().sum(-1)\n",
    "packed_seqs = pack_padded_sequence(e, lengths.cpu(), batch_first=True, enforce_sorted=False)        \n",
    "# it's important to tell torch that the first axis of our tensors is for the batch (with `batch_first=True`)\n",
    "# it's also important to tell torch that our sequences are _not_ sorted by length (with `enfore_sorted=False`)\n",
    "\n",
    "# Next, we run the LSTM on packed sequences,\n",
    "#  this returns, for every sequence, the states for all steps and a tuple (final state, final cell)\n",
    "h, (last_h, last_c) = toy_lstm(packed_seqs)\n",
    "\n",
    "# Finally, before going ahead and using `h`, we call `pad_packed_sequence`\n",
    "#  this returns the tensor of states with padding positions zeroed out\n",
    "h, _ = pad_packed_sequence(h, batch_first=True)\n",
    "\n",
    "assert h.shape == toy_batch.shape + (toy_hidden_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h)  # se how the state information is zeroed out for the -PAD- positions\n",
    "# this is because we used torch's helper functions (pack_padded_sequence and pad_packed_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_h) # this tensor, returned by the LSTM class gives us convenient access to the last state of each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to combine the LSTM outputs into a single vector, we could use average pooling, for example. This does not destroy the infromation about word order, since that information is already coded in the LSTM outputs.\n",
    "Alternatively, we could use the last state of the sequence. This is a practical choice, and there's no theory to support one option or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIie8FNzMblI",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Bidirectional_RNN_encoder'></a>\n",
    "### Bidirectional RNN encoder\n",
    "\n",
    "In the previous section, when encoding a document $x$, we processed the words from left-to-right. In reality, there's no need to limit ourselves that order. Rather, we have access to the document anyway, so why not also encode it from right-to-left. This way, whenever we look at a given position $i$, we can obtain information from the left (i.e., from $\\mathbf e_1, \\ldots, \\mathbf e_i$) and from the right (i.e., from $\\mathbf e_i, \\ldots, \\mathbf e_l$). This can be useful in certain tasks.  \n",
    "\n",
    "An RNN cell, by design, makes computations in a single direction (e.g., left-to-right), but we can use 2 different RNN cells, one that reads the sequence in one order and another that reads the sequence in reversed order.\n",
    "\n",
    "We don't need to invent a new RNN cell for this, we can simply reverse the inputs to a standard RNN cell:\n",
    "\\begin{align}\n",
    "\\mathbf r_{1:l} &= \\mathrm{reverse}(\\mathbf e_{1:l}) \\\\\n",
    "\\mathbf v_i &= \\mathrm{rnnstep}_K(\\mathbf v_{i-1}, \\mathbf r_{i}; \\theta_{\\text{renc}}) ~.\n",
    "\\end{align}\n",
    "Because the inputs have been reversed in order. For example, in a sentence of length $l=10$, $\\mathbf v_2$ knows about $w_{9}$ through $\\mathbf r_2$ and about $w_{>9}$ through $\\mathbf v_{1}$. The last state $\\mathbf v_l$ has information about the entire document $w_{1:l}$, but processed it in reversed order.\n",
    "\n",
    "Therefore a reversed RNN encoder can be denoted compactly as follows:\n",
    "\\begin{align}\n",
    "\\mathbf v_{1:l} &= \\mathrm{rnnenc}_K(\\mathrm{reverse}(\\mathbf e_{1:l}); \\theta_{\\text{renc}})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Note that we named the parameter set differently: $\\theta_{\\text{enc}}$ for the first RNN cell, and $\\theta_{\\text{renc}}$ for the second one, that's because we indeed want to have two different sets of parameters. If we used the same set of parameters for both directions, that probably would not work very well, as reading in one direction and reading in another are conceptually two different operations.\n",
    "\n",
    "The **bidirectional RNN encoder** is our prefered text encoder, it can be denoted as follows:\n",
    "\\begin{align}\n",
    "\\mathbf o_{1:l} &= \\mathrm{birnn}_{2K}(\\mathbf e_{1:l}; \\theta_{\\text{enc}} \\cup \\theta_{\\text{renc}})\n",
    "\\end{align}\n",
    "and here are the operations that it performs:\n",
    "\\begin{align}\n",
    "\\mathbf u_{1:l} &= \\mathrm{rnnenc}_K(\\mathbf e_{1:l}; \\theta_{\\text{enc}})\\\\\n",
    "\\mathbf v_{1:l} &= \\mathrm{rnnenc}_K(\\mathrm{reverse}(\\mathbf e_{1:l}); \\theta_{\\text{renc}})\\\\\n",
    "\\mathbf o_{i} &= \\mathrm{concat}(\\mathbf u_i, \\mathbf v_{l-i+1}) & \\text{for }i \\in \\{1, \\ldots, l\\}\n",
    "\\end{align}\n",
    "\n",
    "Its outputs are $2K$-dimensional because after processing the sequence from left-to-right with the first RNN encoder and from right-to-left with the second RNN encoder, it then concatenates the two views of the process in such a way that $\\mathbf o_i$ has information about $w_i$, $w_{<i}$ and $w_{>i}$.\n",
    "\n",
    "See the figure as an illustration of how the two RNN cells can be used to obtain the bidirectional RNN encoder: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/img/birnn.jpeg\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Luckily, `nn.LSTM` implements all that for us, and we don't really need to worry about reversing anything ourserlves. See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZEXWM3rDZpW"
   },
   "outputs": [],
   "source": [
    "toy_bilstm = nn.LSTM(\n",
    "    input_size=toy_emb_dim,\n",
    "    hidden_size=toy_hidden_size,\n",
    "    num_layers=1,\n",
    "    batch_first=True,\n",
    "    bidirectional=True,  # now we employ one LSTM for each direction\n",
    ")\n",
    "toy_bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parameters(toy_bilstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhi8SmRKUeJp"
   },
   "outputs": [],
   "source": [
    "assert num_parameters(toy_bilstm) == 2*num_parameters(toy_lstm), \"A BiLSTM is made of two LSTMs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM7iWWAGDd65"
   },
   "outputs": [],
   "source": [
    "# [batch_size, max_len, 2*hidden_dim]\n",
    "# as for the standard LSTM, we only use the first of its outputs, namely, \n",
    "# a tensor of states, this time the states are concatenated for two directions, \n",
    "# thus they will be twice as large\n",
    "\n",
    "# As before, we use torch's helper functions to correctly deal with the variable length\n",
    "# of our padded sequences\n",
    "packed_seqs = pack_padded_sequence(e, lengths.cpu(), batch_first=True, enforce_sorted=False)        \n",
    "h2, (last_h2, last_c2) = toy_bilstm(packed_seqs)\n",
    "h2, _ = pad_packed_sequence(h2, batch_first=True)\n",
    "assert h2.shape == toy_batch.shape + (2*toy_hidden_size,)  # BiLSTM outputs are the concatenation of 2 LSTM outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2.shape # the shape here is as expected [batch_size, max_len, 2*hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_h2.shape  # the shape here is a bit different: [num_layers, batch_size, hidden_dim]\n",
    "# where each direction counts as 1 layer, hence num_layers is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to concatenate the final states of each sequence\n",
    "# we can first move the first axis (num_layers) to the end of the tensor using `permute`\n",
    "# and then flatten the last two axis (hidden_dim, num_layers)\n",
    "# obtaining output shape [batch_size, num_layers * hidden_dim]\n",
    "torch.flatten(torch.permute(last_h2, (1, 2, 0)), 1, 2).shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNuHBiG349O5I8O42aWP8nQ",
   "collapsed_sections": [],
   "include_colab_link": false,
   "name": "Encoders.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
