{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open this notebook on Colab](https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/Encoders.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "jzvWfS-ELNzE",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Guide\n",
    "\n",
    "Neural networks (NNs) are functions that take real-valued vectors as inputs and produce real-valued vectors as outputs. They are quite flexible, but not flexible enough to process _symbolic_ data (such as language) without special treatment. \n",
    "\n",
    "In this notebook, we discuss techniques that we can use to map from a symbolic space, such as a finite countable set (e.g., the vocabulary of a language) to a real coordinate space (a real-valued vector space of fixed dimensionality). \n",
    "\n",
    "This notebook stands as lecture notes for the _Feature Learning_ module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "KqR7WUDXLeME",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ILOs\n",
    "\n",
    "After completing this lab you should be able to\n",
    "\n",
    "* specify neural text encoders in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "YBR2bPwLL9gj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## General Notes\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3.\n",
    "* Use Torch. \n",
    "* This tutorial runs on CPU with no problem.\n",
    "\n",
    "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up.\n",
    "\n",
    "If you need a short introduction to PyTorch [check this tutorial](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "CqDZh0QJJsOu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "toc"
    ]
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    "### Topics \n",
    "\n",
    "* [Text Encoders](#sec:Text_Encoders)\n",
    "* [Vocabulary](#sec:Vocabulary)\n",
    "* [From Tokens to Vectors](#sec:From_Tokens_to_Vectors)\n",
    "\t* [One-Hot Encoding](#sec:One-Hot_Encoding)\n",
    "\t* [Word embeddings](#sec:Word_embeddings)\n",
    "* [Pooling from multiple vectors](#sec:Pooling_from_multiple_vectors)\n",
    "\t* [Sum pooling](#sec:Sum_pooling)\n",
    "\t* [Average pooling](#sec:Average_pooling)\n",
    "* [Mapping from one real coordinate space to another](#sec:Mapping_from_one_real_coordinate_space_to_another)\n",
    "\t* [Linear transformation](#sec:Linear_transformation)\n",
    "\t* [Nonlinear activation functions](#sec:Nonlinear_activation_functions)\n",
    "* [Composing multiple vectors](#sec:Composing_multiple_vectors)\n",
    "\t* [Concatenation](#sec:Concatenation)\n",
    "\t* [Feed forward network](#sec:Feed_forward_network)\n",
    "\t* [Recurrent neural network](#sec:Recurrent_neural_network)\n",
    "\t \t* [LSTM](#sec:LSTM)\n",
    "\t \t* [Bidirectional RNN encoder](#sec:Bidirectional_RNN_encoder)\n",
    "\n",
    "\n",
    "### Table of ungraded exercises\n",
    "\n",
    "1. [One-hot encoding](#ungraded-1)\n",
    "1. [Token embedding](#ungraded-2)\n",
    "1. [Sum pooling](#ungraded-3)\n",
    "1. [Average pooling](#ungraded-4)\n",
    "1. [Limitations of pooling](#ungraded-5)\n",
    "1. [Linear transformation](#ungraded-6)\n",
    "1. [Activation functions](#ungraded-7)\n",
    "1. [Concatenation](#ungraded-8)\n",
    "1. [FFNN](#ungraded-9)\n",
    "1. [BiLSTM](#ungraded-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "fVnfg0kMLrsi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "LzjCfsJDNL1B",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "QV4oRuW-XYED",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "kjldtg5dJW7a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Text_Encoders'></a>\n",
    "# Text Encoders\n",
    "\n",
    "In NLP applications, we often have to *encode* a piece of text, that is, map it to one (or more) vector(s) in some real coordinate space. For example, that is the case in text classification.\n",
    "\n",
    "In this section we will discuss standard NN building blocks and how to use them to encode a document (i.e., turn a document into features) and then map that encoding to the parameters of our choice of probability mass function (pmf). \n",
    "Whenever a neural network has parameters of its own, these are initialised in some standard way (typically at random). At initialisation, these parameters are uniformative. That is, we can use the NN, but the outputs are not optimised for any specific purpose. You will implement a training procedure in T4, for now, it is sufficient to focus on how to specify the NN functions and making sure we understand their inputs, their outputs and what operations they perform. \n",
    "\n",
    "Throughout, we will focus on building blocks useful in the design of $C$-way text classifiers. We assume a *document* is a sequence $x=\\langle w_1, \\ldots, w_l \\rangle$ of $l$ tokens, each token comes from a vocabulary $\\mathcal V$ of $V$ tokens. The label space $\\mathcal C$ of our text classifier is made of $C$ classes. Hence, our goal is to map from any given $x$ to a $C$-dimensional probability vector $\\boldsymbol \\pi^{(x)} \\in \\Delta_{C-1}$.\n",
    "\n",
    "\n",
    "The rough idea is as follows:\n",
    "* we convert the tokens in a document to fixed-dimensional vectors ;\n",
    "* then, we map these vectors to a single vector representing the entire document (depending on how we design this operation, it may or may not discard information such as the order in which the tokens ocurred);\n",
    "* finally, we map this document encoding to a vector of $C$ logits (and softmax gives us $C$ probabilities), which then is used to parameterise the Categorical pmf.\n",
    "\n",
    "In order to maximise the benefit of working with computation graphs, we will design our NNs to process batches of documents, where the documents in a batch may differ in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Vocabulary'></a>\n",
    "# Vocabulary\n",
    "\n",
    "\n",
    "\n",
    "The first thing we do when working with text is to map our tokens to unique 0-based integer identifiers (ids). It does not matter which token gets which id, so long as the correspondence between tokens and ids is fixed and unique.\n",
    "\n",
    "Here's a toy example: a vocabulary of 10 known symbols and their unique 0-based integer identifiers. The first 4 symbols are reserved for special use, the remaining ones are words in our toy language.\n",
    "```\n",
    "id | symbol\n",
    "---| -------\n",
    "0  | -PAD-\n",
    "1  | -BOS-\n",
    "2  | -EOS-\n",
    "3  | -UNK-\n",
    "4  | and\n",
    "5  | are\n",
    "6  | awesome\n",
    "7  | cats\n",
    "8  | cute\n",
    "9  | dogs\n",
    "```\n",
    "\n",
    "A vocabulary often contains some special symbols, which help us design good text encoders. \n",
    "* For example,  `-UNK-` is used in place of unknown words: if `otters` was never seen in training, but occurs at one point in  `otters are cute`, we change that document to `-UNK- are cute`.\n",
    "* When working with batches of documents of different length, we extend shorter documents to match the length of the longest one, so they can be stacked together into something like a table:\n",
    "```\n",
    "|-------|------|-------|-------|---------|\n",
    "| cats  | and  | dogs  | are   | awesome |\n",
    "| cute  | cats | -PAD- | -PAD- | -PAD-   |\n",
    "| -UNK- | are  | cute  | -PAD- | -PAD-   |\n",
    "|-------|------|-------|-------|---------|\n",
    "```\n",
    "where the special symbol `-PAD-` identifies cells that are not part of any document.\n",
    "\n",
    "There are various implementations of vocabulary, here's a very basic one (for most projects we need to adapt it to our needs, in T4 we will adapt it to a concrete application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Constructs a 1-to-1 map between symbols (strings) and 0-based identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reserve: list, default=None):\n",
    "        \"\"\"\n",
    "        reserve: reserve this symbols in order\n",
    "        \"\"\"\n",
    "        if len(reserve) != len(set(reserve)):\n",
    "            raise ValueError(\"Every reserved symbol must be unique\")\n",
    "        self._reserve = tuple(reserve)\n",
    "        self._sym2id = dict()\n",
    "        self._symbols = []        \n",
    "        for sym in reserve:\n",
    "            self.add(sym)\n",
    "        self._default = default    \n",
    "        if default is not None:\n",
    "            self.add(default)        \n",
    "        \n",
    "    def add(self, sym: str):\n",
    "        \"\"\"Add a symbol (if it is unique) and return its index\"\"\"\n",
    "        idx = self._sym2id.get(sym, None)\n",
    "        if idx is None:\n",
    "            idx = len(self._symbols)\n",
    "            self._symbols.append(sym)\n",
    "            self._sym2id[sym] = idx\n",
    "        return idx\n",
    "    \n",
    "    def idx(self, sym):\n",
    "        \"\"\"Return the index of an existing symbol\"\"\"\n",
    "        idx = self._sym2id.get(sym, None)\n",
    "        if idx is None:\n",
    "            if self._default is None:\n",
    "                raise KeyError(f\"Unknown symbol {sym}\")\n",
    "            else:\n",
    "                idx = self._sym2id.get(self._default)\n",
    "        return idx\n",
    "    \n",
    "    def symbol(self, idx):\n",
    "        \"\"\"Return the symbol associated with an index\"\"\"\n",
    "        return self._symbols[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Vocabulary size\"\"\"\n",
    "        return len(self._symbols)\n",
    "    \n",
    "    def items(self):\n",
    "        \"\"\"Items in the dictionary of symbols\"\"\"\n",
    "        return self._sym2id.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary(['-PAD-', '-BOS-', '-EOS-', '-UNK-'], default='-UNK-')\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, s in vocabulary.items():\n",
    "    print(k, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary.add(\"and\"))\n",
    "print(vocabulary.add(\"are\"))\n",
    "print(vocabulary.add(\"awesome\"))\n",
    "print(vocabulary.add(\"cats\")) \n",
    "print(vocabulary.add(\"cats\")) # duplicates aren't added twice\n",
    "print(vocabulary.add(\"cute\"))\n",
    "print(vocabulary.add(\"dogs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary.idx(\"dogs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary.symbol(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary.symbol(vocabulary.idx(\"otters\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, s in vocabulary.items():\n",
    "    print(s, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, if we talk about a \"token\" we mean a _token id_ (i.e., the 0-based integer that identifies that symbol uniquely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:From_Tokens_to_Vectors'></a>\n",
    "# From Tokens to Vectors\n",
    "\n",
    "Neural networks are functions that take real-valued vectors as inputs and produce real-valued vectors as outputs. They are quite flexible, but not flexible enough to process _symbolic_ data (such as language) without special treatment. \n",
    "\n",
    "In this section we discuss techniques that we can use to map from a symbolic space, such as the set of known words (i.e., the vocabulary) to a real coordinate space (a real-valued vector space of fixed dimensionality). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:One-Hot_Encoding'></a>\n",
    "## One-Hot Encoding\n",
    "\n",
    "\n",
    "If we know a *finite* set $\\mathcal V$ of tokens (e.g., words), and the total number of unique symbols in it is some number $V = |\\mathcal V|$, then the simplest technique to map tokens to vectors is to map each token $t \\in \\mathcal V$ to a vector $\\mathbf v = \\mathrm{onehot}_V(t)$ such that $\\mathbf v \\in \\mathbb R^V$, $v_t=1$ and $v_{d\\neq t}=0$. \n",
    "\n",
    "This technique is called _one-hot encoding_ because it returns a vector whose coordinates are $0$ for all but one dimension (that which indicates the token we are encoding), which gets 1.\n",
    "\n",
    "Example: using the vocabulary from the example, $\\mathrm{onehot}_{10}(\\texttt{awesome})= (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^\\top$.} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we illustrate how to obtain one-hot encodings using `F.one_hot` from torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the 10-dimensional encoding of 'awesome' \n",
    "F.one_hot(\n",
    "    torch.tensor(vocabulary.idx(\"awesome\")).long(), \n",
    "    len(vocabulary) # output dimensionality\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the output dimensionality is fixed, this means we can only encode up to $V$ elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    F.one_hot(\n",
    "        torch.tensor(-1).long(), \n",
    "        len(vocabulary) # output dimensionality\n",
    "    )\n",
    "except RuntimeError:\n",
    "    print(f\"Torch is 0-based, hence for a {len(vocabulary)}-dimensional space, we can encode from 0 to {len(vocabulary)-1}\")\n",
    "    \n",
    "try: \n",
    "    F.one_hot(\n",
    "        torch.tensor(len(vocabulary)).long(), \n",
    "        len(vocabulary) # output dimensionality\n",
    "    )\n",
    "except RuntimeError:\n",
    "    print(f\"Torch is 0-based, hence for a {len(vocabulary)}-dimensional space, we can encode from 0 to {len(vocabulary)-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always work with batches of symbols, for example, a batch containing 3 documents of different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [vocabulary.idx(\"cats\"), vocabulary.idx(\"and\"), vocabulary.idx(\"dogs\"), vocabulary.idx(\"are\"), vocabulary.idx(\"awesome\")], # first document \n",
    "            [vocabulary.idx(\"cute\"), vocabulary.idx(\"cats\")] + [vocabulary.idx(\"-PAD-\")] * 3, # second document\n",
    "            [vocabulary.idx(\"otters\"), vocabulary.idx(\"are\"), vocabulary.idx(\"cute\")] + [vocabulary.idx(\"-PAD-\")] * 2 # third document\n",
    "        ]\n",
    "    ).long(), # three example documents (already expressed as sequences of token ids)\n",
    "    len(vocabulary) # vocabulary size (pretend we only know 10 words)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-1'></a> **Ungraded Exercise 1 - One-hot encoding**\n",
    "\n",
    "1. The output of `F.one_hot` above should have dimensionality [3, 5, 10]. Can you tell why?\n",
    "2. In general, if the input batch has shape `[B, L]` and our vocabulary has `V` tokens, what's the expected output size of `F.one_hot(input_batch, V)`?\n",
    "3. How many trainable parameters are needed to specify a one-hot encoding function for a vocabulary of size `V`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. Every token id in the input should have been converted to a 10-dimensional one-hot vector.\n",
    "The input has shape [3, 5], that is, we have 3 sequences of length 5 (counting PADs).\n",
    "Hence, the output should be [3, 5, 10].\n",
    "2. In general, for [B, L] inputs we expect [B, L, V] outputs.\n",
    "3. None. The one-hot encoding function does not require trainable parameters, the output is \"hard-coded\" to be exactly the one-hot representation of a symbol, for each and every symbol in the vocabulary. \n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT8JUS4nKSpt",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Word_embeddings'></a>\n",
    "## Word embeddings\n",
    "\n",
    "\n",
    "Our next operation is a bit more interesting. It allows us to associate with each token a vector of trainable parameters. So, instead of encoding a symbol $t \\in \\mathcal V$ into a $V$-dimensional one-hot vector, we encode it into a $D$-dimensional vector of real-numbers. We normally refer to this operation as *embedding* the token into a $D$-dimensional space. Suppose we have a table of parameters $\\mathbf E \\in \\mathbb R^{V \\times D}$, with one $D$-dimensional row for each of the known symbols in the vocabulary $\\mathcal V$. Then, for some symbol $t \\in \\mathcal V$, the embedding operation returns the vector $\\mathbf e \\in \\mathbb R^D$ that corresponds to it.\n",
    "\n",
    "Sometimes, we need to describe this operation in written form (for example, to sketch our model architecture). Here is a compact notation for it:\n",
    "\\begin{align}\n",
    "    \\mathbf e &= \\mathrm{embed}_D(t; \\mathbf E)\n",
    "\\end{align}\n",
    "\n",
    "The subscript $_D$ indicates the output dimensionality. After `;` we have the trainable parameters of the operation.\n",
    "\n",
    "\n",
    "If we had a sequence of symbols, for example, a document $w_{1:l}$, we could apply the embedding operation to each symbol in the sequence, and denote it this way:\n",
    "\\begin{align}\n",
    "    \\mathbf e_i &= \\mathrm{embed}_D(w_i; \\mathbf E) \\quad \\text{for }i=1,\\ldots, l\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use torch to specify an embedding layer. We will design a toy embedding layer, for a toy vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PU-SYeT-bim"
   },
   "outputs": [],
   "source": [
    "# this creates the layer with untrained parameters\n",
    "toy_emb_dim = 2\n",
    "toy_vocab_size = len(vocabulary)\n",
    "toy_emb = nn.Embedding(\n",
    "    num_embeddings=toy_vocab_size, \n",
    "    embedding_dim=toy_emb_dim, \n",
    ")\n",
    "toy_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that pytorch will intialise all 10 vectors for us, each 2-dimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toy_emb.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch initialised those embeddings for us, at random. It is possible to intialise embeddings with meaningful features (you will see one example in T4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 2-dimensional embedding layer, we can plot the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if toy_emb.embedding_dim == 2:\n",
    "    _ = plt.scatter(toy_emb.weight[:,0].detach().numpy(), toy_emb.weight[:,1].detach().numpy(), marker='x')\n",
    "    for t, i in vocabulary.items(): # pretending our vocabulary is this toy example\n",
    "        _ = plt.annotate(t, toy_emb.weight[i].detach().numpy(), fontsize=12)\n",
    "    _ = plt.xlabel(\"Embedding dimension 1\")\n",
    "    _ = plt.ylabel(\"Embedding dimension 2\")    \n",
    "    _ = plt.title(\"Embeddings at initialisation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method of the embedding module will embed every token in a batch of token ids.\n",
    "Let's test test it on a toy batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qj47hI4aRiA2"
   },
   "outputs": [],
   "source": [
    "toy_batch = torch.tensor(\n",
    "    [\n",
    "        [vocabulary.idx(s) for s in \"cats and dogs are awesome\".split()],\n",
    "        [vocabulary.idx(s) for s in \"cute cats -PAD- -PAD- -PAD-\".split()],\n",
    "        [vocabulary.idx(s) for s in \"otters are cute -PAD- -PAD-\".split()],\n",
    "        [vocabulary.idx(s) for s in \"cute rabbits and cute otters\".split()]\n",
    "    ]\n",
    ")\n",
    "toy_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O93Nc_pG-qKT"
   },
   "outputs": [],
   "source": [
    "# this embeds the tokens in the sequences in the batch\n",
    "e = toy_emb(toy_batch)\n",
    "print(e.shape)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to count the number of parameters in a layer, here is some helper code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHJOVtd-NX9y"
   },
   "outputs": [],
   "source": [
    "def num_parameters(torch_module):\n",
    "    \"\"\"A helper to count the number of parameters in a torch module\"\"\"\n",
    "    return sum(np.prod(theta.shape) for theta in torch_module.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-2'></a> **Ungraded Exercise 2 - Token embedding**\n",
    "\n",
    "Assume the input batch has shape `[B, L]`, our vocabulary has size `V` and our embedding vectors are `D`-dimensional. \n",
    "\n",
    "1. Once we pass the input batch through the embedding layer, what's the output shape?\n",
    "2. How many trainable parameters do we need in order to specify an embedding layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. The output shape is `[B, L, D]` because each one of the token ids is mapped to a `D`-dimensional vector.\n",
    "2. We need $V \\times D$ trainable parameters, that is, $D$ parameters for each of the $V$ symbols in the vocabulary.\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# We can use this to verify that the output shape is correct\n",
    "\n",
    "assert toy_emb(toy_batch).shape == toy_batch.shape + (toy_emb_dim,)\n",
    "\n",
    "# We can use this to verify the size of the embedding layer\n",
    "\n",
    "assert num_parameters(toy_emb) == toy_vocab_size * toy_emb_dim, \"Embedding layers are built upon [V, D] matrices\"\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Pooling_from_multiple_vectors'></a>\n",
    "# Pooling from multiple vectors\n",
    "\n",
    "Sometimes we need to combine a variable number of $D$-dimensional vectors into a single $D$-dimensional vector, this is usually referred to as *pooling*. There are different pooling operations, some with and some without trainable parameters, for now we only cover those without trainable parameters.\n",
    "\n",
    "General idea:\n",
    "* Input: a collection $\\mathbf e_1, \\ldots, \\mathbf e_l$ of $l > 0$ vectors, all $D$-dimensional.\n",
    "* Output: a single $D$-dimensional vector $\\mathbf u \\in \\mathbf R^D$.\n",
    "\n",
    "*Batched* implementations of pooling operations must give special treatment to positions that should not affect the result (i.e., those that correspond to `-PAD-`), as we will discuss in each case below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Sum_pooling'></a>\n",
    "## Sum pooling \n",
    "\n",
    "The output is the elementwise sum of the inputs.\n",
    "\n",
    "* Input: a collection of $l > 0$ vectors $\\mathbf e_1, \\ldots, \\mathbf e_l$, all of the same dimensionality $D$.\n",
    "* Output $\\mathbf u \\in \\mathbb R^D$ defined as\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf u &= \\sum_{i=1}^l \\mathbf e_i\n",
    "\\end{align}\n",
    "\n",
    "That is, for each dimension $d \\in [D]$, we have\n",
    "\\begin{align}\n",
    "    u_d &=  \\sum_{i=1}^l e_{i,d}\n",
    "\\end{align}\n",
    "\n",
    "For a *batched* implementation of this operation, input vectors in padded positions should be treated as if they were $\\mathbf 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_pooling(input_sequences, sequence_mask):\n",
    "    \"\"\"\n",
    "    Returns the sum of the vectors along the sequence dimension.\n",
    "    \n",
    "    input_sequences: [batch_size, max_length, D] a batch of sequences of D-dimensional vectors\n",
    "    sequence_mask: [batch_size, max_length] indicates which positions are valid (i.e., not PAD)\n",
    "        we use 1 for valid (not PAD) and 0 for PAD\n",
    "    \n",
    "    Output shape is [batch_size, D]\n",
    "    \"\"\"\n",
    "    \n",
    "    # here we replace padding positions by D-dimensional vectors of 0s, \n",
    "    #  this way those options won't contribute to the sum\n",
    "    # [batch_size, max_length, D]    \n",
    "    masked = torch.where(\n",
    "        # we create an extra axis at the end of the tensor\n",
    "        sequence_mask.unsqueeze(-1),  # this has shape [batch_size, max_length, 1]\n",
    "        input_sequences,  # this has shape [batch_size, max_length, D]\n",
    "        torch.zeros_like(input_sequences)  # this has shape [batch_size, max_length, D]\n",
    "    )\n",
    "    \n",
    "    # we sum, along the sequence dimension (second last),\n",
    "    #  the valid vectors (those that are not PAD)\n",
    "    # [batch_size, D]\n",
    "    return torch.sum(masked, dim=-2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this using our toy batch. To obtain a sequence mask, we compare the tokens in the batch to the PAD id (we are using `0` for that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_batch, toy_batch != vocabulary.idx(\"-PAD-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pooling(F.one_hot(toy_batch, toy_vocab_size), toy_batch != vocabulary.idx(\"-PAD-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-3'></a> **Ungraded Exercise 3 - Sum pooling**\n",
    "\n",
    "Suppose we have a batch of documents, with shape `[B, L]`, we turn the tokens into one-hot vectors of dimensionality `V`. Then, then we apply sum pooling to this input batch. \n",
    "\n",
    "1. What's the shape of the output?\n",
    "2. Can we interpret the result as a bag-of-words encoding of the documents in the batch?\n",
    "3. How many trainable parameters are needed to specify the sum pooling operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. The input batch has shape `[B, L]`, after one-hot encoding, we get `[B, L, V]`, since each token becomes a V-dimensional one-hot vector. Finally, sum pooling works along the sequence dimension, hence we have output shape `[B, V]`.\n",
    "2. Yes, that's precisely how we can interpret the output. you can see it in the toy example above. \n",
    "3. None, the sum pooling operation is an elementwise transformation of existing vectors and it does not require any trainable parameters. \n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "h = sum_pooling(F.one_hot(toy_batch, toy_vocab_size), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "# Note how the 'step' or 'time' dimension is gone\n",
    "assert h.shape == (toy_batch.shape[0], toy_vocab_size)\n",
    "# In the last document h[-1], \"cute\" occurs twice:\n",
    "assert h[-1, vocabulary.idx(\"cute\")] == 2\n",
    "# In the last document h[-1], we have 2 unknown words (rabbits and otters):\n",
    "assert h[-1, vocabulary.idx(\"-UNK-\")] == 2\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TCR1DkBLOBM",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Average_pooling'></a>\n",
    "## Average pooling \n",
    "\n",
    "When the output is the elementwise average of the inputs, this is known as *average pooling*. \n",
    "\n",
    "* Input: a collection of $l > 0$ vectors $\\mathbf e_1, \\ldots, \\mathbf e_l$, all of the same dimensionality $D$.\n",
    "* Output $\\mathbf u \\in \\mathbb R^D$ defined as\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf u &= \\frac{1}{l} \\sum_{i=1}^l \\mathbf e_i\n",
    "\\end{align}\n",
    "\n",
    "That is, for each dimension $d \\in [D]$, we have\n",
    "\\begin{align}\n",
    "    u_d &= \\frac{1}{l} \\sum_{i=1}^l e_{i,d}\n",
    "\\end{align}\n",
    "\n",
    "A *batched* implementation of this operation must ignore padded inputs (treat them as $\\mathbf 0$ and also count sequence length without them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnEclurjA0u5"
   },
   "outputs": [],
   "source": [
    "def average_pooling(input_sequences, sequence_mask):\n",
    "    \"\"\"\n",
    "    Returns the average encoding of each sequence.\n",
    "    \n",
    "    input_sequences: [batch_size, max_length, D] a batch of sequences of D-dimensional vectors\n",
    "    sequence_mask: [batch_size, max_length] indicates which positions are valid (i.e., not PAD)\n",
    "        we use 1 for valid (not PAD) and 0 for PAD\n",
    "    \n",
    "    Output shape is [batch_size, D]\n",
    "    \"\"\"\n",
    "    \n",
    "    # here we replace padding positions by D-dimensional vectors of 0s, \n",
    "    #  this way those options won't contribute to the sum\n",
    "    # [batch_size, max_length, D]    \n",
    "    masked = torch.where(\n",
    "        # we create an extra axis at the end of the tensor\n",
    "        sequence_mask.unsqueeze(-1),  # this has shape [batch_size, max_length, 1]\n",
    "        input_sequences,  # this has shape [batch_size, max_length, D]\n",
    "        torch.zeros_like(input_sequences)  # this has shape [batch_size, max_length, D]\n",
    "    )\n",
    "    \n",
    "    # we sum, along the sequence dimension (second last),\n",
    "    #  the valid vectors (those that are not PAD)\n",
    "    # we also divide by sequence length\n",
    "    # [batch_size, D]\n",
    "    avg = torch.sum(masked, dim=-2) / torch.sum(sequence_mask.float(), dim=-1, keepdims=True)\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-4'></a> **Ungraded Exercise 4 - Average pooling**\n",
    "\n",
    "For an input batch with shape `[B, L, D]`, where `D` is the embedding dimension and `L` is max sequence length.\n",
    "\n",
    "1. What's the output shape if we do average pooling over the sequence dimension?\n",
    "2. Does average pooling require trainable parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. `[B, D]` since average pooling will eliminate the sequence dimension, by elementwise average of the vectors in the sequence\n",
    "2. No, it simply operates over the input vectors.\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# This checks the shape of the output\n",
    "h = average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "assert h.shape == (toy_batch.shape[0], toy_emb_dim)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-5'></a> **Ungraded Exercise 5 - Limitations of pooling**\n",
    "\n",
    "Can you already recognise a big limitation of `sum_pooling` or `average_pooling` (in combination with one-hot or embeddings) as a means to represent a document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "Sum or average pooling are not sensitive to the order in which the input vectors are presented. So, if the inputs to pooling do not preserve information about word order, then neither will the output. You can see that in our `toy_batch` the first and the third document are permutations of one another, hence they have the same BoW encoding and the same average embedding encoding.\n",
    "\n",
    "Later, in this tutorial, we will address this limitation.\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_toy_batch = torch.tensor(\n",
    "    [\n",
    "        [vocabulary.idx(s) for s in \"dogs are awesome pets\".split()],\n",
    "        [vocabulary.idx(s) for s in \"awesome pets are dogs\".split()],\n",
    "        [vocabulary.idx(s) for s in \"dogs awesome pets are\".split()],\n",
    "        [vocabulary.idx(s) for s in \"are dogs awesome pets\".split()]\n",
    "    ]\n",
    ")\n",
    "another_toy_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "bow = sum_pooling(F.one_hot(another_toy_batch, toy_vocab_size), another_toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "avgemb = average_pooling(toy_emb(another_toy_batch), another_toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "assert torch.all(bow[0] == bow[1])\n",
    "assert torch.all(bow[0] == bow[2])\n",
    "assert torch.all(bow[0] == bow[3])\n",
    "assert torch.allclose(avgemb[0], avgemb[1])  # when comparing floating numbers, we use `allclose` rather than `all`\n",
    "assert torch.allclose(avgemb[0], avgemb[2])  # when comparing floating numbers, we use `allclose` rather than `all`\n",
    "assert torch.allclose(avgemb[0], avgemb[3])  # when comparing floating numbers, we use `allclose` rather than `all`\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Mapping_from_one_real_coordinate_space_to_another'></a>\n",
    "# Mapping from one real coordinate space to another\n",
    "\n",
    "Sometimes we are working with vectors of a certain dimensionality $I$ and we need to convert them to vectors of another dimensionality $O$. This is very common, for example, when mapping a document encoding (e.g., average embedding, or BoW encoding) to $C$ logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Linear_transformation'></a>\n",
    "## Linear transformation \n",
    "\n",
    "A *linear transformation* is the standard way to get this done:  with trainable parameters $\\mathbf W \\in \\mathbb R^{O\\times I}$ and $\\mathbf b \\in \\mathbb R^O$ we can map any $I$-dimensional input $\\mathbf u$ to an output $\\mathbf v \\in \\mathbf R^O$ via $\\mathbf v = \\mathbf W \\mathbf u + \\mathbf b$.\n",
    "\n",
    "In some situations, we prefer to use this transformation without a bias vector (i.e., setting $\\mathbf b$ to a vector of $0$s); sometimes, this is also called a *projection*. \n",
    "\n",
    "Sometimes, we need to describe this operation in written form (for example, to sketch our model architecture). Here is a compact notation for it:\n",
    "\\begin{align}\n",
    "    \\mathbf v &= \\mathrm{linear}_O(\\mathbf u; \\mathbf W, \\mathbf b)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we can construct a linear transformation with its trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_linear = nn.Linear(\n",
    "    toy_emb_dim, # number of inputs\n",
    "    3 # number of outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a toy example, you can inspect the initialised parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_linear.weight, toy_linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, linear layers work on batched inputs too, so let's use the toy batch, convert its tokens to D-dimensional embeddings, combine all vectors in each sequence using average pooling, and then, finally, project each D-dimensional document encoding to 3 logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_linear(average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-6'></a> **Ungraded Exercise 6 - Linear transformation**\n",
    "\n",
    "If we have a batch of inputs with shape `[B, L]`, use a `D`-dimensional embedding layer to encode tokens,  average pooling to encode documents, and then a linear transformation to `K`-dimensional outputs:\n",
    "\n",
    "1. What's the shape of the output?\n",
    "2. How many trainable parameters are necessary to specify the linear layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    " \n",
    "1. After the embedding layer we have `[B, L, D]`, after the pooling operation we have `[B, D]` and after the linear transformation we have `[B, K]`.\n",
    "2. We need to store a matrix of shape `[K, D]` and a vector of `K` biases to linearly transform `D`-dimensional vectors into `K`-dimensional vectors.\n",
    "    \n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Nonlinear_activation_functions'></a>\n",
    "## Nonlinear activation functions\n",
    "\n",
    "\n",
    "\n",
    "Sometimes we need to work on a _subspace_ of the real coordinate space, for example, where numbers are constrained to being positive, or strictly positive, or strictly positive and sum up to 1, etc.\n",
    "We can achieve this by working with _activation functions_. An activation function will not change the dimensionality of its input, and it does not require any trainable parameter, typically, an activation function is a formula that transforms a vector elementwise. \n",
    "\n",
    "For example, if $\\mathbf u \\in \\mathbb R^D$\n",
    "\n",
    "* $\\exp(\\mathbf u)$ applies $\\exp(u_d)$ to each coordinate $u_d$ of $\\mathbf u$, hence returning a vector of $D$ strictly positive numbers;\n",
    "* $\\mathrm{relu}(\\mathbf u)$ applies $\\max(0, u_d)$ to each coordinate $u_d$ of $\\mathbf u$, hence returning a vector of $D$ positive (and possibly 0) numbers;\n",
    "* $\\tanh(\\mathbf u)$ applies $\\tanh(u_d)$ to each coordinate $u_d$ of $\\mathbf u$, hence returning a vector $D$ numbers in the space $(-1, 1)$;\n",
    "* $\\mathrm{sigmoid}(\\mathbf u)$ applies $\\frac{1}{1+\\exp(u_d)}$ to each coordinate $u_d$ of $\\mathbf u$, hence returning a vector of $D$ independently normalised probability values (i.e., each in the space $(0, 1)$);\n",
    "* $\\mathrm{softplus}(\\mathbf u)$ applies $\\log(1+\\exp(u_d))$ to each coordinate $u_d$ of $\\mathbf u$, hence returning a vector of $D$ strictly positive numbers;\n",
    "* $\\mathrm{softmax}(\\mathbf u)$ applies $\\frac{\\exp(u_d)}{\\sum_{k=1}^D \\exp(u_k)}$ to each coordinate $u_d$ of $\\mathbf u$, hence returning a vector of $D$ strictly positive numbers that add up to 1 (i.e., a point in the simplex $\\Delta_{D-1} \\subset \\mathbb R^D$).\n",
    "\n",
    "<details>\n",
    "<summary> <b> Remarks about sigmoid and softplus </b> (click to expand) </summary> \n",
    "\n",
    "The _sigmoid_ function is also known as the _logistic_ function (in statistics). In various reference texts, the sigmoid function is denoted by $\\sigma(\\cdot)$, that is, $\\sigma(a) = \\frac{1}{1+\\exp(a)}$. Here are some useful results about the sigmoid function: \n",
    "* $\\sigma(a)= \\frac{1}{1+\\exp(a)}\\\\=\\frac{\\exp(u_d)}{1+\\exp(u_d)}\\\\=1-\\sigma(-a)$ \n",
    "* $\\frac{\\mathrm{d}}{\\mathrm{d}a}\\sigma(a)=\\sigma(a) \\times (1-\\sigma(a))$\n",
    "\n",
    "\n",
    "The _softplus_ function can be thought of as a smooth approximation to the $\\mathrm{relu}$ function. It is often used when we need strictly positive numbers while also needing more numerical stability than the $\\exp$ function can offer. In particular, the derivative of the $\\exp$ function is the $\\exp$ function (\\ie, $\\frac{\\mathrm{d}}{\\mathrm{d}a}\\exp(a)=\\exp(a)$), while the derivative of the $\\mathrm{softplus}$ function is the $\\mathrm{sigmoid}$  function: \n",
    "* $\\frac{\\mathrm{d}}{\\mathrm{d}a}\\mathrm{softplus}(a) = \\mathrm{sigmoid}(a)$\n",
    "\n",
    "the main advantage being that $\\exp$ can become very large, while $\\mathrm{sigmoid}$ is always between $0$ and $1$.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "torch.exp(torch.tensor([-2.5, -1., 0., 1, 2.5]))\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.relu(torch.tensor([-2.5, -1., 0., 1, 2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tanh(torch.tensor([-2.5, -1., 0., 1, 2.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these operations also work with batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-7'></a> **Ungraded Exercise 7 - Activation functions**\n",
    "\n",
    "Play a bit with the following activations and explain what they do:\n",
    "1. `torch.sigmoid`\n",
    "2. `F.softplus`\n",
    "3. `F.softmax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# sigmoid maps from R to (0, 1), it's appropriate to predict probability values\n",
    "torch.sigmoid(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]))\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# softplus maps from R to R+, it's appropriate to predict statistical parameters that must be strictly positive\n",
    "# like rate, scale, concentration, etc.\n",
    "F.softplus(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]))\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# softmax maps from R^K to Simplex K-1, it's appropriate to predict probability vectors (that are normalised as to sum to 1)\n",
    "print(F.softmax(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]), dim=-1))\n",
    "assert torch.allclose(torch.sum(F.softmax(torch.tensor([[-2.5, -1., 0., 1, 2.5], [-3., -2., -1, 0.5, 1.5]]), dim=-1), -1), torch.ones(2))\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Composing_multiple_vectors'></a>\n",
    "# Composing multiple vectors\n",
    "\n",
    "\n",
    "We now look into _composition functions_ that combine multiple vectors of fixed dimensionality into one (or more) vectors, while possibly changing the dimensionality of the output with respect to the dimensionality of the input(s). While pooling functions are necessarily discarding some information available in the input (e.g., sum discards order in the input collection, average discards order and size of the input collection, maximum discards inputs that are not the largest, etc.), we compose vectors  to a) _not_ discard anything important, and b) let the inputs _interact_ to create new, more complex features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMgtKxTPKxBR",
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Concatenation'></a>\n",
    "## Concatenation\n",
    "\n",
    "\n",
    "The simplest thing we can do, in order not to discard _any_ information, is to concatenate input vectors in their given order.\n",
    "For example, we can concatenate an $I_1$-dimensional vector $\\mathbf u$ with an $I_2$-dimensional vector $\\mathbf v$ obtaining an $I_1+I_2$-dimensional vector:\n",
    "\\begin{align}\n",
    "    \\mathbf h &= \\mathrm{concat}(\\mathbf u, \\mathbf v) = (u_1, \\ldots, u_{I_1}, v_1, \\ldots, v_{I_2})^\\top~.\n",
    "\\end{align}\n",
    "\n",
    "Another way to denote the concatenation of $l$ vectors $\\mathbf u_1, \\ldots, \\mathbf u_l$, each of dimensionality $\\operatorname{dim}(\\mathbf u_i)$, is to write $[\\mathbf u_1, \\ldots, \\mathbf u_l]$, the output vector will then have dimensionality $\\sum_{i=1}^l \\operatorname{dim}(\\mathbf u_i)$.\n",
    "\n",
    "\n",
    "A few things to consider about concatenation. It does not require any trainable parameters, and can be applied to any number of input vectors. However, the more inputs we have, the more outputs we have. If in a certain context we need to deal with variable-length inputs, we would then have to deal with variable-length outputs, which is sometimes not possible. \n",
    "Besides, and perhaps most importantly, while not discarding any information, concatenation is unable to create new features (e.g., by letting input features interact), this, however, we address next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-8'></a> **Ungraded Exercise 8 - Concatenation**\n",
    "\n",
    "Concatenate the BoW encoding of the documents in the toy batch with their average embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "bow = sum_pooling(F.one_hot(toy_batch, toy_vocab_size), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "avgemb = average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "bow_avg = torch.cat([bow, avgemb], -1)\n",
    "assert bow_avg.shape[-1] == toy_vocab_size + toy_emb_dim\n",
    "print(bow_avg)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Feed_forward_network'></a>\n",
    "## Feed forward network\n",
    "\n",
    "\n",
    "The simplest feed-forward network (FFNN) combines two linear transformations with a non-linear activation function in between. \n",
    "The input features _interact_ forming the intermediate (\"hidden\") features.\n",
    "It is also possible to make an FFNN _deeper_, by stacking additional linear transformations (again, with nonlinearities in between).\n",
    "\n",
    "This is the simplest example:\n",
    "\\begin{align}\n",
    "    \\mathbf h &= a(\\mathrm{linear}_H(\\mathbf u; \\theta_{\\text{hid}})) \\\\\n",
    "    \\mathbf v &= \\mathrm{linear}_O(\\mathbf h; \\theta_{\\text{out}}) \n",
    "\\end{align}\n",
    "where $a(\\cdot)$ is an elementwise nonlinearity (e.g., an activation function), typically $\\tanh$ or $\\mathrm{relu}$, $\\theta_{\\text{hid}}$ are the parameters of the first input-to-hidden layer (a weight matrix and a bias vector) and  $\\theta_{\\text{out}}$ are the parameters of the second hidden-to-output layer (another weight matrix and another bias vector). See that instead of writing down the parameters explicitly, we named them (using $\\theta$ and a suggestive subscript); this is a convenient notation shortcut.\n",
    "\n",
    "Here's a visual depiction of an FFNN\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/img/example-ffnn.png\" width=\"250\" />\n",
    "\n",
    "A nice way to prescribe FFNNs in torch is to use the so-called `Sequential` API, which stacks transformations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we concatenate two views of each document: the bag-of-words encoding and the average token embedding.\n",
    "\n",
    "We then map each such vector to 7 ReLU hidden units and then to 3 output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 7\n",
    "output_size = 3\n",
    "toy_ffnn = nn.Sequential(\n",
    "    nn.Linear(toy_vocab_size + toy_emb_dim, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_size)\n",
    ")\n",
    "assert num_parameters(toy_ffnn) == ((toy_vocab_size + toy_emb_dim)*hidden_size + hidden_size + hidden_size*output_size + output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a FFNN accepts batched inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = sum_pooling(F.one_hot(toy_batch, toy_vocab_size), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "avgemb = average_pooling(toy_emb(toy_batch), toy_batch != vocabulary.idx(\"-PAD-\"))\n",
    "bow_avg = torch.cat([bow, avgemb], -1)\n",
    "assert toy_ffnn(bow_avg).shape == (bow_avg.shape[0], output_size)\n",
    "toy_ffnn(bow_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-9'></a> **Ungraded Exercise 9 - FFNN**\n",
    "\n",
    "Use the Sequential API to specify a FFNN similar to the example above, but with 2 hidden layers: the first with 7 units and the second with 13 units. Use ReLU for the first hidden layer and Tanh for the second hidden layer. The output of your FFNN should have 3 units, like in the example above. Inspect the number of parameters of the layer. Then, test it on the concatenation of bow and avg encodings, as we did above. Explain why the output shape is the same as in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "hidden_sizes = [7, 13]\n",
    "output_size = 3\n",
    "toy_ffnn2 = nn.Sequential(\n",
    "    nn.Linear(toy_vocab_size + toy_emb_dim, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_sizes[1], output_size)\n",
    ")\n",
    "assert num_parameters(toy_ffnn2) == ((toy_vocab_size + toy_emb_dim)*hidden_sizes[0] + hidden_sizes[0] + hidden_sizes[0]*hidden_sizes[1] + hidden_sizes[1] + hidden_sizes[1]*output_size + output_size)\n",
    "\n",
    "assert toy_ffnn2(bow_avg).shape == (bow_avg.shape[0], output_size)\n",
    "\n",
    "assert toy_ffnn2(bow_avg).shape == toy_ffnn(bow_avg).shape\n",
    "\n",
    "# The output shape is the same as before \n",
    "# because the shape is determined by the ouptut_size \n",
    "# and not by the number of hidden layers\n",
    "toy_ffnn2(bow_avg)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "chHImOvsCrnA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Recurrent_neural_network'></a>\n",
    "## Recurrent neural network \n",
    "\n",
    "A recurrent neural network (RNN) uses one or more feed-forward networks inside of a for-loop to iterate over the steps of a sequence, at each step the RNN combines a _recurrent_ state and the input at that step using a FFNN. Rather than one different FFNN per step, the RNN reuses the same FFNN. Suppose we have a sequence of vectors $\\mathbf e_{1:l}$, all of which are $I$-dimensional, for example, those are the the token embeddings for a document $w_{1:l}$.\n",
    "\n",
    "The RNN has a initial state $\\mathbf u_0$, this is an $H$-dimensional vector of trainable parameters. Then, at each step $i \\in [l]$ of the sequence, the RNN composes the preceding recurrent state $\\mathbf u_{i-1}$ with the current input $\\mathbf e_i$, this combination produces a new value $\\mathbf u_i$ for the recurrent state, which will be used in the next step. See the figure below for an illustration.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/img/example-rnn.png\" width=\"600\" />\n",
    "\n",
    "We can describe the computation as follows:\n",
    "\\begin{align}\n",
    "\\mathbf u_i &= \\mathrm{rnnstep}_H(\\mathbf u_{i-1}, \\mathbf e_{i}; \\theta) \\quad \\text{for }i \\in [l].\n",
    "\\end{align}\n",
    "\n",
    "The simplest implementation of the block _rnnstep_ looks like this:\n",
    "\\begin{align}\n",
    "\\mathrm{rnnstep}_H(\\mathbf u_{i-1}, \\mathbf e_{i}; \\theta) &= \\tanh(\\mathbf R \\mathbf u_{i-1} + \\mathbf W \\mathbf e_{i}) \n",
    "\\end{align}\n",
    "where the trainable parameters $\\theta = \\{\\mathbf u_0 \\in \\mathbb R^H, \\mathbf R \\in \\mathbb R^{H\\times H}, \\mathbf W \\in \\mathbb R^{H\\times I}, \\}$ are two matrices that project the recurrent state and the current input to size $H$.\n",
    "This is even simpler than employing a full FFNN, since we only have a hidden layer.\n",
    "\n",
    "\n",
    "If you pay close attention to the illustration, or to the formulae, you will see that the recurrent state at any one position $i$ can potentially store information from any of the inputs $\\mathbf e_1, \\ldots, \\mathbf e_{i-1}$. Besides, due to the nonlinearity of the _rnnstep_ function, the state $\\mathbf u_i$ is sensitive to the order in which the inputs are presented (that is, if we presented inputs in a different order, the numerical values of the coordinates of $\\mathbf u_i$ might differ). This is an important aspect of a feature function for natural language processing, given that natural languages encode a lot of information in word order.\n",
    "\n",
    "This form of RNN is also called an RNN _encoder_, in allusion to the fact that $\\mathbf u_i$ encodes the vector sequence $\\mathbf e_{1:i}$. In written form, a call to an RNN encoder can be denoted even more compactly as \n",
    "\\begin{align}\n",
    "\\mathbf u_{1:l} &= \\mathrm{rnnenc}_H(\\mathbf e_{1:l}; \\theta) ~.\n",
    "\\end{align}\n",
    "This function takes a sequence of vectors as input and returns a sequence of $H$-dimensional vectors, each encoding a longer subsequence of the input sequence. The last vector $\\mathbf u_l$ of the output sequence has the potential to store information about the entire input sequence (in its given order).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "BgTTQ_uWN0qV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:LSTM'></a>\n",
    "### LSTM\n",
    "\n",
    "\n",
    "The simplest RNN suffers from certain instability issues (they struggle to retain information from long sequences and they lead to numerical problems in optimisation). \n",
    "A modern RNN-type architecture that does not exhibit these problems is the Long Short-Term Memory. %(https://arxiv.org/pdf/1503.04069.pdf) (LSTM for short). [It's already implemented for us in torch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n",
    "You do not need to study the LSTM paper, what you will need to know will be explained in this notebook. \n",
    "It is not necessary, for the applications in this course, to study the LSTM paper. For completeness, we briefly explain the internal design of the LSTM here, but this level of detail is much more than what we need in this course. \n",
    "The choice of letters we use in this part are internal to the LSTM and are not to be confused for letters used in other contexts.\n",
    "\n",
    "For a step $t$, let $\\mathbf e_t$ be an $I$-dimensional input to an LSTM (e.g., this may be an embedding for the token $w_t$ in a document).\n",
    "\n",
    "At this point, the memory of an LSTM is made of two $K$-dimensional vectors called the _cell vector_ $\\mathbf c_{t-1}$ and the _hidden state_ $\\mathbf h_{t-1}$, each of which is $K$-dimensional. When we process the input $\\mathbf x_t$ with an LSTM, these two vectors are updated step by step as shown below:\n",
    "\\begin{align}\n",
    "    \\mathbf i_t &=\\mathrm{sigmoid}(\\mathrm{linear}_K(\\mathbf e_t; \\theta_1) + \\mathrm{linear}_K(\\mathbf h_{t-1}; \\theta_2))\\\\\n",
    "    \\mathbf f_t &=\\mathrm{sigmoid}(\\mathrm{linear}_K(\\mathbf e_t; \\theta_3) + \\mathrm{linear}_K(\\mathbf h_{t-1}; \\theta_4))\\\\\n",
    "    \\mathbf g_t &=\\tanh(\\mathrm{linear}_K(\\mathbf e_t; \\theta_5) + \\mathrm{linear}_K(\\mathbf h_{t-1}; \\theta_6))\\\\\n",
    "    \\mathbf o_t&=\\mathrm{sigmoid}(\\mathrm{linear}_K(\\mathbf e_t; \\theta_7) + \\mathrm{linear}_K(\\mathbf h_{t-1}; \\theta_8))\\\\\n",
    "    \\mathbf c_t &= \\mathbf f_t \\odot \\mathbf c_{t-1} + \\mathbf i_t \\odot \\mathbf g_t \\\\\n",
    "    \\mathbf h_t &= \\mathbf o_t \\odot \\tanh(\\mathbf c_t)\n",
    "\\end{align}\n",
    "The first four steps compute the following using the input and the hidden state: the _input gate_ $\\mathbf i_t$, then the _forget gate_ $\\mathbf f_t$, the _draft cell_ $\\mathbf g_t$, and the _output gate_ $\\mathbf o_t$. \n",
    "These are all $K$-dimensional, and the linear transformations all have their own parameters (there 8 such affine transformations in total, they map either from $I$ dimensions to $K$ dimensions, or from $K$ dimensions to $K$ dimensions, and they have biases vectors in them). The last couple of steps finally update the cell and the hidden state by combining the intermediate gates and draft cell. The symbol $\\odot$ denotes elementwise multiplication. After the update the LSTM memory is made of two states $\\mathbf c_t $ and $\\mathbf h_t$, each $K$-dimensional.  The torch implementation gives us access to both of them, and we will see later how to use it.\n",
    "\n",
    "\n",
    "Typically, we regard the cell states $\\mathbf c_{1:l}$ as something internal to the LSTM and we rarely need to use them for anything outside of it. It is the hidden state $\\mathbf h_i$ at each step that we normally want to use in applications (e.g., as a representation of the sequence $\\mathbf e_{1:i}$).\n",
    "Hence, in this book, it is sufficient to think of the LSTM encoder as a function \n",
    "\\begin{align}\n",
    "\\mathbf h_{1:l} &= \\mathrm{lstm}_K(\\mathbf e_{1:l}; \\theta) \n",
    "\\end{align}\n",
    "that returns a sequence $\\mathbf h_{1:l}$ of hidden states encoding the input sequence $\\mathbf e_{1:l}$. This also shows that an LSTM is indeed just a special type of RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcyP1TxcC6Q5"
   },
   "outputs": [],
   "source": [
    "toy_hidden_size = 6\n",
    "toy_lstm = nn.LSTM(\n",
    "    input_size=toy_emb_dim, # size of the vectors in the input sequence\n",
    "    hidden_size=toy_hidden_size, # size of the recurrent cell\n",
    "    num_layers=1,\n",
    "    batch_first=True,  # this is important, it's telling the nn.LSTM class\n",
    "    bidirectional=False,  # we will explain this argument in the next example\n",
    ")\n",
    "toy_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9F1CDM5UNwcp"
   },
   "outputs": [],
   "source": [
    "num_parameters(toy_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dG1fCFVnDLCO"
   },
   "outputs": [],
   "source": [
    "# [batch_size, max_len, emb_dim]\n",
    "e = toy_emb(toy_batch)\n",
    "# [batch_size, max_len, hidden_dim]\n",
    "# internally, the LSTM maintains two vectors in the memory\n",
    "# the forward method of the LSTM class will return \n",
    "# a tensor which has the sequence of so called hidden states (this is usually what you want to use in a text encoder)\n",
    "# and tuple of tensors that can be used in case you need access to the internal \n",
    "# mechanism of the LSTM cell\n",
    "\n",
    "# For convenience, torch provides two auxiliary functions that help us deal with \n",
    "# batches of sequences that may differ in length\n",
    "# First, we pack the padded sequences in a special way using `pack_padded_sequence`\n",
    "#  for this to work correctly, torch needs to know the length of the sequences (discounting padding)\n",
    "# [batch_size]\n",
    "lengths = (toy_batch != vocabulary.idx(\"-PAD-\")).long().sum(-1)\n",
    "packed_seqs = pack_padded_sequence(e, lengths.cpu(), batch_first=True, enforce_sorted=False)        \n",
    "# it's important to tell torch that the first axis of our tensors is for the batch (with `batch_first=True`)\n",
    "# it's also important to tell torch that our sequences are _not_ sorted by length (with `enfore_sorted=False`)\n",
    "\n",
    "# Next, we run the LSTM on packed sequences,\n",
    "#  this returns, for every sequence, the states for all steps and a tuple (final state, final cell)\n",
    "h, (last_h, last_c) = toy_lstm(packed_seqs)\n",
    "\n",
    "# Finally, before going ahead and using `h`, we call `pad_packed_sequence`\n",
    "#  this returns the tensor of states with padding positions zeroed out\n",
    "h, _ = pad_packed_sequence(h, batch_first=True)\n",
    "\n",
    "assert h.shape == toy_batch.shape + (toy_hidden_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h)  # se how the state information is zeroed out for the -PAD- positions\n",
    "# this is because we used torch's helper functions (pack_padded_sequence and pad_packed_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_h) # this tensor, returned by the LSTM class gives us convenient access to the last state of each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to combine the LSTM outputs into a single vector, we could use average pooling, for example. This does not destroy the infromation about word order, since that information is already coded in the LSTM outputs.\n",
    "Alternatively, we could use the last state of the sequence. This is a practical choice, and there's no theory to support one option or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "DIie8FNzMblI",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "topic"
    ]
   },
   "source": [
    "<a name='sec:Bidirectional_RNN_encoder'></a>\n",
    "### Bidirectional RNN encoder\n",
    "\n",
    "When we encode a document, for example, in text classification, we have the entire document available to us and we are by no means constrained to processing the words from left-to-right. Why not also encode it from right-to-left, for example? \n",
    "\n",
    "Even better, why not do both? This way, whenever we look at a given position $i$, we can obtain information from its left (i.e., from $\\mathbf e_1, \\ldots, \\mathbf e_i$) and from its right (i.e., from $\\mathbf e_i, \\ldots, \\mathbf e_l$). This gives us a fully contextualised view of the token that sits at the $i$th position of a document.\n",
    "\n",
    "An RNN cell, by design, makes computations in a single direction (e.g., left-to-right), but we can use 2 different RNN cells, one that reads the sequence in one order and another that reads the sequence in reversed order.\n",
    "\n",
    "We don't need to invent a new RNN cell for this, we can simply reverse the inputs to a standard RNN cell:\n",
    "\\begin{align}\n",
    "\\mathbf r_{1:l} &= \\mathrm{reverse}(\\mathbf e_{1:l}) \\\\\n",
    "\\mathbf v_i &= \\mathrm{rnnstep}_K(\\mathbf v_{i-1}, \\mathbf r_{i}; \\theta_{\\text{renc}}) ~.\n",
    "\\end{align}\n",
    "Because the inputs have been reversed in order. For example, in a sentence of length $l=10$, $\\mathbf v_2$ knows about $w_{9}$ through $\\mathbf r_2$ and about $w_{>9}$ through $\\mathbf v_{1}$. The last state $\\mathbf v_l$ has information about the entire document $w_{1:l}$, but processed it in reversed order.\n",
    "\n",
    "Therefore a reversed RNN encoder can be denoted compactly as follows:\n",
    "\\begin{align}\n",
    "\\mathbf v_{1:l} &= \\mathrm{rnnenc}_K(\\mathrm{reverse}(\\mathbf e_{1:l}); \\theta_{\\text{renc}})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Note that we named the parameter set differently: $\\theta_{\\text{enc}}$ for the first RNN cell, and $\\theta_{\\text{renc}}$ for the second one, that's because we indeed want to have two different sets of parameters. If we used the same set of parameters for both directions, that probably would not work very well, as reading in one direction and reading in another are conceptually two different operations.\n",
    "\n",
    "The **bidirectional RNN encoder** is our prefered text encoder, it can be denoted as follows:\n",
    "\\begin{align}\n",
    "\\mathbf o_{1:l} &= \\mathrm{birnn}_{2K}(\\mathbf e_{1:l}; \\theta_{\\text{enc}} \\cup \\theta_{\\text{renc}})\n",
    "\\end{align}\n",
    "and here are the operations that it performs:\n",
    "\\begin{align}\n",
    "\\mathbf u_{1:l} &= \\mathrm{rnnenc}_K(\\mathbf e_{1:l}; \\theta_{\\text{enc}})\\\\\n",
    "\\mathbf v_{1:l} &= \\mathrm{rnnenc}_K(\\mathrm{reverse}(\\mathbf e_{1:l}); \\theta_{\\text{renc}})\\\\\n",
    "\\mathbf o_{i} &= \\mathrm{concat}(\\mathbf u_i, \\mathbf v_{l-i+1}) & \\text{for }i \\in \\{1, \\ldots, l\\}\n",
    "\\end{align}\n",
    "\n",
    "Its outputs are $2K$-dimensional because after processing the sequence from left-to-right with the first RNN encoder and from right-to-left with the second RNN encoder, it then concatenates the two views of the process in such a way that $\\mathbf o_i$ has information about $w_i$, $w_{<i}$ and $w_{>i}$.\n",
    "\n",
    "See the figure as an illustration of how the two RNN cells can be used to obtain the bidirectional RNN encoder: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/probabll/ntmi-tutorials/main/img/example-birnn.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Luckily, `nn.LSTM` implements all that for us, and we don't really need to worry about reversing anything ourserlves. See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "xZEXWM3rDZpW",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_bilstm = nn.LSTM(\n",
    "    input_size=toy_emb_dim,\n",
    "    hidden_size=toy_hidden_size,\n",
    "    num_layers=1,\n",
    "    batch_first=True,\n",
    "    bidirectional=True,  # now we employ one LSTM for each direction\n",
    ")\n",
    "toy_bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parameters(toy_bilstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhi8SmRKUeJp"
   },
   "outputs": [],
   "source": [
    "assert num_parameters(toy_bilstm) == 2*num_parameters(toy_lstm), \"A BiLSTM is made of two LSTMs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ungraded-10'></a> **Ungraded Exercise 10 - BiLSTM**\n",
    "\n",
    "Use `toy_bilstm` to encode our `toy_batch` (remember that from an API point of view, a BiLSTM is just like an LSTM, hence you should use torch auxiliary functions `pack_padded_sequence` and `pad_packed_sequence`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# [batch_size, max_len, 2*hidden_dim]\n",
    "# as for the standard LSTM, we only use the first of its outputs, namely, \n",
    "# a tensor of states, this time the states are concatenated for two directions, \n",
    "# thus they will be twice as large\n",
    "\n",
    "# As before, we use torch's helper functions to correctly deal with the variable length\n",
    "# of our padded sequences\n",
    "packed_seqs = pack_padded_sequence(e, lengths.cpu(), batch_first=True, enforce_sorted=False)        \n",
    "h2, (last_h2, last_c2) = toy_bilstm(packed_seqs)\n",
    "h2, _ = pad_packed_sequence(h2, batch_first=True)\n",
    "assert h2.shape == toy_batch.shape + (2*toy_hidden_size,)  # BiLSTM outputs are the concatenation of 2 LSTM outputs\n",
    "\n",
    "print(\"h shape:\", h2.shape) # the shape here is as expected [batch_size, max_len, 2*hidden_dim]\n",
    "print(\"last shape:\", last_h2.shape)  # the shape here is a bit different: [num_layers, batch_size, hidden_dim]\n",
    "# where each direction counts as 1 layer, hence num_layers is 2\n",
    "\n",
    "# If we wanted to use the final states we would have to concatenate\n",
    "#  the states from different directions\n",
    "# We first move the first axis (num_layers) to the end of the tensor using `permute`\n",
    "#  and then flatten the last two axis (hidden_dim, num_layers)\n",
    "#  obtaining output shape [batch_size, num_layers * hidden_dim]\n",
    "print(\"last states reshaped and concatenated:\", torch.flatten(torch.permute(last_h2, (1, 2, 0)), 1, 2).shape)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# What Next?\n",
    "\n",
    "In T4 you will experiment using these blocks to develop text encoders for a text classifier. \n",
    "There, you will have training data (labelled documents) which you can use to estimate the parameters of the NN blocks (which in this notebook were never trained, they were only initialised at random). \n",
    "\n",
    "In T4, you will also learn some important tricks needed for effective optimisation of NN blocks, such as regularisation techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary> <b>Click to see a solution</b> </summary>\n",
    "\n",
    "If you double-click the cell, you will be able to copy the code:\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "    \n",
    "</details>      \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNuHBiG349O5I8O42aWP8nQ",
   "collapsed_sections": [],
   "include_colab_link": false,
   "name": "Encoders.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
